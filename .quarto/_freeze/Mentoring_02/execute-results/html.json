{
  "hash": "67687a86af7f14f1b4cb3863f34ebacb",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Mentoring 2 - Introduction to Machine Learning\njupyter: python3\nexecute:\n  eval: false\n---\n\n\n\n\n\nMentoring Session - Job Preparation Program - Pacmann AI\n\n\n## Instructions\n\n\n1. Please fill all the given tasks in here\n2. You can use any library\n3. For modeling, please use `sklearn` library\n4. You are taksed to create a function based machine learning model. (If you cannot create the functions from the start, you can create without a function first, then put it all into a function)\n5. Make sure you are following all the function descriptions\n6. **THIS DATA IS SCRAPED FROM LAMUDI**. YOU ARE **PROHIBITED** TO USE THIS DATA OUTSIDE PACMANN MENTORING.\n6. Submit your result to the submission form\n\n## Dataset Description\n\n\n**Note**\n\n- This dataset is scraped from [Lamudi](https://www.lamudi.co.id/)\n- We perform several edit for this mentoring purposes. So, please use the dataset from [here](https://drive.google.com/file/d/109ZcOSllPPWCETc01tXiI3Fx8BWRU-32/view?usp=sharing).\n\n**Description**\n- We're looking to predict the rent price of a house\n- The dataset contains of the following fields\n\n<center>\n\n|Feature|Type|Descriptions|\n|:--|:--|:--|\n|`name`|`str`|The name (title) of a house|\n|`url`|`str`|The house url|\n|`bedrooms`|`int`|The number of bedrooms|\n|`bathrooms`|`int`|The number of bathrooms|\n|`floors`|`int`|The number of floors|\n|`land_area`|`float`|The area of land in m2|\n|`building_area`|`float`|The building area in m2|\n|`longitude`|`float`|The house longitude coordinate in degree|\n|`latitude`|`float`|The house latitude coordinate in degree|\n| `price` | `int` | The yearly rent price (IDR) , (**our target**)|\n\n## Modeling Workflow\n\n\n```\n1. Import data to Python\n2. Data Preprocessing\n3. Training a Machine Learning Models\n4. Test Prediction\n5. Lets Explore\n```\n\n### 1. Import data to Python (5 pts)\n\n::: {#55ee23ab .cell execution_count=1}\n``` {.python .cell-code}\n####################################################\n# Import Numpy and Pandas library\n# Write your code here\n# 1 pts\n####################################################\n```\n:::\n\n\n::: {#53ab9c30 .cell execution_count=2}\n``` {.python .cell-code}\n####################################################\n# Create a function named read_data\n# - Has an input of filename, i.e. fname\n# - Read the data as a Pandas DataFrame\n# - Drop duplicate on `url`, keep the last ones\n# - Drop col `names` and set `url` as index\n# - Print the data shape\n# - Return the dataset\n# Write your code here (4 pts)\n####################################################\n```\n:::\n\n\n::: {#98295dc6 .cell execution_count=3}\n``` {.python .cell-code}\n# Read the Uber data (JUST RUN THE CODE)\ndata = read_data(fname='scrape_house_edit.csv')\n```\n:::\n\n\n::: {#b02367d3 .cell execution_count=4}\n``` {.python .cell-code}\n# JUST RUN THE CODE\ndata.head()\n```\n:::\n\n\n### 2. Data Preprocessing (22 pts)\n\n\n**The processing pipeline**\n```\n2.1 Input-Output Split\n2.2 Train-Valid-Test Split\n2.3 Remove & Preprocess Anomalous Data\n2.4 Numerical Imputation\n2.5 Feature Engineering the Data\n2.6 Create a Preprocessing Function\n```\n\n#### 2.1. Input-Output Split (3 pts)\n\n\n- We're going to split input & output according to the modeling objective.\n- Create a function to split the input & output\n\n::: {#0d0c1481 .cell execution_count=5}\n``` {.python .cell-code}\n####################################################\n# Create a function named split_input_output\n# - Has two arguments\n#   - data, a pd Dataframe\n#   - target_col, a column (str)\n# - Print the data shape after splitting\n# - Return X, y\n# Write your code here\n####################################################\n```\n:::\n\n\n::: {#aad098f1 .cell execution_count=6}\n``` {.python .cell-code}\n# Load the train data only (JUST RUN THE CODE)\nX, y = split_input_output(data=data,\n                          target_col='price')\n```\n:::\n\n\n::: {#4391fa11 .cell execution_count=7}\n``` {.python .cell-code}\nX.head()  # (JUST RUN THE CODE)\n```\n:::\n\n\n::: {#a2773394 .cell execution_count=8}\n``` {.python .cell-code}\ny.head()  # (JUST RUN THE CODE)\n```\n:::\n\n\n#### 2.2. Train-Valid-Test Split (3 pts)\n\n\n- Now, we want to split the data before modeling.\n- Split the data into three set:\n  - Train, for training the model\n  - Validation, for choosing the best model\n  - Test, for error generalization\n\n- You should make the splitting proportion train (80%), valid (10%), and test (10%)\n\n::: {#f3de6193 .cell execution_count=9}\n``` {.python .cell-code}\n####################################################\n# Create a function named split_train_test\n# - Has two arguments\n#   - X, the input (pd.Dataframe)\n#   - y, the output (pd.Dataframe)\n#   - test_size, the test size between 0-1 (float)\n#   - seed, the random state (int)\n# - Print the data shape after splitting\n# - Return X_train, X_test, y_train, y_test\n# - You can use an sklearn library to help you\n# Write your code here\n####################################################\n```\n:::\n\n\n::: {#d19c94a5 .cell execution_count=10}\n``` {.python .cell-code}\n# Split the data\n# First, split the train & not train\nX_train, X_not_train, y_train, y_not_train = # Write your code here\n\n# Then, split the valid & test\nX_valid, X_test, y_valid, y_test = # Write your code here\n```\n:::\n\n\n::: {#ba52a507 .cell execution_count=11}\n``` {.python .cell-code}\n# Validate (JUST RUN THE CODE)\nprint(len(X_train)/len(X))  # should be 0.8\nprint(len(X_valid)/len(X))  # should be 0.1\nprint(len(X_test)/len(X))   # should be 0.1\n```\n:::\n\n\n::: {#a43e6046 .cell execution_count=12}\n``` {.python .cell-code}\nX_train.head()  # (JUST RUN THE CODE)\n```\n:::\n\n\n#### EDA before Preprocessing (JUST RUN THE CODE)\n\n\n- Find the number of missing values\n\n::: {#9f16e3c6 .cell execution_count=13}\n``` {.python .cell-code}\n100 * (X_train.isna().sum(0) / len(X_train))\n```\n:::\n\n\n- We will impute all these variables if there is any missing value\n\n- First, check the features distribution\n\n::: {#a22ca55a .cell execution_count=14}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n```\n:::\n\n\n::: {#e202433e .cell execution_count=15}\n``` {.python .cell-code}\n# Plot histogram\nfig, ax = plt.subplots(nrows=3, ncols=3, figsize=(12, 8))\naxes = ax.flatten()\n\nfor i, col in enumerate(X_train.columns):\n    sns.kdeplot(X_train[col], ax=axes[i])\n    axes[i].set_title(f'Distribution of {col}')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\nSummary:\n- Our data obviously have anomalies, e.g.\n  - there's no such thing of 50 `floors` for family-sized house, or\n  - 30 bathrooms, or\n  - (`longitude`, `latitude`) = (`0`, `0`) are not even in Indonesia.\n- We have to clean the data from this anomalies\n- We can assume that our numerical data have a skewed distribution, thus we'll use median to imput the missing values.\n\n::: {#7615087a .cell execution_count=16}\n``` {.python .cell-code}\nX_train.describe()\n```\n:::\n\n\n- Let's find the cut-off value of each features\n\n::: {#f3e0c5b1 .cell execution_count=17}\n``` {.python .cell-code}\n# this is the anomaly (check it by yourself)\nX_train[X_train['bedrooms']>11]\n```\n:::\n\n\n::: {#8f05f549 .cell execution_count=18}\n``` {.python .cell-code}\n# this is the anomaly (check it by yourself)\nX_train[X_train['bathrooms']>8]\n```\n:::\n\n\n::: {#d2bffa5d .cell execution_count=19}\n``` {.python .cell-code}\n# This is anomalous\n# We can set this and replace it with 1\nX_train[X_train['floors']<1]\n```\n:::\n\n\n::: {#bfcf111c .cell execution_count=20}\n``` {.python .cell-code}\n# this is the anomaly (check it by yourself)\nX_train[X_train['land_area']>1_700]\n```\n:::\n\n\n::: {#d00ccc84 .cell execution_count=21}\n``` {.python .cell-code}\n# this is the anomaly (check it by yourself)\nX_train[X_train['building_area']>1_000]\n```\n:::\n\n\n::: {#40b10e75 .cell execution_count=22}\n``` {.python .cell-code}\n# this is the anomaly (check it by yourself)\nX_train[X_train['longitude']<=1.0]\n```\n:::\n\n\n- We know that the house is real, but the coordinate is not.\n- We can trait the non-Indonesian coordinate as missing values\n\n- Next, explore the `price`\n\n::: {#058e6de5 .cell execution_count=23}\n``` {.python .cell-code}\nsns.kdeplot(y_train)\nplt.title(f'Distribution of Price')\nplt.show()\n```\n:::\n\n\n::: {#818cfde0 .cell execution_count=24}\n``` {.python .cell-code}\n# Check for outliers\ncond = y_train > 600_000_000\npd.concat((X_train[cond], y_train[cond]), axis=1)\n\n# We will exclude this, these houses are too expensive\n```\n:::\n\n\n- Explore the relation between features and `price`\n\n::: {#f9b9cfe9 .cell execution_count=25}\n``` {.python .cell-code}\n# Concat the data first\ntrain_data = pd.concat((X_train, y_train), axis=1)\ntrain_data.head()\n```\n:::\n\n\n::: {#b41346b7 .cell execution_count=26}\n``` {.python .cell-code}\n# Create a heatmap\n# Get the correlation matrix\ncorr = train_data.corr()\ncorr\n```\n:::\n\n\n::: {#4fbe2b9f .cell execution_count=27}\n``` {.python .cell-code}\n# Plot the heatmap\nsns.heatmap(corr, annot=True)\nplt.show()\n```\n:::\n\n\n- We can see, features `bedrooms`, `bathrooms`, `land_area`, and `building_area` have high correlation with `price`.\n\n**Conclusion for preprocessing**\n- First, remove the data from anomalous data\n- Then, generate imputer.\n\n#### 2.3. Remove & Preprocess Anomalous Data (6 pts)\n\n\n- Let's remove our data from anomalous.\n- Please see the EDA to help you remove the anomalous data\n\n::: {#5004e7d0 .cell execution_count=28}\n``` {.python .cell-code}\nX_train.describe()\n```\n:::\n\n\n::: {#8ebf719e .cell execution_count=29}\n``` {.python .cell-code}\n######################################################################\n# Find the data index to drop\n# Remember to carefully read the EDA part\n# Save the dropped index to idx_to_drop (list of index)\n# Write your code here (2 pts)\n######################################################################\n```\n:::\n\n\n::: {#46c5c938 .cell execution_count=30}\n``` {.python .cell-code}\n# Check the index (JUST RUN THE CODE)\nprint(f'Number of index to drop:', len(idx_to_drop))\nidx_to_drop\n```\n:::\n\n\n- Now, lets drop the data for `X_train` and also `y_train`\n\n::: {#678e83ad .cell execution_count=31}\n``` {.python .cell-code}\n######################################################################\n# Drop the anomalous data\n# Save the dropped data into X_train_dropped and y_train_dropped\n# Write your code here (1 pts)\n######################################################################\n```\n:::\n\n\n::: {#c9577fb7 .cell execution_count=32}\n``` {.python .cell-code}\n# Validate (JUST RUN THE CODE)\nprint('Shape of X train after dropped:', X_train_dropped.shape)\nX_train_dropped.head()\n```\n:::\n\n\n::: {#8ac37975 .cell execution_count=33}\n``` {.python .cell-code}\n# Validate (JUST RUN THE CODE)\nprint('Shape of y train after dropped:', y_train_dropped.shape)\ny_train_dropped.head()\n```\n:::\n\n\n- Great!\n- Next, we replace the missing `longitude` and `latitude` to `np.nan`\n- Please recall the definition of missing `longitude` and `latitude` in the EDA section\n\n::: {#c97dac04 .cell execution_count=34}\n``` {.python .cell-code}\n######################################################################\n# Replace the missing longitude and latitude to np.nan\n# Write your code here (2 pts)\n######################################################################\n```\n:::\n\n\n- Then, replace the `floors` of 0.0 to 1.0\n\n::: {#21e760b8 .cell execution_count=35}\n``` {.python .cell-code}\n######################################################################\n# Replace the 0.0 floors to 1.0\n# Write your code here (1 pts)\n######################################################################\n```\n:::\n\n\n::: {#eb525d1b .cell execution_count=36}\n``` {.python .cell-code}\n# Validate (JUST RUN THE CODE)\nX_train_dropped.describe()\n```\n:::\n\n\n::: {#0bbc95ef .cell execution_count=37}\n``` {.python .cell-code}\n# Plot histogram (JUST RUN THE CODE)\nfig, ax = plt.subplots(nrows=3, ncols=3, figsize=(12, 8))\naxes = ax.flatten()\n\nfor i, col in enumerate(X_train_dropped.columns):\n    sns.kdeplot(X_train_dropped[col], ax=axes[i])\n    axes[i].set_title(f'Distribution of {col}')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n::: {#444e4a9f .cell execution_count=38}\n``` {.python .cell-code}\n# Create a heatmap (JUST RUN THE CODE)\ncorr = train_data.corr()\nsns.heatmap(corr, annot=True)\nplt.show()\n```\n:::\n\n\n::: {#7f977c34 .cell execution_count=39}\n``` {.python .cell-code}\n# Visualize price distribution (JUST RUN THE CODE)\nsns.kdeplot(y_train_dropped)\nplt.title(f'Distribution of Price')\nplt.show()\n```\n:::\n\n\n#### 2.4. Create Imputation (3 pts)\n\n\n- Now, let's perform a numerical imputation (because all features are numerical)\n- First check the missing value of the numerical data\n\n::: {#f817b9e0 .cell execution_count=40}\n``` {.python .cell-code}\n# Check missing value (JUST RUN THE CODE)\nX_train_dropped.isna().sum(0)\n```\n:::\n\n\n- Create a function to fit a numerical features imputer\n\n::: {#eaaf25b7 .cell execution_count=41}\n``` {.python .cell-code}\n####################################################\n# Create function to fit & transform numerical imputers\n# The fit function is called by num_imputer_fit\n# - it needs 1 input, the data (pd.DataFrame)\n# - the missing value is np.nan\n# - the imputation strategy is median\n# - it return the imputer\n#\n# The transform function is called by num_imputer_transform\n# - it needs 2 input, data (pd.DataFrame) and imputer (sklearn object)\n# - it return the imputed data in pd.DataFrame format\n#\n# Write your code here\n####################################################\n```\n:::\n\n\n- Perform imputation\n\n::: {#639725b9 .cell execution_count=42}\n``` {.python .cell-code}\n# Get the numerical imputer\nnum_imputer = # Write your code here\n\n# Transform the data\nX_train_imputed = # Write your code here\n```\n:::\n\n\n::: {#00cb8e29 .cell execution_count=43}\n``` {.python .cell-code}\n# Validate (JUST RUN THE CODE)\nX_train_imputed.isna().sum(0)\n```\n:::\n\n\nGreat!\n\n#### 2.5. Feature engineering the data (3 pts)\n\n\n- We standardize the data so that it can perform well during model optimization (4 pts)\n\n::: {#557884ef .cell execution_count=44}\n``` {.python .cell-code}\n####################################################\n# Create two functions to perform scaling & transform scaling\n# The scaling is Standardization\n# The first function is to fit the scaler, called by fit_scaler\n# - You need an input, a data (pd.Dataframe)\n# - You create a standardization scaler (please use sklearn)\n# - Your output is the scaler\n#\n# The second function is to transform data using scaler, called by transform_scaler\n# - There are two inputs, a data (pd.Dataframe), a scaler (sklearn object)\n# - You scaled the data, then return the scaled data\n# Write your code here\n####################################################\n```\n:::\n\n\n::: {#a6d0ea77 .cell execution_count=45}\n``` {.python .cell-code}\n# Fit the scaler\nscaler = # Write your code here\n\n# Transform the scaler\nX_train_clean = # Write your code here\n```\n:::\n\n\n::: {#3a6f606e .cell execution_count=46}\n``` {.python .cell-code}\n# Validate (JUST RUN THE CODE)\nX_train_clean.describe().round(4)\n```\n:::\n\n\nGreat!\n\n#### 2.6. Create the preprocess function (4 pts)\n\n\n- Now, let's create a function to preprocess other set of data (valid & test) so that we can predict that\n\n::: {#8028af43 .cell execution_count=47}\n``` {.python .cell-code}\n####################################################\n# Create a function to preprocess the dataset\n# You called the function preprocess_data\n# - It needs many input\n#   - data, pd.DataFrame\n#   - num_imputer, the numerical imputer, sklearn object\n#   - scaler, the data scaler, sklearn object\n# - You preprocess the data following step 2.4 - 2.5\n# - You return the clean data\n#\n# Write your code here\n####################################################\n```\n:::\n\n\n::: {#e0508c68 .cell execution_count=48}\n``` {.python .cell-code}\n# Preprocess the data training again\nX_train_clean = # Write your code here\n```\n:::\n\n\n::: {#e31353e2 .cell execution_count=49}\n``` {.python .cell-code}\n# Validate (JUST RUN THE CODE)\nX_train_clean.head()\n```\n:::\n\n\n::: {#01ebe995 .cell execution_count=50}\n``` {.python .cell-code}\n# Transform other set of data\nX_valid_clean = # Write your code here\n\nX_test_clean = # Write your code here\n```\n:::\n\n\n### 3. Training Machine Learning Models (43 pts)\n\n\n```\n3.1 Prepare model evaluation function\n3.2 Train & evaluate several models\n3.3 Choose the best model\n```\n\n#### 3.1. Preprare model evaluation function (10 pts)\n\n\n- Before modeling, let's prepare two functions\n  - `extract_cv_results`: to return the score and best param from hyperparameter search\n  - `evaluate_model`: to return the RMSE of a model\n\n::: {#e7dfb6a9 .cell execution_count=51}\n``` {.python .cell-code}\n####################################################\n# First, create a function to extract the CV results\n# - The function name is extract_cv_results\n# - It needs one input, called by `cv_obj` (a GridSearchCV sklearn object)\n# - It returns three output:\n#   1. the CV score on train set (float)\n#   2. the CV score on valid set (float)\n#   3. The best params (dictionary)\n#\n#\n# Next, create a function to evaluate model called `rmse`\n# - It needs 2 input\n#   - y_actual, the actual output (pd.DataFrame or numpy array)\n#   - y_pred, the predicted output (pd.DataFrame or numpy array)\n# - You calculate the model performance using root mean squared error metrics\n# - Then return the rmse\n#\n# Write your code here\n####################################################\n```\n:::\n\n\n#### 3.2. Train and Cross Validate Several Models (23 pts)\n\n\n- Now, let's train & evaluate several models\n- You should check, which one of the following model is the best model\n\n  1. Baseline model (**3 pts**)\n  2. k-NN (**3 pts**)\n  3. Linear Regression (**4 pts**)\n  4. Decision Tree (**4 pts**)\n  5. Ridge (**4 pts**)\n  6. Lasso (**4 pts**)\n\n- We're going to perform a `GridSearchCV`, with\n  - number of CV = 10\n  - scoring = root mean squared error\n  - return the train score\n\n::: {#3aa878e0 .cell execution_count=52}\n``` {.python .cell-code}\n####################################################\n# Import sklearn library of those six models + gridsearchcv\n# Write your code here\n# This is 1 pts\n####################################################\n```\n:::\n\n\n##### Perform CV for baseline model (3 pts)\n- Return as `reg_base`\n\n::: {#d9ef4534 .cell execution_count=53}\n``` {.python .cell-code}\n####################################################\n# Perform GridSearchCV for Baseline model\n# return the results as reg_base\n# Write your code here\n####################################################\n```\n:::\n\n\n::: {#69573ad4 .cell execution_count=54}\n``` {.python .cell-code}\n# Validate the CV Score (JUST RUN THE CODE)\ntrain_base, valid_base, best_param_base = extract_cv_results(reg_base)\n\nprint(f'Train score - Baseline model: {train_base/(10**6):.2f} Juta')\nprint(f'Valid score - Baseline model: {valid_base/(10**6):.2f} Juta')\nprint(f'Best Params - Baseline model: {best_param_base}')\n```\n:::\n\n\n##### Perform CV for k-NN Model (3 pts)\n- Do a parameter search for `k = {1, 10, 25, 50, 100, 150, 200, 250}`\n- Return as `reg_knn`\n\n::: {#f06e33a5 .cell execution_count=55}\n``` {.python .cell-code}\n####################################################\n# Perform GridSearchCV for kNN model\n# return the results as reg_knn\n# Write your code here\n####################################################\n```\n:::\n\n\n::: {#d49d5b20 .cell execution_count=56}\n``` {.python .cell-code}\n# Validate the CV Score (JUST RUN THE CODE)\ntrain_knn, valid_knn, best_param_knn = extract_cv_results(reg_knn)\n\nprint(f'Train score - kNN model: {train_knn/(10**6):.2f} Juta')\nprint(f'Valid score - kNN model: {valid_knn/(10**6):.2f} Juta')\nprint(f'Best Params - kNN model: {best_param_knn}')\n```\n:::\n\n\n##### Perform CV for Linear Regression Model (4 pts)\n- Return as `reg_lr`\n\n::: {#0023d86c .cell execution_count=57}\n``` {.python .cell-code}\n####################################################\n# Perform GridSearchCV for Linear Regression model\n# return the results as reg_lr\n# Write your code here\n####################################################\n```\n:::\n\n\n::: {#83561cdf .cell execution_count=58}\n``` {.python .cell-code}\n# Validate the CV Score (JUST RUN THE CODE)\ntrain_lr, valid_lr, best_param_lr = extract_cv_results(reg_lr)\n\nprint(f'Train score - LinReg model: {train_lr/(10**6):.2f} Juta')\nprint(f'Valid score - LinReg model: {valid_lr/(10**6):.2f} Juta')\nprint(f'Best Params - LinReg model: {best_param_lr}')\n```\n:::\n\n\n##### Perform CV for Decision Tree Model (4 pts)\n- You search the best hyperparameter from\n  - maximum depth : 2, 10, 30, 100, None\n  - minimum samples required to split : 2, 25, 50, 100, 150\n  - minimum samples at leaf : 2, 5, 10, 20\n- Return as `reg_dt`\n\n::: {#00239989 .cell execution_count=59}\n``` {.python .cell-code}\n####################################################\n# Perform GridSearchCV for Decision Tree model\n# return the results as reg_dt\n# Write your code here\n####################################################\n```\n:::\n\n\n::: {#250588ef .cell execution_count=60}\n``` {.python .cell-code}\n# Validate the CV Score (JUST RUN THE CODE)\ntrain_dt, valid_dt, best_param_dt = extract_cv_results(reg_dt)\n\nprint(f'Train score - Decision Tree model: {train_dt/(10**6):.2f} Juta')\nprint(f'Valid score - Decision Tree model: {valid_dt/(10**6):.2f} Juta')\nprint(f'Best Params - Decision Tree model: {best_param_dt}')\n```\n:::\n\n\n##### Perform CV for Ridge Model (4 pts)\n- You search the best hyperparameter from\n  - regularization strength: [$10^{-6}$, ..., $10^{10}$],\n- Return as `reg_ridge`\n\n::: {#cae9d291 .cell execution_count=61}\n``` {.python .cell-code}\n####################################################\n# Perform GridSearchCV for Ridge model\n# return the results as reg_ridge\n# Write your code here\n####################################################\n```\n:::\n\n\n::: {#72739917 .cell execution_count=62}\n``` {.python .cell-code}\n# Validate the CV Score (JUST RUN THE CODE)\ntrain_ridge, valid_ridge, best_param_ridge = extract_cv_results(reg_ridge)\n\nprint(f'Train score - Ridge model: {train_ridge/(10**6):.2f} Juta')\nprint(f'Valid score - Ridge model: {valid_ridge/(10**6):.2f} Juta')\nprint(f'Best Params - Ridge model: {best_param_ridge}')\n```\n:::\n\n\n##### Perform CV for Lasso Model (4 pts)\n- You search the best hyperparameter from\n  - regularization strength: [$10^{-6}$, ..., $10^{10}$]\n- Return as `reg_lasso`\n\n::: {#b3f4efc0 .cell execution_count=63}\n``` {.python .cell-code}\n####################################################\n# Perform GridSearchCV for Lasso model\n# return the results as reg_lasso\n# Write your code here\n####################################################\n```\n:::\n\n\n::: {#caeda119 .cell execution_count=64}\n``` {.python .cell-code}\n# Validate the CV Score (JUST RUN THE CODE)\ntrain_lasso, valid_lasso, best_param_lasso = extract_cv_results(reg_lasso)\n\nprint(f'Train score - Lasso model: {train_lasso/(10**6):.2f} Juta')\nprint(f'Valid score - Lasso model: {valid_lasso/(10**6):.2f} Juta')\nprint(f'Best Params - Lasso model: {best_param_lasso}')\n```\n:::\n\n\n#### 3.3. Choose the best model (10 pts)\n\n\nLets summarize the model\n\n::: {#1f5877b7 .cell execution_count=65}\n``` {.python .cell-code}\n# Summarize (JUST RUN THE CODE)\nsummary_df = pd.DataFrame(\n    data={\n        'model': ['Baseline', 'kNN', 'Linear Regression', 'Decision Tree', 'Ridge', 'Lasso'],\n        'train_score': [train_base, train_knn, train_lr, train_dt, train_ridge, train_lasso],\n        'valid_score': [valid_base, valid_knn, valid_lr, valid_dt, valid_ridge, valid_lasso],\n        'best_params': [best_param_base, best_param_knn, best_param_lr, best_param_dt, best_param_ridge, best_param_lasso]\n    }\n)\n\nsummary_df['train_score'] /= 10**6\nsummary_df['valid_score'] /= 10**6\nsummary_df\n```\n:::\n\n\nFrom the previous results, which one is the best model? (3 pts)\n\n```\nAnswer in this section (you can use bahasa)\n\n```\n\nWhy do you choose that model? (3 pts)\n\n```\nExplain your answer in here (you can use bahasa)\n\n```\n\nAnd, create a `reg_best` to store the best model\n\n::: {#e5cc699d .cell execution_count=66}\n``` {.python .cell-code}\n#####################################################################\n# Recreate or retrain your best regression model\n# Set is as reg_best\n# Write your code in here (4 pts)\n#####################################################################\n```\n:::\n\n\n### 4. Predictions & Evaluations (JUST RUN THE CODE)\n\n\n```\n4.1 Predict & Evaluate on the Train Data\n4.2 Predict & Evaluate on the Test Data\n```\n\n#### 4.1. Predict & evaluate on train & valid data\n\n::: {#523b2bfb .cell execution_count=67}\n``` {.python .cell-code}\n# Predict (JUST RUN THE CODE)\ny_train_pred = reg_best.predict(X_train_clean)\n```\n:::\n\n\n::: {#b53a8e5e .cell execution_count=68}\n``` {.python .cell-code}\n# Find error (JUST RUN THE CODE)\ntrain_error = rmse(y_train_dropped, y_train_pred)\nprint(f'RMSE on Train data: {train_error/10**6:.2f} Juta')\n```\n:::\n\n\n::: {#fb42937b .cell execution_count=69}\n``` {.python .cell-code}\n# Visualize & compare the prediction (JUST RUN THE CODE)\nplt.scatter(y_train_dropped/1e6, y_train_pred/1e6)\n\nplt.plot([0, 600], [0, 600], c='red')\nplt.xlim(0, 600); plt.ylim(0, 600)\nplt.xlabel('y actual [Juta Rupiah]'); plt.ylabel('y predicted [Juta Rupiah]')\nplt.title('Comparison of y actual vs y predicted on Train Data')\nplt.show()\n```\n:::\n\n\n::: {#9c529f38 .cell execution_count=70}\n``` {.python .cell-code}\n# Predict (JUST RUN THE CODE)\ny_valid_pred = reg_best.predict(X_valid_clean)\n```\n:::\n\n\n::: {#0128f4a2 .cell execution_count=71}\n``` {.python .cell-code}\n# Find error (JUST RUN THE CODE)\nvalid_error = rmse(y_valid, y_valid_pred)\nprint(f'RMSE on Valid data: {valid_error/10**6:.2f} Juta')\n```\n:::\n\n\n::: {#d64b4dbf .cell execution_count=72}\n``` {.python .cell-code}\n# Visualize & compare the prediction (JUST RUN THE CODE)\nplt.scatter(y_valid/1e6, y_valid_pred/1e6)\n\nplt.plot([0, 600], [0, 600], c='red')\nplt.xlim(0, 600); plt.ylim(0, 600)\nplt.xlabel('y actual [Juta Rupiah]'); plt.ylabel('y predicted [Juta Rupiah]')\nplt.title('Comparison of y actual vs y predicted on Valid Data')\nplt.show()\n```\n:::\n\n\n#### 4.2. Predict & evaluate on test data\n\n::: {#cab23663 .cell execution_count=73}\n``` {.python .cell-code}\n# Predict (JUST RUN THE CODE)\ny_test_pred = reg_best.predict(X_test_clean)\n```\n:::\n\n\n::: {#88a71c66 .cell execution_count=74}\n``` {.python .cell-code}\n# Final generalization\ntest_error = rmse(y_test, y_test_pred)\nprint(f'RMSE on Test data: {test_error/10**6:.2f} Juta')\n```\n:::\n\n\n::: {#9665d072 .cell execution_count=75}\n``` {.python .cell-code}\n# Visualize & compare the prediction (JUST RUN THE CODE)\nplt.scatter(y_test/1e6, y_test_pred/1e6)\n\nplt.plot([0, 600], [0, 600], c='red')\nplt.xlim(0, 600); plt.ylim(0, 600)\nplt.xlabel('y actual [Juta Rupiah]'); plt.ylabel('y predicted [Juta Rupiah]')\nplt.title('Comparison of y actual vs y predicted on Test Data')\nplt.show()\n```\n:::\n\n\n### 5. Explore and generate a better model! (30)\n\n\n- Please explore by yourself!\n- Your **task** is to create a better model from the previous results.\n\n::: {#c000bad2 .cell execution_count=76}\n``` {.python .cell-code}\n#########################################\n# Write your code here\n# Feel free to explore\n#########################################\n```\n:::\n\n\n",
    "supporting": [
      "Mentoring_02_files"
    ],
    "filters": [],
    "includes": {}
  }
}