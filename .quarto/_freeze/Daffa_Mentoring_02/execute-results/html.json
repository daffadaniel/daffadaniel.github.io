{
  "hash": "31d1a3bce0d4fb84e37e4177480bb5b6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Mentoring 2 - Introduction to Machine Learning\njupyter: python3\nexecute:\n  eval: false\n---\n\n\n\nMentoring Session - Job Preparation Program - Pacmann AI\n\n\n## Instructions\n\n\n1. Please fill all the given tasks in here\n2. You can use any library\n3. For modeling, please use `sklearn` library\n4. You are taksed to create a function based machine learning model. (If you cannot create the functions from the start, you can create without a function first, then put it all into a function)\n5. Make sure you are following all the function descriptions\n6. **THIS DATA IS SCRAPED FROM LAMUDI**. YOU ARE **PROHIBITED** TO USE THIS DATA OUTSIDE PACMANN MENTORING.\n6. Submit your result to the submission form\n\n## Dataset Description\n\n\n**Note**\n\n- This dataset is scraped from [Lamudi](https://www.lamudi.co.id/)\n- We perform several edit for this mentoring purposes. So, please use the dataset from [here](https://drive.google.com/file/d/109ZcOSllPPWCETc01tXiI3Fx8BWRU-32/view?usp=sharing).\n\n**Description**\n- We're looking to predict the rent price of a house\n- The dataset contains of the following fields\n\n<center>\n\n|Feature|Type|Descriptions|\n|:--|:--|:--|\n|`name`|`str`|The name (title) of a house|\n|`url`|`str`|The house url|\n|`bedrooms`|`int`|The number of bedrooms|\n|`bathrooms`|`int`|The number of bathrooms|\n|`floors`|`int`|The number of floors|\n|`land_area`|`float`|The area of land in m2|\n|`building_area`|`float`|The building area in m2|\n|`longitude`|`float`|The house longitude coordinate in degree|\n|`latitude`|`float`|The house latitude coordinate in degree|\n| `price` | `int` | The yearly rent price (IDR) , (**our target**)|\n\n## Modeling Workflow\n\n\n```\n1. Import data to Python\n2. Data Preprocessing\n3. Training a Machine Learning Models\n4. Test Prediction\n5. Lets Explore\n```\n\n### 1. Import data to Python (5 pts)\n\n::: {#a296ef22 .cell execution_count=1}\n``` {.python .cell-code}\n####################################################\n# Import Numpy and Pandas library\n# Write your code here\n# 1 pts\n####################################################\nimport numpy as np\nimport pandas as pd\n```\n:::\n\n\n::: {#128efbcb .cell execution_count=2}\n``` {.python .cell-code}\n####################################################\n# Create a function named read_data\n# - Has an input of filename, i.e. fname\n# - Read the data as a Pandas DataFrame\n# - Drop duplicate on `url`, keep the last ones\n# - Drop col `names` and set `url` as index\n# - Print the data shape\n# - Return the dataset\n# Write your code here (4 pts)\n####################################################\ndef read_data(fname):\n  filename = '/content/' + fname\n\n  # read csv as pandas dataframe\n  dataset = pd.read_csv(filename)\n  print('Data shape raw               : ', dataset.shape)\n\n  # drop duplicate url\n  print('Number of duplicate url      : ', dataset.duplicated(subset = 'url', keep = False).sum() )\n  dataset.drop_duplicates(subset = ['url'], keep = 'last', inplace = True)\n  print(\"Data shape after dropping    : \", dataset.shape)\n\n  # drop 'name' column and set 'url' as index\n  dataset.drop(columns = 'name', inplace = True)\n  dataset.set_index(keys = 'url', inplace = True)\n\n  # print data shape\n  print('Data shape final             : ', dataset.shape)\n\n  return dataset\n```\n:::\n\n\n::: {#98e09afd .cell execution_count=3}\n``` {.python .cell-code}\n# Read the Uber data (JUST RUN THE CODE)\ndata = read_data(fname='scrape_house_edit.csv')\n```\n:::\n\n\n::: {#3e8853b8 .cell execution_count=4}\n``` {.python .cell-code}\n# JUST RUN THE CODE\ndata.head()\n```\n:::\n\n\n### 2. Data Preprocessing (22 pts)\n\n\n**The processing pipeline**\n```\n2.1 Input-Output Split\n2.2 Train-Valid-Test Split\n2.3 Remove & Preprocess Anomalous Data\n2.4 Numerical Imputation\n2.5 Feature Engineering the Data\n2.6 Create a Preprocessing Function\n```\n\n#### 2.1. Input-Output Split (3 pts)\n\n\n- We're going to split input & output according to the modeling objective.\n- Create a function to split the input & output\n\n::: {#ecbb1f56 .cell execution_count=5}\n``` {.python .cell-code}\n####################################################\n# Create a function named split_input_output\n# - Has two arguments\n#   - data, a pd Dataframe\n#   - target_col, a column (str)\n# - Print the data shape after splitting\n# - Return X, y\n# Write your code here\n####################################################\ndef split_input_output(data, target_col):\n  X = data.drop(columns = target_col)\n  y = data[target_col]\n  print('X shape:', X.shape)\n  print('y shape: ', y.shape)\n  return X, y\n```\n:::\n\n\n::: {#8bcbeb9b .cell execution_count=6}\n``` {.python .cell-code}\n# Load the train data only (JUST RUN THE CODE)\nX, y = split_input_output(data=data,\n                          target_col='price')\n```\n:::\n\n\n::: {#a533836a .cell execution_count=7}\n``` {.python .cell-code}\nX.head()  # (JUST RUN THE CODE)\n```\n:::\n\n\n::: {#94403537 .cell execution_count=8}\n``` {.python .cell-code}\ny.head()  # (JUST RUN THE CODE)\n```\n:::\n\n\n#### 2.2. Train-Valid-Test Split (3 pts)\n\n\n- Now, we want to split the data before modeling.\n- Split the data into three set:\n  - Train, for training the model\n  - Validation, for choosing the best model\n  - Test, for error generalization\n\n- You should make the splitting proportion train (80%), valid (10%), and test (10%)\n\n::: {#71ca2ff0 .cell execution_count=9}\n``` {.python .cell-code}\n####################################################\n# Create a function named split_train_test\n# - Has two arguments\n#   - X, the input (pd.Dataframe)\n#   - y, the output (pd.Dataframe)\n#   - test_size, the test size between 0-1 (float)\n#   - seed, the random state (int)\n# - Print the data shape after splitting\n# - Return X_train, X_test, y_train, y_test\n# - You can use an sklearn library to help you\n# Write your code here\n####################################################\nfrom sklearn.model_selection import train_test_split\n\ndef split_train_test(X, y, test_size, seed):\n\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state=seed)\n\n  print('X train shape: ', X_train.shape)\n  print('y train shape: ',  y_train.shape)\n  print('X test shape : ', X_test.shape)\n  print('y test shape : ', y_test.shape)\n  print('\\n')\n\n  return X_train, X_test, y_train, y_test\n```\n:::\n\n\n::: {#b4decf97 .cell execution_count=10}\n``` {.python .cell-code}\n# Split the data\n# First, split the train & not train\nX_train, X_not_train, y_train, y_not_train = split_train_test(X, y, test_size =0.2, seed = 42)\n\n# Then, split the valid & test\nX_valid, X_test, y_valid, y_test = split_train_test(X_not_train,  y_not_train, test_size =0.5, seed = 42)\n```\n:::\n\n\n::: {#89fc816e .cell execution_count=11}\n``` {.python .cell-code}\n# Validate (JUST RUN THE CODE)\nprint(len(X_train)/len(X))  # should be 0.8\nprint(len(X_valid)/len(X))  # should be 0.1\nprint(len(X_test)/len(X))   # should be 0.1\n```\n:::\n\n\n::: {#a364e09a .cell execution_count=12}\n``` {.python .cell-code}\nX_train.head()  # (JUST RUN THE CODE)\n```\n:::\n\n\n#### EDA before Preprocessing (JUST RUN THE CODE)\n\n\n- Find the number of missing values\n\n::: {#7d9e5e0c .cell execution_count=13}\n``` {.python .cell-code}\n100 * (X_train.isna().sum(0) / len(X_train))\n```\n:::\n\n\n- We will impute all these variables if there is any missing value\n\n- First, check the features distribution\n\n::: {#ac8520c4 .cell execution_count=14}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n```\n:::\n\n\n::: {#95772258 .cell execution_count=15}\n``` {.python .cell-code}\n# Plot histogram\nfig, ax = plt.subplots(nrows=3, ncols=3, figsize=(12, 8))\naxes = ax.flatten()\n\nfor i, col in enumerate(X_train.columns):\n    sns.kdeplot(X_train[col], ax=axes[i])\n    axes[i].set_title(f'Distribution of {col}')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\nSummary:\n- Our data obviously have anomalies, e.g.\n  - there's no such thing of 50 `floors` for family-sized house, or\n  - 30 bathrooms, or\n  - (`longitude`, `latitude`) = (`0`, `0`) are not even in Indonesia.\n- We have to clean the data from this anomalies\n- We can assume that our numerical data have a skewed distribution, thus we'll use median to imput the missing values.\n\n::: {#555352b9 .cell execution_count=16}\n``` {.python .cell-code}\nX_train.describe()\n```\n:::\n\n\n- Let's find the cut-off value of each features\n\n::: {#c19321f3 .cell execution_count=17}\n``` {.python .cell-code}\n# this is the anomaly (check it by yourself)\nX_train[X_train['bedrooms']>10]\n```\n:::\n\n\n::: {#95411f2c .cell execution_count=18}\n``` {.python .cell-code}\n# this is the anomaly (check it by yourself)\nX_train[X_train['bathrooms']>8]\n```\n:::\n\n\n::: {#980154af .cell execution_count=19}\n``` {.python .cell-code}\n# This is anomalous\n# We can set this and replace it with 1\nX_train[X_train['floors']<1]\n```\n:::\n\n\n::: {#4f99a13d .cell execution_count=20}\n``` {.python .cell-code}\n# this is the anomaly (check it by yourself)\nX_train[X_train['land_area']>1_000]\n```\n:::\n\n\n::: {#8ecf1d9f .cell execution_count=21}\n``` {.python .cell-code}\n# this is the anomaly (check it by yourself)\nX_train[X_train['building_area']>1_000]\n```\n:::\n\n\n::: {#b3cbf633 .cell execution_count=22}\n``` {.python .cell-code}\n# this is the anomaly (check it by yourself)\nX_train[X_train['longitude']<=1.0]\n```\n:::\n\n\n- We know that the house is real, but the coordinate is not.\n- We can trait the non-Indonesian coordinate as missing values\n\n- Next, explore the `price`\n\n::: {#33c3731b .cell execution_count=23}\n``` {.python .cell-code}\nsns.kdeplot(y_train)\nplt.title(f'Distribution of Price')\nplt.show()\n```\n:::\n\n\n::: {#ec115aa0 .cell execution_count=24}\n``` {.python .cell-code}\n# Check for outliers\ncond = y_train > 600_000_000\npd.concat((X_train[cond], y_train[cond]), axis=1)\n\n# We will exclude this, these houses are too expensive\n```\n:::\n\n\n- Explore the relation between features and `price`\n\n::: {#81589c4e .cell execution_count=25}\n``` {.python .cell-code}\n# Concat the data first\ntrain_data = pd.concat((X_train, y_train), axis=1)\ntrain_data.head()\n```\n:::\n\n\n::: {#d07c20cc .cell execution_count=26}\n``` {.python .cell-code}\n# Create a heatmap\n# Get the correlation matrix\ncorr = train_data.corr()\ncorr\n```\n:::\n\n\n::: {#9ad1f1de .cell execution_count=27}\n``` {.python .cell-code}\n# Plot the heatmap\nsns.heatmap(corr, annot=True)\nplt.show()\n```\n:::\n\n\n- We can see, features `bedrooms`, `bathrooms`, `land_area`, and `building_area` have high correlation with `price`.\n\n**Conclusion for preprocessing**\n- First, remove the data from anomalous data\n- Then, generate imputer.\n\n#### 2.3. Remove & Preprocess Anomalous Data (6 pts)\n\n\n- Let's remove our data from anomalous.\n- Please see the EDA to help you remove the anomalous data\n\n::: {#4594f356 .cell execution_count=28}\n``` {.python .cell-code}\nX_train.describe()\n```\n:::\n\n\n::: {#50bf659f .cell execution_count=29}\n``` {.python .cell-code}\n######################################################################\n# Find the data index to drop\n# Remember to carefully read the EDA part\n# Save the dropped index to idx_to_drop (list of index)\n# Write your code here (2 pts)\n######################################################################\n\n\nbedrooms_anomaly = X_train[X_train['bedrooms']>11].index.tolist()\n\nbathrooms_anomaly = X_train[X_train['bathrooms']>8].index.tolist()\n\nland_area_anomaly = X_train[(X_train['land_area']>1_700) | (X_train['land_area']==0)].index.tolist()\n\nbuilding_area_anomaly = X_train[(X_train['building_area']>1_000) | (X_train['building_area']== 0)].index.tolist()\n\nprice_anomaly = y_train[y_train > 600_000_000].index.tolist()\n\n\nidx_to_drop = set(bedrooms_anomaly + bathrooms_anomaly + land_area_anomaly + building_area_anomaly + price_anomaly)\n```\n:::\n\n\n::: {#4d64164c .cell execution_count=30}\n``` {.python .cell-code}\n# Check the index (JUST RUN THE CODE)\nprint(f'Number of index to drop:', len(idx_to_drop))\nidx_to_drop\n```\n:::\n\n\n- Now, lets drop the data for `X_train` and also `y_train`\n\n::: {#a93309c5 .cell execution_count=31}\n``` {.python .cell-code}\n######################################################################\n# Drop the anomalous data\n# Save the dropped data into X_train_dropped and y_train_dropped\n# Write your code here (1 pts)\n######################################################################\n\nX_train_dropped = X_train.drop(index = idx_to_drop)\ny_train_dropped = y_train.drop(index = idx_to_drop)\n```\n:::\n\n\n::: {#2ba963f4 .cell execution_count=32}\n``` {.python .cell-code}\n# Validate (JUST RUN THE CODE)\nprint('Shape of X train after dropped:', X_train_dropped.shape)\nX_train_dropped.head()\n```\n:::\n\n\n::: {#68dc2873 .cell execution_count=33}\n``` {.python .cell-code}\n# Validate (JUST RUN THE CODE)\nprint('Shape of y train after dropped:', y_train_dropped.shape)\ny_train_dropped.head()\n```\n:::\n\n\n- Great!\n- Next, we replace the missing `longitude` and `latitude` to `np.nan`\n- Please recall the definition of missing `longitude` and `latitude` in the EDA section\n\n::: {#a418e26d .cell execution_count=34}\n``` {.python .cell-code}\n######################################################################\n# Replace the missing longitude and latitude to np.nan\n# Write your code here (2 pts)\n######################################################################\n#Latitude Indonesia: -11.15° hingga +6.08°\n#Longitude Indonesia: 94.45° hingga 141.05°\n\nX_train_dropped.loc[(X_train_dropped['latitude'] < - 12) | (X_train_dropped['latitude'] > 7), 'latitude'] = np.nan\nX_train_dropped.loc[(X_train_dropped['longitude'] <= 95) | (X_train_dropped['longitude'] >= 142) , 'longitude'] = np.nan\n```\n:::\n\n\n- Then, replace the `floors` of 0.0 to 1.0\n\n::: {#2f30d12b .cell execution_count=35}\n``` {.python .cell-code}\n######################################################################\n# Replace the 0.0 floors to 1.0\n# Write your code here (1 pts)\n######################################################################\nX_train_dropped.loc[X_train_dropped['floors'] == 0, 'floors'] = 1\n```\n:::\n\n\n::: {#cf3480c3 .cell execution_count=36}\n``` {.python .cell-code}\n# Validate (JUST RUN THE CODE)\nX_train_dropped.describe()\n```\n:::\n\n\n::: {#ecbeb956 .cell execution_count=37}\n``` {.python .cell-code}\n# Plot histogram (JUST RUN THE CODE)\nfig, ax = plt.subplots(nrows=3, ncols=3, figsize=(12, 8))\naxes = ax.flatten()\n\nfor i, col in enumerate(X_train_dropped.columns):\n    sns.kdeplot(X_train_dropped[col], ax=axes[i])\n    axes[i].set_title(f'Distribution of {col}')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n::: {#b8e525cd .cell execution_count=38}\n``` {.python .cell-code}\n# Create a heatmap (JUST RUN THE CODE)\ncorr = train_data.corr()\nsns.heatmap(corr, annot=True)\nplt.show()\n```\n:::\n\n\n::: {#8e1576be .cell execution_count=39}\n``` {.python .cell-code}\n# Visualize price distribution (JUST RUN THE CODE)\nsns.kdeplot(y_train_dropped)\nplt.title(f'Distribution of Price')\nplt.show()\n```\n:::\n\n\n#### 2.4. Create Imputation (3 pts)\n\n\n- Now, let's perform a numerical imputation (because all features are numerical)\n- First check the missing value of the numerical data\n\n::: {#a57e28f5 .cell execution_count=40}\n``` {.python .cell-code}\n# Check missing value (JUST RUN THE CODE)\nX_train_dropped.isna().sum(0)\n```\n:::\n\n\n- Create a function to fit a numerical features imputer\n\n::: {#411b233f .cell execution_count=41}\n``` {.python .cell-code}\n####################################################\n# Create function to fit & transform numerical imputers\n# The fit function is called by num_imputer_fit\n# - it needs 1 input, the data (pd.DataFrame)\n# - the missing value is np.nan\n# - the imputation strategy is median\n# - it return the imputer\n#\n# The transform function is called by num_imputer_transform\n# - it needs 2 input, data (pd.DataFrame) and imputer (sklearn object)\n# - it return the imputed data in pd.DataFrame format\n#\n# Write your code here\n####################################################\nfrom sklearn.impute import SimpleImputer\n\ndef num_imputer_fit(data):\n  imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')\n\n  imputer.fit(data)\n\n  return imputer\n\ndef num_imputer_transform(data, imputer):\n  imputer.set_output(transform = \"pandas\")\n  data = imputer.transform(data)\n  return data\n```\n:::\n\n\n- Perform imputation\n\n::: {#e5075601 .cell execution_count=42}\n``` {.python .cell-code}\n# Get the numerical imputer\nnum_imputer = num_imputer_fit(X_train_dropped)# Write your code here\n\n# Transform the data\nX_train_imputed = num_imputer_transform(X_train_dropped, num_imputer)# Write your code here\n```\n:::\n\n\n::: {#ffb3d26e .cell execution_count=43}\n``` {.python .cell-code}\n# Validate (JUST RUN THE CODE)\nX_train_imputed.isna().sum(0)\n```\n:::\n\n\nGreat!\n\n#### 2.5. Feature engineering the data (3 pts)\n\n\n- We standardize the data so that it can perform well during model optimization (4 pts)\n\n::: {#b8eb9821 .cell execution_count=44}\n``` {.python .cell-code}\n####################################################\n# Create two functions to perform scaling & transform scaling\n# The scaling is Standardization\n# The first function is to fit the scaler, called by fit_scaler\n# - You need an input, a data (pd.Dataframe)\n# - You create a standardization scaler (please use sklearn)\n# - Your output is the scaler\n#\n# The second function is to transform data using scaler, called by transform_scaler\n# - There are two inputs, a data (pd.Dataframe), a scaler (sklearn object)\n# - You scaled the data, then return the scaled data\n# Write your code here\n####################################################\nfrom sklearn.preprocessing import StandardScaler\n\ndef fit_scaler(data):\n  scaler = StandardScaler()\n  scaler.fit(data)\n  return scaler\n\ndef transform_scaler(data, scaler):\n  scaler.set_output(transform = 'pandas')\n  data = scaler.transform(data)\n  return data\n```\n:::\n\n\n::: {#712a163b .cell execution_count=45}\n``` {.python .cell-code}\n# Fit the scaler\nscaler = fit_scaler(X_train_imputed)# Write your code here\n\n# Transform the scaler\nX_train_clean = transform_scaler(X_train_imputed, scaler)# Write your code here\n```\n:::\n\n\n::: {#56b4526b .cell execution_count=46}\n``` {.python .cell-code}\n# Validate (JUST RUN THE CODE)\nX_train_clean.describe().round(4)\n```\n:::\n\n\nGreat!\n\n#### 2.6. Create the preprocess function (4 pts)\n\n\n- Now, let's create a function to preprocess other set of data (valid & test) so that we can predict that\n\n::: {#a3ed2b6b .cell execution_count=47}\n``` {.python .cell-code}\n####################################################\n# Create a function to preprocess the dataset\n# You called the function preprocess_data\n# - It needs many input\n#   - data, pd.DataFrame\n#   - num_imputer, the numerical imputer, sklearn object\n#   - scaler, the data scaler, sklearn object\n# - You preprocess the data following step 2.4 - 2.5\n# - You return the clean data\n#\n# Write your code here\n####################################################\ndef preprocess_data(data, num_imputer, scaler):\n  data = num_imputer_transform(data, num_imputer)\n  data = transform_scaler(data, scaler)\n  return data\n```\n:::\n\n\n::: {#2e1d45c8 .cell execution_count=48}\n``` {.python .cell-code}\n# Preprocess the data training again\nX_train_clean = preprocess_data(X_train_dropped, num_imputer, scaler) # Write your code here\n```\n:::\n\n\n::: {#805fd50c .cell execution_count=49}\n``` {.python .cell-code}\n# Validate (JUST RUN THE CODE)\nX_train_clean.head()\n```\n:::\n\n\n::: {#b59f4180 .cell execution_count=50}\n``` {.python .cell-code}\n# Transform other set of data\nprint(\"Original data shape: \",X_valid.shape)\nX_valid_clean = preprocess_data(X_valid, num_imputer, scaler)# Write your code here\nprint(\"Cleaned data shape: \",X_valid_clean.shape)\n\nprint(\"Original data shape:\", X_test.shape)\nX_test_clean = preprocess_data(X_test, num_imputer, scaler) # Write your code here\nprint(\"Cleaned data shape:\", X_test_clean.shape)\n```\n:::\n\n\n### 3. Training Machine Learning Models (43 pts)\n\n\n```\n3.1 Prepare model evaluation function\n3.2 Train & evaluate several models\n3.3 Choose the best model\n```\n\n#### 3.1. Preprare model evaluation function (10 pts)\n\n\n- Before modeling, let's prepare two functions\n  - `extract_cv_results`: to return the score and best param from hyperparameter search\n  - `evaluate_model`: to return the RMSE of a model\n\n::: {#a97e0fbc .cell execution_count=51}\n``` {.python .cell-code}\n####################################################\n# First, create a function to extract the CV results\n# - The function name is extract_cv_results\n# - It needs one input, called by `cv_obj` (a GridSearchCV sklearn object)\n# - It returns three output:\n#   1. the CV score on train set (float)\n#   2. the CV score on valid set (float)\n#   3. The best params (dictionary)\n#\n#\n# Next, create a function to evaluate model called `rmse`\n# - It needs 2 input\n#   - y_actual, the actual output (pd.DataFrame or numpy array)\n#   - y_pred, the predicted output (pd.DataFrame or numpy array)\n# - You calculate the model performance using root mean squared error metrics\n# - Then return the rmse\n#\n# Write your code here\n####################################################\nfrom sklearn.metrics import root_mean_squared_error\n\n\ndef extract_cv_results(cv_obj):\n  train_score = -cv_obj.cv_results_['mean_train_score'][cv_obj.best_index_]\n  valid_score = -cv_obj.best_score_\n  best_params = cv_obj.best_params_\n  return train_score, valid_score, best_params\n\n\ndef rmse(y_actual, y_pred):\n    return np.sqrt(np.mean((y_actual - y_pred) ** 2))\n```\n:::\n\n\n#### 3.2. Train and Cross Validate Several Models (23 pts)\n\n\n- Now, let's train & evaluate several models\n- You should check, which one of the following model is the best model\n\n  1. Baseline model (**3 pts**)\n  2. k-NN (**3 pts**)\n  3. Linear Regression (**4 pts**)\n  4. Decision Tree (**4 pts**)\n  5. Ridge (**4 pts**)\n  6. Lasso (**4 pts**)\n\n- We're going to perform a `GridSearchCV`, with\n  - number of CV = 10\n  - scoring = root mean squared error\n  - return the train score\n\n::: {#d71dc294 .cell execution_count=52}\n``` {.python .cell-code}\n####################################################\n# Import sklearn library of those six models + gridsearchcv\n# Write your code here\n# This is 1 pts\n####################################################\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\n```\n:::\n\n\n##### Perform CV for baseline model (3 pts)\n- Return as `reg_base`\n\n::: {#ca31c982 .cell execution_count=53}\n``` {.python .cell-code}\n####################################################\n# Perform GridSearchCV for Baseline model\n# return the results as reg_base\n# Write your code here\n####################################################\ndummy = DummyRegressor()\nparam_grid = {\n               'strategy': ['mean', 'median']\n             }\n\nreg_base = GridSearchCV(estimator=dummy, param_grid=param_grid, cv=5, scoring='neg_root_mean_squared_error', return_train_score=True)\n\nreg_base.fit(X = X_train_clean, y =y_train_dropped)\n```\n:::\n\n\n::: {#0808a470 .cell execution_count=54}\n``` {.python .cell-code}\n# Validate the CV Score (JUST RUN THE CODE)\ntrain_base, valid_base, best_param_base = extract_cv_results(reg_base)\n\nprint(f'Train score - Baseline model: {train_base/(10**6):.2f} Juta')\nprint(f'Valid score - Baseline model: {valid_base/(10**6):.2f} Juta')\nprint(f'Best Params - Baseline model: {best_param_base}')\n```\n:::\n\n\n##### Perform CV for k-NN Model (3 pts)\n- Do a parameter search for `k = {1, 10, 25, 50, 100, 150, 200, 250}`\n- Return as `reg_knn`\n\n::: {#cd5fb0ed .cell execution_count=55}\n``` {.python .cell-code}\n####################################################\n# Perform GridSearchCV for kNN model\n# return the results as reg_knn\n# Write your code here\n####################################################\nknn = KNeighborsRegressor()\nparam_grid = {\n               'n_neighbors' : [1, 10, 25, 50, 100, 150, 200, 250]\n             }\n\nreg_knn = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='neg_root_mean_squared_error', return_train_score=True)\n\nreg_knn.fit(X = X_train_clean, y = y_train_dropped)\n```\n:::\n\n\n::: {#98176b22 .cell execution_count=56}\n``` {.python .cell-code}\n# Validate the CV Score (JUST RUN THE CODE)\ntrain_knn, valid_knn, best_param_knn = extract_cv_results(reg_knn)\n\nprint(f'Train score - kNN model: {train_knn/(10**6):.2f} Juta')\nprint(f'Valid score - kNN model: {valid_knn/(10**6):.2f} Juta')\nprint(f'Best Params - kNN model: {best_param_knn}')\n```\n:::\n\n\n##### Perform CV for Linear Regression Model (4 pts)\n- Return as `reg_lr`\n\n::: {#b582ccbf .cell execution_count=57}\n``` {.python .cell-code}\n####################################################\n# Perform GridSearchCV for Linear Regression model\n# return the results as reg_lr\n# Write your code here\n####################################################\nlinear= LinearRegression()\nparam_grid = {\n    'fit_intercept': [True, False],\n    'copy_X': [True, False],\n    'n_jobs': [None, -1]\n}\nreg_lr = GridSearchCV(estimator=linear, param_grid=param_grid, cv=5, scoring='neg_root_mean_squared_error', return_train_score=True)\n\nreg_lr.fit(X = X_train_clean, y = y_train_dropped)\n```\n:::\n\n\n::: {#5e4fda71 .cell execution_count=58}\n``` {.python .cell-code}\n# Validate the CV Score (JUST RUN THE CODE)\ntrain_lr, valid_lr, best_param_lr = extract_cv_results(reg_lr)\n\nprint(f'Train score - LinReg model: {train_lr/(10**6):.2f} Juta')\nprint(f'Valid score - LinReg model: {valid_lr/(10**6):.2f} Juta')\nprint(f'Best Params - LinReg model: {best_param_lr}')\n```\n:::\n\n\n##### Perform CV for Decision Tree Model (4 pts)\n- You search the best hyperparameter from\n  - maximum depth : 2, 10, 30, 100, None\n  - minimum samples required to split : 2, 25, 50, 100, 150\n  - minimum samples at leaf : 2, 5, 10, 20\n- Return as `reg_dt`\n\n::: {#4c64e8c6 .cell execution_count=59}\n``` {.python .cell-code}\n####################################################\n# Perform GridSearchCV for Decision Tree model\n# return the results as reg_dt\n# Write your code here\n####################################################\ndtree = DecisionTreeRegressor()\nparam_grid = {'max_depth' : [2, 10, 30, 100, None],\n              'min_samples_split' : [2, 25, 50, 100, 150],\n              'min_samples_leaf': [2, 5, 10, 20]}\n\nreg_dt = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=5, scoring='neg_root_mean_squared_error', return_train_score=True)\n\nreg_dt.fit(X = X_train_clean, y = y_train_dropped)\n```\n:::\n\n\n::: {#f6fad3ed .cell execution_count=60}\n``` {.python .cell-code}\n# Validate the CV Score (JUST RUN THE CODE)\ntrain_dt, valid_dt, best_param_dt = extract_cv_results(reg_dt)\n\nprint(f'Train score - Decision Tree model: {train_dt/(10**6):.2f} Juta')\nprint(f'Valid score - Decision Tree model: {valid_dt/(10**6):.2f} Juta')\nprint(f'Best Params - Decision Tree model: {best_param_dt}')\n```\n:::\n\n\n##### Perform CV for Ridge Model (4 pts)\n- You search the best hyperparameter from\n  - regularization strength: [$10^{-6}$, ..., $10^{10}$],\n- Return as `reg_ridge`\n\n::: {#054fad2d .cell execution_count=61}\n``` {.python .cell-code}\n####################################################\n# Perform GridSearchCV for Ridge model\n# return the results as reg_ridge\n# Write your code here\n####################################################\nridge = Ridge()\nparam_grid = { 'alpha': np.logspace(-6, 10, num=17)}\n\n\nreg_ridge = GridSearchCV(estimator = ridge, param_grid=param_grid, cv=5, scoring='neg_root_mean_squared_error', return_train_score=True)\n\nreg_ridge.fit(X = X_train_clean, y = y_train_dropped)\n```\n:::\n\n\n::: {#2bb44d5b .cell execution_count=62}\n``` {.python .cell-code}\n# Validate the CV Score (JUST RUN THE CODE)\ntrain_ridge, valid_ridge, best_param_ridge = extract_cv_results(reg_ridge)\n\nprint(f'Train score - Ridge model: {train_ridge/(10**6):.2f} Juta')\nprint(f'Valid score - Ridge model: {valid_ridge/(10**6):.2f} Juta')\nprint(f'Best Params - Ridge model: {best_param_ridge}')\n```\n:::\n\n\n##### Perform CV for Lasso Model (4 pts)\n- You search the best hyperparameter from\n  - regularization strength: [$10^{-6}$, ..., $10^{10}$]\n- Return as `reg_lasso`\n\n::: {#929ef022 .cell execution_count=63}\n``` {.python .cell-code}\n####################################################\n# Perform GridSearchCV for Lasso model\n# return the results as reg_lasso\n# Write your code here\n####################################################\nlasso = Lasso()\nparam_grid = { 'alpha': np.logspace(-6, 10, num=17)}\n\n\nreg_lasso = GridSearchCV(estimator = lasso, param_grid=param_grid, cv=5, scoring='neg_root_mean_squared_error', return_train_score=True)\n\nreg_lasso.fit(X = X_train_clean, y = y_train_dropped)\n```\n:::\n\n\n::: {#270a7849 .cell execution_count=64}\n``` {.python .cell-code}\n# Validate the CV Score (JUST RUN THE CODE)\ntrain_lasso, valid_lasso, best_param_lasso = extract_cv_results(reg_lasso)\n\nprint(f'Train score - Lasso model: {train_lasso/(10**6):.2f} Juta')\nprint(f'Valid score - Lasso model: {valid_lasso/(10**6):.2f} Juta')\nprint(f'Best Params - Lasso model: {best_param_lasso}')\n```\n:::\n\n\n#### 3.3. Choose the best model (10 pts)\n\n\nLets summarize the model\n\n::: {#8dce8d66 .cell execution_count=65}\n``` {.python .cell-code}\n# Summarize (JUST RUN THE CODE)\nsummary_df = pd.DataFrame(\n    data={\n        'model': ['Baseline', 'kNN', 'Linear Regression', 'Decision Tree', 'Ridge', 'Lasso'],\n        'train_score': [train_base, train_knn, train_lr, train_dt, train_ridge, train_lasso],\n        'valid_score': [valid_base, valid_knn, valid_lr, valid_dt, valid_ridge, valid_lasso],\n        'best_params': [best_param_base, best_param_knn, best_param_lr, best_param_dt, best_param_ridge, best_param_lasso]\n    }\n)\n\nsummary_df['train_score'] /= 10**6\nsummary_df['valid_score'] /= 10**6\nsummary_df\n```\n:::\n\n\nFrom the previous results, which one is the best model? (3 pts)\n\n```\nBerdasarkan Hasil diatas, model terbaik adalah Lasso.\n\n```\n\nWhy do you choose that model? (3 pts)\n\n```\nkarena memiliki nilai valid_score yang rendah, trains_score yang cukup baik dan tidak overfit.\n```\n\nAnd, create a `reg_best` to store the best model\n\n::: {#beb3521e .cell execution_count=66}\n``` {.python .cell-code}\n#####################################################################\n# Recreate or retrain your best regression model\n# Set is as reg_best\n# Write your code in here (4 pts)\n#####################################################################\nparam_grid = { 'alpha': np.logspace(-6, 10, num=17)}\n\nreg_best = GridSearchCV(estimator = lasso, param_grid=param_grid, cv=5, scoring='neg_root_mean_squared_error', return_train_score=True)\n\nreg_best.fit(X = X_train_clean, y = y_train_dropped)\n```\n:::\n\n\n### 4. Predictions & Evaluations (JUST RUN THE CODE)\n\n\n```\n4.1 Predict & Evaluate on the Train Data\n4.2 Predict & Evaluate on the Test Data\n```\n\n#### 4.1. Predict & evaluate on train & valid data\n\n::: {#3adcf344 .cell execution_count=67}\n``` {.python .cell-code}\n# Predict (JUST RUN THE CODE)\ny_train_pred = reg_best.predict(X_train_clean)\n```\n:::\n\n\n::: {#e74fedf0 .cell execution_count=68}\n``` {.python .cell-code}\n# Find error (JUST RUN THE CODE)\ntrain_error = rmse(y_train_dropped, y_train_pred)\nprint(f'RMSE on Train data: {train_error/10**6:.2f} Juta')\n```\n:::\n\n\n::: {#ad22f7b7 .cell execution_count=69}\n``` {.python .cell-code}\n# Visualize & compare the prediction (JUST RUN THE CODE)\nplt.scatter(y_train_dropped/1e6, y_train_pred/1e6)\n\nplt.plot([0, 600], [0, 600], c='red')\nplt.xlim(0, 600); plt.ylim(0, 600)\nplt.xlabel('y actual [Juta Rupiah]'); plt.ylabel('y predicted [Juta Rupiah]')\nplt.title('Comparison of y actual vs y predicted on Train Data')\nplt.show()\n```\n:::\n\n\n::: {#c467bc1f .cell execution_count=70}\n``` {.python .cell-code}\n# Predict (JUST RUN THE CODE)\ny_valid_pred = reg_best.predict(X_valid_clean)\n```\n:::\n\n\n::: {#9a274bb2 .cell execution_count=71}\n``` {.python .cell-code}\n# Find error (JUST RUN THE CODE)\nvalid_error = rmse(y_valid, y_valid_pred)\nprint(f'RMSE on Valid data: {valid_error/10**6:.2f} Juta')\n```\n:::\n\n\n::: {#ffac4463 .cell execution_count=72}\n``` {.python .cell-code}\n# Visualize & compare the prediction (JUST RUN THE CODE)\nplt.scatter(y_valid/1e6, y_valid_pred/1e6)\n\nplt.plot([0, 600], [0, 600], c='red')\nplt.xlim(0, 600); plt.ylim(0, 600)\nplt.xlabel('y actual [Juta Rupiah]'); plt.ylabel('y predicted [Juta Rupiah]')\nplt.title('Comparison of y actual vs y predicted on Valid Data')\nplt.show()\n```\n:::\n\n\n#### 4.2. Predict & evaluate on test data\n\n::: {#124d2823 .cell execution_count=73}\n``` {.python .cell-code}\n# Predict (JUST RUN THE CODE)\ny_test_pred = reg_best.predict(X_test_clean)\n```\n:::\n\n\n::: {#ffdeba2e .cell execution_count=74}\n``` {.python .cell-code}\n# Final generalization\ntest_error = rmse(y_test, y_test_pred)\nprint(f'RMSE on Test data: {test_error/10**6:.2f} Juta')\n```\n:::\n\n\n::: {#f90875d4 .cell execution_count=75}\n``` {.python .cell-code}\n# Visualize & compare the prediction (JUST RUN THE CODE)\nplt.scatter(y_test/1e6, y_test_pred/1e6)\n\nplt.plot([0, 600], [0, 600], c='red')\nplt.xlim(0, 600); plt.ylim(0, 600)\nplt.xlabel('y actual [Juta Rupiah]'); plt.ylabel('y predicted [Juta Rupiah]')\nplt.title('Comparison of y actual vs y predicted on Test Data')\nplt.show()\n```\n:::\n\n\n### 5. Explore and generate a better model! (30)\n\n\n- Please explore by yourself!\n- Your **task** is to create a better model from the previous results.\n\n::: {#5790d76d .cell execution_count=76}\n``` {.python .cell-code}\n#########################################\n# Write your code here\n# Feel free to explore\n#########################################\n\n#Transformasi akar y pada variabel dependen\ny_train_dropped = np.sqrt(y_train_dropped)\n\nlasso_2 = Lasso()\nparam_grid = { 'alpha': np.logspace(-6, 10, num=17)}\n\n\nreg_explore = GridSearchCV(estimator = lasso_2, param_grid=param_grid, cv=5, scoring='neg_root_mean_squared_error', return_train_score=True)\n\nreg_explore.fit(X = X_train_clean, y = y_train_dropped)\n```\n:::\n\n\n::: {#cd8add97 .cell execution_count=77}\n``` {.python .cell-code}\ntrain_lasso2, valid_lasso2, best_param_lasso2 = extract_cv_results(reg_explore)\n\nprint(f'Train score - Lasso model: {(train_lasso2)**2/(10**6):.2f} Juta')\nprint(f'Valid score - Lasso model: {(valid_lasso2)**2/(10**6):.2f} Juta')\nprint(f'Best Params - Lasso model: {best_param_lasso2}')\n```\n:::\n\n\n::: {#2bede82c .cell execution_count=78}\n``` {.python .cell-code}\ny_train_pred = reg_explore.predict(X_train_clean)\n```\n:::\n\n\n::: {#6507cb8e .cell execution_count=79}\n``` {.python .cell-code}\ntrain_error2 = rmse((y_train_dropped)**2, (y_train_pred)**2)\nprint(f'RMSE on Train data: {train_error/10**6:.2f} Juta')\n```\n:::\n\n\n::: {#8717f27e .cell execution_count=80}\n``` {.python .cell-code}\ny_valid_pred2 = reg_explore.predict(X_valid_clean)**2\n```\n:::\n\n\n::: {#f4372648 .cell execution_count=81}\n``` {.python .cell-code}\nvalid_error2 = rmse(y_valid, y_valid_pred2)\nprint(f'RMSE on Valid data: {valid_error2/10**6:.2f} Juta')\n```\n:::\n\n\n::: {#093a629c .cell execution_count=82}\n``` {.python .cell-code}\ny_test_pred2 = reg_explore.predict(X_test_clean)**2\n```\n:::\n\n\n::: {#622fef36 .cell execution_count=83}\n``` {.python .cell-code}\n# Final generalization\ntest_error2 = rmse(y_test, y_test_pred2)\nprint(f'RMSE on Test data: {test_error2/10**6:.2f} Juta')\n```\n:::\n\n\n::: {#89a8b20c .cell execution_count=84}\n``` {.python .cell-code}\nperbandingan = pd.DataFrame( data = {'model' : ['reg_best', 'reg_explore'],\n                                     'train_RMSE': [train_error, train_error2],\n                                     'valid_RMSE': [valid_error, valid_error2],\n                                     'test_RMSE':[test_error, test_error2]})\n\nperbandingan['train_RMSE'] /= 10**6\nperbandingan['valid_RMSE'] /= 10**6\nperbandingan['test_RMSE'] /= 10**6\n\nperbandingan\n```\n:::\n\n\n",
    "supporting": [
      "Daffa_Mentoring_02_files"
    ],
    "filters": [],
    "includes": {}
  }
}