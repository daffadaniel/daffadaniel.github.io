---
title: Mentoring 3
date: "2024-06-28"
categories: [ML, Portofolio]
toc: true
toc-depth: 4
jupyter: python3
execute:
  eval: false
  freeze: true
---

# Dataset Description

Dataset Link: https://www.kaggle.com/datasets/laotse/credit-risk-dataset

Dataset berisi data terkait peminjam uang.
Data ini memiliki 12 variabel dengan `loan_status` sebagai variabel dependen (output) dan sisanya sebagai variabel independen (input).

Berikut adalah deskripsi mengenai arti dari setiap kolom pada data:

<div style="text-align: center;">

|Feature Name|	Description|
|:--|:--|
|person_age	|Age|
|person_income|	Annual Income|
|person_home_ownership|	Home ownership|
|person_emp_length|	Employment length (in years)|
|loan_intent|	Loan intent|
|loan_grade|	Expected Risk Grade|
|loan_amnt|	Loan amount|
|loan_int_rate|	Interest rate|
|loan_status|	0 : non default, 1: default|
|loan_percent_income|	Percent income|
|cb_person_default_on_file|	Historical default|
|cb_preson_cred_hist_length|	Credit history length|

</div>

Catatan:
- Tidak terdapat keterangan mengenai mata uang yang digunakan pada `person_income` dan `loan_amnt`
- `loan_grade` adalah klasifikasi ekspektasi risiko pemberian pinjaman, yaitu A sampai G untuk pinjaman dengan risiko default (gagal bayar) yang rendah hingga tinggi. (sumber:https://blog.groundfloor.com/groundfloorblog/about-loan-grading)

# Modeling Workflow

Tujuan: Membuat model Classifier (default, non default) untuk meminimalkan potensi kerugian.

## Task 1: Data Preparation

```{python}
import pandas as pd
import numpy as np
```

### Load the Data

```{python}
def read_data(fname):
  filename = '/content/' + fname

  # read csv as pandas dataframe
  dataset = pd.read_csv(filename)
  dataset['id'] = dataset.index
  dataset.set_index('id', inplace = True)
  print('Data shape raw               : ', dataset.shape)

  # drop duplicates
  print('Number of duplicates         : ', dataset.duplicated().sum())

  dataset.drop_duplicates(keep = 'last', inplace = True)

  # print data shape
  print('Data shape after dropping    : ', dataset.shape)

  return dataset
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
df = read_data(fname= 'credit_risk_dataset.csv')
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 258}
df.head()
```

### Data Splitting

```{python}
from sklearn.model_selection import train_test_split
```

```{python}
# Splitting the input and output columns

def split_input_output(data, target_col):
  X = data.drop((target_col), axis = 1)
  y = data[target_col]
  print("X shape: " +str(X.shape))
  print("y shape: " + str(y.shape))
  return X, y
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
X, y = split_input_output(data=df,
                          target_col='loan_status')
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 258}
X.head()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 272}
y.head()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 178}
y.value_counts(normalize = True)
```

Variabel y memiliki kelas yang imbalance sehingga perlu dilakukan stratified splitting pada train-test split.

```{python}
# Train test split
def split_train_test(X,y, test_size = 0.2, seed = 123):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, stratify = y, random_state=seed)
  print("X train shape  : ", X_train.shape)
  print("y train shape  : ", y_train.shape)
  print("X test shape   : ", X_test.shape)
  print("y test shape   : ", y_test.shape)
  print("\n")

  return X_train, X_test, y_train, y_test
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
# Training set
X_train, X_test, y_train, y_test = split_train_test(X,y)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
print(len(X_train)/len(X))
print(len(X_test)/len(X))
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
# Memastikan data test dan train memiliki proporsi kelas yang sama
print(y_train.value_counts(normalize= True))
print(y_test.value_counts(normalize= True))
```

### EDA

```{python}
import matplotlib.pyplot as plt
import seaborn as sns
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
X_train.info()
```

Keterangan:
- Kolom `person_home_ownership`, `loan_intent`, `loan_grade`, dan  `cb_person_default_on_file` bertipe kategorik.
- Terdapat missing value pada `person_emp_length`(numerik) dan `loan_int_rate` (numerik)

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 429}
X_train.isna().sum()
```

```{python}
# Lakukan splitting Variabel x numerik dengan kategorik
def split_num_cat(data, num_cols, cat_cols):
  data_num = data[num_cols]
  data_cat = data[cat_cols]
  print("Numeric Data shape: "+ str(data_num.shape))
  print("Categoric Data shape: "+ str(data_cat.shape))

  return data_num, data_cat
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
num_columns = ['person_age', 'person_income', 'person_emp_length',
                'loan_amnt', 'loan_int_rate', 'loan_percent_income',
                'cb_person_cred_hist_length' ]

cat_columns = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']

X_train_num, X_train_cat = split_num_cat(X_train, num_columns, cat_columns)
```

#### Statistik Deskriptif Data Numerik

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 269}
X_train_num.describe().T
```

Keterangan:
- Terdapat indikasi outlier pada kolom `person_age`, `person_income`, dan  `person_emp_length`.
- Kolom numerik memiliki distribusi nilai yang sangat berbeda satu sama lain.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 1000}
# Membuat subplot
fig, axes = plt.subplots(len(num_columns), 2, figsize=(12, 16))

# Membuat plot untuk setiap kolom numerik
for i, column in enumerate(num_columns):
   # Histogram dengan KDE
   sns.histplot(X_train_num[column], kde=True, ax=axes[i, 0], color='skyblue')
   axes[i, 0].set_title(f'Histogram of {column}')
   axes[i, 0].set_xlabel(column)
   axes[i, 0].set_ylabel('Frequency')

   # Boxplot
   sns.boxplot(x=X_train_num[column], ax=axes[i, 1], color='lightgreen')
   axes[i, 1].set_title(f'Boxplot of {column}')
   axes[i, 1].set_xlabel(column)

plt.tight_layout()
plt.show()
```

`person_age`, `person_income`, dan `person_emp_length` memiliki anomali

#### Pemeriksaan Anomali Data Numerik

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 175}
X_train_num[X_train_num['person_age']> 100]
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 238}
X_train_num[ X_train_num['person_income'] >= 1_000_000]
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 143}
X_train_num[X_train_num['person_emp_length'] > 60]
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 455}
X_train_num[X_train_num['person_emp_length'] == 0]
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
X_train_num[X_train_num['person_emp_length'] == 0].shape
```

Cukup banyak baris yang memiliki kolom `person_emp_length` = 0, saya akan berasumsi bahwa hal ini berarti objek observasi tidak pernah bekerja untuk orang lain tetapi memiliki penghasilan misalnya pengusaha dan bukanlah sebuah anomali pada data.  

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 238}
X_train_num[X_train_num['loan_percent_income'] == 0]
```

#### Pengecekan Data Kategorik

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 241}
X_train_cat['person_home_ownership'].value_counts()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 304}
X_train_cat['loan_intent'].value_counts()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 335}
X_train_cat['loan_grade'].value_counts()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 178}
X_train_cat['cb_person_default_on_file'].value_counts()
```

Kolom `cb_person_default_on_file` perlu diubah menjadi boolean

#### Data Preprocessing Plan:
- Hapus data anomali pada kolom numerik.
- Handle missing value pada `person_emp_length` dan `loan_int_rate` dengan imputasi nilai median
- Lakukan Standarisasi pada kolom numerik.
- Ubah `cb_person_default_on_file` menjadi boolean
- Lakukan ordinal encoding pada kolom `loan_grade`
- lakukan one hot encoding pada `person_home_ownership` dan `loan_intent`

### Data Preprocessing

```{python}
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import OrdinalEncoder
```

####  Penanganan Anomali pada Data

```{python}
age_anomaly = X_train_num[X_train_num['person_age']> 100].index.tolist()

income_anomaly = X_train_num[X_train_num['person_income'] >= 1_000_000].index.tolist()

emp_length_anomaly = X_train_num[X_train_num['person_emp_length'] > 60].index.tolist()

idx_to_drop = set(emp_length_anomaly + income_anomaly + age_anomaly)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
print(f'Number of index to drop:', len(idx_to_drop))
idx_to_drop
```

```{python}
X_train_num_dropped = X_train_num.drop(index = idx_to_drop)
y_train_dropped = y_train.drop(index = idx_to_drop)
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 255}
print('Shape of X train after dropped:', X_train_num_dropped.shape)
X_train_num_dropped.head()
```

#### Penanganan Missing Value

```{python}
def num_imputer_fit(data):
  imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')

  imputer.fit(data)

  return imputer

def num_imputer_transform(data, imputer):
  imputer.set_output(transform = "pandas")
  data = imputer.transform(data)
  return data
```

```{python}
# Get the numerical imputer
num_imputer = num_imputer_fit(X_train_num_dropped)

# Transform the data
X_train_imputed = num_imputer_transform(X_train_num_dropped, num_imputer)
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 304}
# Validasi hasil
X_train_imputed.isna().sum()
```

#### Standardisasi

```{python}
def fit_scaler(data):
  scaler = StandardScaler()
  scaler.fit(data)
  return scaler

def transform_scaler(data, scaler):
  scaler.set_output(transform = 'pandas')
  data = scaler.transform(data)
  return data
```

```{python}
scaler = fit_scaler(X_train_imputed)

X_train_num_scaled = transform_scaler(X_train_imputed, scaler)
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 238}
X_train_num_scaled.head()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 300}
# Validasi hasil
X_train_num_scaled.describe().round(4)
```

#### Penanganan Inkonsistensi Format Data Kategorik

```{python}
#| colab: {base_uri: https://localhost:8080/}
X_train_cat.loc[:,'cb_person_default_on_file'] = (X_train_cat['cb_person_default_on_file'].replace({'N' : 0, 'Y' : 1 }).astype(int))
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 238}
X_train_cat.head()
```

#### Encoding Data Kategorik

```{python}
def cat_OHencode_fit(data):
  OHencoder = OneHotEncoder(sparse_output=False, handle_unknown = 'ignore')
  OHencoder.fit(data[['person_home_ownership',	'loan_intent']])
  return OHencoder

def cat_ORDencode_fit(data):
  ORDencoder = OrdinalEncoder()
  ORDencoder.fit(data[['loan_grade']])
  return ORDencoder


def cat_encoder_transform(data, onehot_encoder, ordinal_encoder):
  OHencoded_data = onehot_encoder.transform(data[['person_home_ownership',	'loan_intent']])
  ORDencoded_data = ordinal_encoder.transform(data[['loan_grade']])

  # simpan index 'id' sebagai kolom
  df_reset = data.reset_index()

  # Ubah data menjadi Dataframe
  df_OHencoded = pd.DataFrame(OHencoded_data, columns= onehot_encoder.get_feature_names_out(['person_home_ownership',	'loan_intent']))
  df_ORDencoded = pd.DataFrame(ORDencoded_data, columns=['loan_grade'])

  # Gabungkan sesuai index
  df_encoded_combined_indexed = pd.concat([df_reset.drop(columns=['person_home_ownership',	'loan_intent','loan_grade']),df_OHencoded, df_ORDencoded], axis=1)
  result_df = df_encoded_combined_indexed.set_index('id')

  return result_df
```

```{python}
# Perform categorical imputation
cat_OHencoder = cat_OHencode_fit(X_train_cat)
cat_ORDencoder = cat_ORDencode_fit(X_train_cat)

# Transform
X_train_cat_encoded = cat_encoder_transform(X_train_cat, cat_OHencoder, cat_ORDencoder)
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 475}
X_train_cat_encoded
```

#### Penggabungan Data Numerik dan Kategorik

```{python}
def concat_data(num_data, cat_data):
  print('\nCleaned Numerical data shape: ' + str(num_data.shape))
  print('Cleaned Categorical data shape: ' + str(cat_data.shape))

  concated_data = num_data.join(cat_data, how = 'inner')
  print('Concated data shape: ' + str(concated_data.shape))

  return concated_data
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
X_train_concat = concat_data(X_train_num_scaled ,X_train_cat_encoded)
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 475}
X_train_concat
```

#### Membuat Fungsi Preprocessing

```{python}
def preprocess_data(data,num_cols, cat_cols,  num_imputer, scaler, cat_onehot_encoder, cat_ordinal_encoder):

  #Splitting Numerik dan kategorik
  X_num, X_cat = split_num_cat(data, num_cols, cat_cols)

  #Penanganan Missing Value
  X_num_imputed = num_imputer_transform(X_num, num_imputer)

  #Standardisasi
  X_train_num_clean = transform_scaler(X_num_imputed, scaler)

  #Penganganan Inkonsistensi
  X_cat.loc[:,'cb_person_default_on_file'] = (X_cat['cb_person_default_on_file'].replace({'N' : 0, 'Y' : 1 }).astype(int))

  #Encoding Data Kategorik
  X_cat_encoded = cat_encoder_transform(X_cat, cat_onehot_encoder, cat_ordinal_encoder)

  #Gabungkan Data
  cleaned_data = concat_data(X_train_num_clean,X_cat_encoded)

  return cleaned_data
```

## Task 2:  Modeling

```{python}
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.dummy import DummyClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier
from scipy.stats import randint,uniform
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
# Preprocessing Data Test
X_test_clean = preprocess_data(X_test,num_columns, cat_columns,  num_imputer, scaler, cat_OHencoder, cat_ORDencoder)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
y_test.shape
```

### Metrics


Untuk meminimalkan kejadian **False Negative** akan digunakan `Recall` sebagai evaluation metric utama.

### Baseline Model

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 80}
dummy_clf = dummy_clf = DummyClassifier(strategy = "most_frequent")
dummy_clf.fit(X = X_train_concat,
              y = y_train_dropped)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
y_pred_dummy = dummy_clf.predict(X_train_concat)

report_dummy_model = classification_report(y_train_dropped, y_pred_dummy)

print(report_dummy_model)
```

### Best Model Search

#### kNN Classifier Model

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 166}
knn_clf = KNeighborsClassifier()

param_grid = {'n_neighbors': range(1, 21), 'metric': ['euclidean', 'manhattan']}
grid_search = GridSearchCV(knn_clf, param_grid, cv=5,scoring='recall')
grid_search.fit(X_train_concat, y_train_dropped)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
print("Best Parameters:", grid_search.best_params_)
print("Mean cross-validated score of the best_estimator:", grid_search.best_score_)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
knn_best_model = grid_search.best_estimator_
y_pred_knn = knn_best_model.predict(X_test_clean)

report_knn_model = classification_report(y_test, y_pred_knn)
print(report_knn_model)
```

#### Decision Tree

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 166}
#Decision Tree
dtree_clf = DecisionTreeClassifier(random_state = 42)

param_distributions = {
    'max_depth': np.arange(1, 50),
    'min_samples_split': np.arange(2, 20),
    'min_samples_leaf': np.arange(1, 20),
    'max_features': [None, 'sqrt', 'log2'],
    'criterion': ['gini', 'entropy']
}

randomcv_dtree = RandomizedSearchCV(
    estimator=dtree_clf,
    param_distributions=param_distributions,
    n_iter=50,
    cv=5,
    scoring = "recall",
    random_state=42,
    n_jobs=-1
)

randomcv_dtree.fit(X = X_train_concat,
              y = y_train_dropped)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
# Menampilkan Hasil Hyperparameter Terbaik
print("Best Parameters:", randomcv_dtree.best_params_)
print("Mean cross-validated score of the best_estimator:", randomcv_dtree.best_score_)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
DT_best_model = randomcv_dtree.best_estimator_
y_pred_DT = DT_best_model.predict(X_test_clean)

report_DT_model = classification_report(y_test, y_pred_DT)
print(report_DT_model)
```

#### Logistic Regresion

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 1000}
#Logistic Regression
logreg_clf = LogisticRegression()

param_grid = {'max_iter': [100, 500, 1000],
              'penalty' : [ 'l1', 'l2',None],
              'solver' : ['liblinear', 'saga'],
              'C': [0.01, 0.1, 1, 10, 100]
              }

gridcv_logreg = GridSearchCV(estimator= logreg_clf, param_grid=param_grid, cv=5, scoring='recall')

gridcv_logreg.fit(X = X_train_concat,
              y = y_train_dropped)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
print("Best Parameters:", gridcv_logreg.best_params_)
print("Mean cross-validated score of the best_estimator:", gridcv_logreg.best_score_)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
logreg_best_model = gridcv_logreg.best_estimator_
y_pred_logreg = logreg_best_model.predict(X_test_clean)

report_logreg_model = classification_report(y_test, y_pred_logreg)
print(report_logreg_model)
```

#### Support Vector Classifier Model

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 183}
svc = SVC()

param_grid = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto'],
    'degree': [2, 3, 4]
}

randomcv_svc = RandomizedSearchCV(
    estimator=svc,
    param_distributions=param_grid,
    scoring='recall',
    cv=5,
    verbose=1,
    n_jobs=-1
)

randomcv_svc.fit(X = X_train_concat,
              y = y_train_dropped)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
print("Best Parameters:", randomcv_svc.best_params_)
print("Mean cross-validated score of the best_estimator:", randomcv_svc.best_score_)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
y_pred_svc = grid_search.best_estimator_.predict(X_test_clean)
print("\nClassification Report:\n", classification_report(y_test, y_pred_svc))
```

#### Bagging Decision Tree

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 213}

tree = DecisionTreeClassifier(random_state=123)
bagging_model = BaggingClassifier(estimator= tree, random_state=123)

# Parameter grid
param_distributions = {
    'n_estimators': randint(10, 100),
    'max_samples': [0.5, 0.7, 1.0],
    'max_features': [0.5, 0.7, 1.0],
    'bootstrap': [True, False],
    'bootstrap_features': [True, False],
    'estimator__max_depth': randint(5, 30),
    'estimator__min_samples_split': randint(2, 20),
}

# RandomizedSearchCV
random_search_bagging = RandomizedSearchCV(
    estimator=bagging_model,
    param_distributions=param_distributions,
    n_iter=20,
    cv=5,
    scoring='recall',
    n_jobs=-1,
    random_state=123
)

random_search_bagging.fit(X = X_train_concat, y = y_train_dropped)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
print("Best Parameters:", random_search_bagging.best_params_)
print("Mean cross-validated score of the best_estimator:", random_search_bagging.best_score_)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
bagging_best_model = random_search_bagging.best_estimator_
y_pred_bagging = bagging_best_model.predict(X_test_clean)

report_bagging_model = classification_report(y_test, y_pred_bagging)
print(report_bagging_model)
```

#### Random Forest

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 166}
rf_clf = RandomForestClassifier(random_state=42)

param_dist = {
    'n_estimators': randint(50, 200),  # Jumlah trees antara 50-200
    'max_depth': [None, 10, 20, 30],  # Kedalaman maksimal tree
    'min_samples_split': randint(2, 10),  # Minimal sampel untuk split
    'min_samples_leaf': randint(1, 5),   # Minimal sampel di leaf node
    'criterion': ['gini', 'entropy'],    # Fungsi untuk split
}

randomcv_rf = RandomizedSearchCV(rf_clf, param_distributions=param_dist, n_iter=20, cv=5, scoring='recall', random_state = 42)
randomcv_rf.fit(X_train_concat, y_train_dropped)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
print("Best Parameters:", randomcv_rf.best_params_)
print("Best Cross-Validated recall:", randomcv_rf.best_score_)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
rf_best_model = randomcv_rf.best_estimator_
y_pred_rf = rf_best_model.predict(X_test_clean)


report_rf_model = classification_report(y_test, y_pred_rf)
print(report_rf_model)
```

#### Adaboost Model

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 213}

tree = DecisionTreeClassifier(random_state=123)
adaboost_model = AdaBoostClassifier(estimator= tree, random_state=123)

param_distributions = {
    'n_estimators': randint(50, 200),
    'learning_rate': uniform(0.01, 1.0),
    'estimator__max_depth': randint(1, 5),
    'estimator__min_samples_split': randint(2, 20),
    'estimator__min_samples_leaf': randint(1, 10),
}

random_search_adaboost = RandomizedSearchCV(
    estimator=adaboost_model,
    param_distributions=param_distributions,
    n_iter=20,
    cv=5,
    scoring='recall',
    n_jobs=-1,
    random_state=123
)


random_search_adaboost.fit(X_train_concat, y_train_dropped)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
print("Best Parameters:", random_search_adaboost.best_params_)
print("Best Cross-Validated recall:", random_search_adaboost.best_score_)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
adaboost_best_model = random_search_adaboost.best_estimator_
y_pred_adaboost = adaboost_best_model.predict(X_test_clean)

report_adaboost = classification_report(y_test, y_pred_adaboost)
print(report_adaboost)
```

### Final Best Model

```{python}
# Best Model: Decision Tree
best_model = randomcv_dtree.best_estimator_
```

## Task 3: Model Evaluation

### Score on test data

```{python}
#| colab: {base_uri: https://localhost:8080/}
y_pred_dt = best_model.predict(X_test_clean)
report_dt = classification_report(y_test, y_pred_dt)
print(report_dt)
```

### Financial Impact Comparison

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 300}
# False Negative potential loss : 35juta
# False positive potential loss : 10 juta

y_pred_dummy = dummy_clf.predict(X_test)

model_predictions = {
    'dummy': y_pred_dummy,
    'knn': y_pred_knn,
    'dt': y_pred_dt,
    'logreg': y_pred_logreg,
    'svc': y_pred_svc,
    'bagging': y_pred_bagging,
    'rf': y_pred_rf,
    'boost': y_pred_adaboost
}

confusion_matrices = {}

for model_name, y_pred in model_predictions.items():
    confusion_matrices[model_name] = confusion_matrix(y_test, y_pred).ravel()

#Financial Loss
cost_fn = 35  # dalam juta
cost_fp = 10

financial_losses = {}

# Hitung Total Loss
for model_name, cm_values in confusion_matrices.items():
    fn, fp = cm_values[2], cm_values[1]  # FN dan FP berada di indeks 2 dan 1
    financial_losses[model_name] = fn * cost_fn + fp * cost_fp


comparison_data = []

# Loop untuk mengisi data financial comparison
for model_name, cm_values in confusion_matrices.items():
    fn, fp = cm_values[2], cm_values[1]  # FN dan FP berada di indeks 2 dan 1
    total_loss = fn * cost_fn + fp * cost_fp
    comparison_data.append({
        'Model': model_name.capitalize(),        # Nama model dengan huruf kapital
        'False Negative (FN)': fn,              # Jumlah FN
        'False Positive (FP)': fp,              # Jumlah FP
        'Total Loss (Rp juta)': total_loss      # Total loss dalam juta
    })

# Buat DataFrame dari data yang terkumpul
financial_comparison = pd.DataFrame(comparison_data)

# Tampilkan DataFrame
financial_comparison
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 694}
financial_comparison_sorted = financial_comparison.sort_values(
    by='Total Loss (Rp juta)',ascending = False)

plt.figure(figsize=(10, 6))
sns.barplot(
    data=financial_comparison_sorted,
    x='Model',
    y='Total Loss (Rp juta)',
    palette='Blues_d'  # Pilih palet warna
)

# Tambahkan label dan judul
plt.title('Financial Loss by Model', fontsize=16)
plt.xlabel('Model', fontsize=12)
plt.ylabel('Total Loss (Rp juta)', fontsize=12)



# Tampilkan plot
plt.tight_layout()
plt.show()
```

Model Bagging memiliki potential loss paling rendah

