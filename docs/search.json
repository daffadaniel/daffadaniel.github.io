[
  {
    "objectID": "posts/05-Churn Modelling/index.html",
    "href": "posts/05-Churn Modelling/index.html",
    "title": "Churn Prediction: 12-week Observation window with 1-Week Gap",
    "section": "",
    "text": "This project predicts whether a customer will churn (stop transacting) in the next 6 weeks based on their past behavior. I used the past 12-week as the observation window and a 1-week gap between observation and prediction windows.\nThe model is designed to simulate real production conditions — no future information is ever used during training.\nDataset: Online Retail II\nThis Project Github: link\n\n\n\n\n\n\n\n\n\n\nComponent\nChoice\nReason\n\n\n\n\nData generation\nSliding window across the entire timeline\n\n\n\nObservation Window\n12 weeks\n~2 median purchase cycles\n\n\nGap\n1 full week (strictly excluded)\nCurrent week is incomplete → cannot be used in features\n\n\nPrediction Window\nNext 6 weeks after gap\nmedian purchase cycles\n\n\nLabel\n0 transactions in PW → churn = 1\nBusiness definition\n\n\nTrain-test split\nStrict time-based (latest period held out as test)\nNo Overlapping period\n\n\n\n\n\n\n\nRecency (weeks since last transaction)\n\nFrequency (number of transactions in 12 weeks)\n\nMonetary value\n\nAverage Order Value (AOV)\n\nCustomer tenure (weeks since first transaction)\n\n\n\n\n\nLogistic Regression\nXgboost\nLightGBM\nCatboost\n\n\n\n\nTo determine the threshold, I evaluate multiple criteria:\n\nF1-maximizing threshold — balances precision and recall (most commonly used in retention campaigns)\n\nClosest point to (0,1) on the ROC curve — geometrically nearest to perfect classification\nThe optimal threshold selected by prioritizing catching more churners (recall) rather than how clean the predictions (precision).\n\nMost of the model final threshold uses the F1-maximizing threshold. This threshold was then fixed and evaluated on a time-based test set (the most recent period, no overlap with training data)\n\n\n\n\n\n\nModel\nAUC\nF1\nRecall\nPrecision\n\n\n\n\nLogistic Regression\n0.565\n0.760\n0.973\n0.635\n\n\nXGBoost\n0.614\n0.754\n0.931\n0.6338\n\n\nCatBoost\n0.620\n0.756\n0.938\n0.633\n\n\nLightGBM\n0.623\n0.759\n0.940\n0.636\n\n\n\n\n\n\n\nPython\n\nPandas\n\nScikit-learn\n\nMatplotlib / Seaborn\nLGBM\nXGBoost\nCatBoost\nQuarto (for documentation)"
  },
  {
    "objectID": "posts/05-Churn Modelling/index.html#project-overview",
    "href": "posts/05-Churn Modelling/index.html#project-overview",
    "title": "Churn Prediction: 12-week Observation window with 1-Week Gap",
    "section": "",
    "text": "This project predicts whether a customer will churn (stop transacting) in the next 6 weeks based on their past behavior. I used the past 12-week as the observation window and a 1-week gap between observation and prediction windows.\nThe model is designed to simulate real production conditions — no future information is ever used during training.\nDataset: Online Retail II\nThis Project Github: link\n\n\n\n\n\n\n\n\n\n\nComponent\nChoice\nReason\n\n\n\n\nData generation\nSliding window across the entire timeline\n\n\n\nObservation Window\n12 weeks\n~2 median purchase cycles\n\n\nGap\n1 full week (strictly excluded)\nCurrent week is incomplete → cannot be used in features\n\n\nPrediction Window\nNext 6 weeks after gap\nmedian purchase cycles\n\n\nLabel\n0 transactions in PW → churn = 1\nBusiness definition\n\n\nTrain-test split\nStrict time-based (latest period held out as test)\nNo Overlapping period\n\n\n\n\n\n\n\nRecency (weeks since last transaction)\n\nFrequency (number of transactions in 12 weeks)\n\nMonetary value\n\nAverage Order Value (AOV)\n\nCustomer tenure (weeks since first transaction)\n\n\n\n\n\nLogistic Regression\nXgboost\nLightGBM\nCatboost\n\n\n\n\nTo determine the threshold, I evaluate multiple criteria:\n\nF1-maximizing threshold — balances precision and recall (most commonly used in retention campaigns)\n\nClosest point to (0,1) on the ROC curve — geometrically nearest to perfect classification\nThe optimal threshold selected by prioritizing catching more churners (recall) rather than how clean the predictions (precision).\n\nMost of the model final threshold uses the F1-maximizing threshold. This threshold was then fixed and evaluated on a time-based test set (the most recent period, no overlap with training data)\n\n\n\n\n\n\nModel\nAUC\nF1\nRecall\nPrecision\n\n\n\n\nLogistic Regression\n0.565\n0.760\n0.973\n0.635\n\n\nXGBoost\n0.614\n0.754\n0.931\n0.6338\n\n\nCatBoost\n0.620\n0.756\n0.938\n0.633\n\n\nLightGBM\n0.623\n0.759\n0.940\n0.636\n\n\n\n\n\n\n\nPython\n\nPandas\n\nScikit-learn\n\nMatplotlib / Seaborn\nLGBM\nXGBoost\nCatBoost\nQuarto (for documentation)"
  },
  {
    "objectID": "posts/05-Churn Modelling/index.html#full-code-details",
    "href": "posts/05-Churn Modelling/index.html#full-code-details",
    "title": "Churn Prediction: 12-week Observation window with 1-Week Gap",
    "section": "Full Code & Details",
    "text": "Full Code & Details\nComplete notebook below.\n\nData Cleaning & EDA Notebook\nI use the same data cleaning and EDA for this project and the cohort analysis project as it uses the same dataset."
  },
  {
    "objectID": "posts/03-Database Design/populating-database.html",
    "href": "posts/03-Database Design/populating-database.html",
    "title": "Designing and Creating Relational Database for Hospital App",
    "section": "",
    "text": "Design and implement a well-structured relational database system for a healthcare environment that covers patient registration, doctor specialization, and appointment management."
  },
  {
    "objectID": "posts/03-Database Design/populating-database.html#mission-statement",
    "href": "posts/03-Database Design/populating-database.html#mission-statement",
    "title": "Designing and Creating Relational Database for Hospital App",
    "section": "",
    "text": "Design and implement a well-structured relational database system for a healthcare environment that covers patient registration, doctor specialization, and appointment management."
  },
  {
    "objectID": "posts/03-Database Design/populating-database.html#creating-table-structures",
    "href": "posts/03-Database Design/populating-database.html#creating-table-structures",
    "title": "Designing and Creating Relational Database for Hospital App",
    "section": "Creating Table Structures",
    "text": "Creating Table Structures\nFrom the mission stated above, let’s breakdown the tables needed for the databases. \nClearly, we need patients,doctors, specializations and appointments as tables. We can also add hospitals to accomodate more than one hospital. \nEach doctor will have it’s own schedule then we can add doctor_schedule as a table. \nSince appointments need to be set at specific time slots for better management, an appointment_slots table is also required. \nAdditionally, We can include medical records to store the diagnosis and treatment of each patient after an appointment.\n\n\n\n\n\n\n\nTable\nDescription\n\n\n\n\npatients\nstores each patient detailed information\n\n\ndoctors\nstores each doctor detailed information\n\n\nappointments\nstores each appointment detailed information\n\n\nspecializations\nstores each specialization detailed information\n\n\nhospitals\nstores hospital detailed information\n\n\ndoctor_schedule\nstores each doctors schedule\n\n\nappointment_slots\nstores appointment slot according to the schedule\n\n\nmedical records\nstores each patient diagnosis and treatment after an appointment\n\n\n\n\nDetermine the Field\n\n\n\n\n\n\n\n\n\n\npatients\nhospitals\nspecializations\ndoctors\ndoctor_schedule\n\n\n\n\npatient_id\nhospital_id\nspecialization_id\ndoctor_id\n\n\n\npatient_name\nhospital_name\nspecialization_name\nhospital_id\ndoctor_id\n\n\npatient_gender\nhospital_address\nspecialization_desc\nspecialization_id\nday_of_the_week\n\n\npatient_birthdate\n\n\ndoctor_name\nstart_time\n\n\npatient_contact_number\n\n\n\nend_time\n\n\n\n\n\n\nappointment_slots\nappointments\nmedical_records\n\n\n\n\nschedule_id\nslot_id\npatient_id\n\n\nschedule_id\nslot_id\ndiagnosis\n\n\nstart_time\nappointment_date\ntreatment\n\n\nend_time\nappointment_status\n\n\n\n\n\n\nDetermine Table Relationship\nHospital-Doctor-Specialization\n \nDoctors-Schedule-Appointments\n \nPatients-Appointments-Records"
  },
  {
    "objectID": "posts/03-Database Design/populating-database.html#database-entity-relationship-diagram",
    "href": "posts/03-Database Design/populating-database.html#database-entity-relationship-diagram",
    "title": "Designing and Creating Relational Database for Hospital App",
    "section": "Database Entity Relationship Diagram",
    "text": "Database Entity Relationship Diagram"
  },
  {
    "objectID": "posts/03-Database Design/populating-database.html#populating-the-database",
    "href": "posts/03-Database Design/populating-database.html#populating-the-database",
    "title": "Designing and Creating Relational Database for Hospital App",
    "section": "Populating the Database",
    "text": "Populating the Database\nAll data in this project were synthetically generated using Faker and random library in Python.\n\n\nCode\nfrom faker import Faker\nimport random\nimport pandas as pd\nfrom datetime import datetime, timedelta, time\n\n\n\n\nCode\nfake = Faker('id_ID')\n\n\n\n\nCode\ndef create_hospital_data(seed = 123):\n  \"\"\"\n  Generate a dummy dataset of hospitals .\n  Args:\n    seed (int, optional): Random seed for reproducibility. Default is 123.\n\n  Returns:\n    pandas.DataFrame: A DataFrame with two columns:\n        - 'hospital_name': The generated hospital names (e.g., 'RS Johnson')\n        - 'hospital_address': The corresponding fake hospital addresses\n  \"\"\"\n  fake.seed_instance(seed)\n  nama_rs = []\n  for i in range (2):\n    nama = fake.last_name()\n    nama_rs.append('RS ' +fake.last_name())\n\n  address_rs =[]\n  for i in range (2):\n    address = fake.address()\n    address_rs.append(fake.address().replace(\"\\n\", \", \"))\n\n  hospital_dict = {'hospital_name':nama_rs, 'hospital_address': address_rs}\n  data = pd.DataFrame(data=hospital_dict)\n\n  return data\n\n\n\n\nCode\nhospitals = create_hospital_data()\nhospitals\n\n\n\n  \n    \n\n\n\n\n\n\nhospital_name\nhospital_address\n\n\n\n\n0\nRS Santoso\nGg. Cikutra Timur No. 571, Bekasi, Jawa Tengah...\n\n\n1\nRS Wasita\nJalan H.J Maemunah No. 174, Magelang, NB 17695\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nCode\ndef create_specializations(seed=123):\n  \"\"\"\n  Generate a dummy dataset of medical specializations using the Faker library.\n\n  Args:\n    seed (int, optional): Random seed for reproducibility. Default is 123.\n\n  Returns:\n    pandas.DataFrame: A DataFrame with two columns:\n        - 'specialization_name': List of predefined medical specializations\n          (e.g., 'Cardiologists', 'Dermatologists').\n        - 'specialization_desc': Randomly generated short descriptions for each specialization.\n  \"\"\"\n  fake.seed_instance(seed)\n  specialization_name = ['General Practitioners', 'Pediatricians','Cardiologists','Dermatologists', 'Orthopedic']\n\n  specialization_desc = fake.texts(nb_texts=5, max_nb_chars=50)\n\n  spec_dict = {'specialization_name':specialization_name,\n            'specialization_desc':specialization_desc}\n  data = pd.DataFrame(data=spec_dict)\n\n  return data\n\n\n\n\nCode\nspecializations = create_specializations()\nspecializations\n\n\n\n  \n    \n\n\n\n\n\n\nspecialization_name\nspecialization_desc\n\n\n\n\n0\nGeneral Practitioners\nEaque quisquam eaque. Fugit natus exercitationem.\n\n\n1\nPediatricians\nCum temporibus quo soluta fugiat.\n\n\n2\nCardiologists\nPariatur commodi tenetur eos.\n\n\n3\nDermatologists\nReprehenderit odit natus vero accusamus.\n\n\n4\nOrthopedic\nMollitia sunt quam harum quod accusamus.\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nCode\ndef create_patients(seed=123, n=100):\n  \"\"\"\n  Generate a synthetic dataset of patient.\n\n  Args:\n      seed (int, optional): Random seed for reproducibility. Default is 123.\n      n (int, optional): Number of patients to generate. Default is 100.\n\n  Returns:\n      pandas.DataFrame: A DataFrame containing the following columns:\n          - 'patient_name': Full name of the patient (first and last name).\n          - 'patient_gender': Gender of the patient ('Female' or 'Male').\n          - 'patient_birthdate': Randomly generated birth date (between ages 15–90).\n          - 'patient_contact': Simulated phone number.\n  \"\"\"\n  fake.seed_instance(seed)\n  random.seed(seed)\n\n  names = [fake.first_name() + \" \" + fake.last_name() for i in range (n)]\n\n  gender_choices = ['Female','Male']\n  genders = [random.choice(gender_choices) for i in range (n)]\n\n  birthdates = [fake.date_of_birth(minimum_age = 15, maximum_age = 90) for i in range (n)]\n\n  contacts = ['08'+ fake.numerify(text=\"##########\") for i in range(n)]\n\n  patient_dict = {'patient_name': names,\n                    'patient_gender': genders,\n                    'patient_birthdate': birthdates,\n                    'patient_contact': contacts}\n  data = pd.DataFrame(data = patient_dict)\n\n  return data\n\n\n\n\nCode\npatients = create_patients()\npatients\n\n\n\n  \n    \n\n\n\n\n\n\npatient_name\npatient_gender\npatient_birthdate\npatient_contact\n\n\n\n\n0\nBala Santoso\nFemale\n1977-03-01\n089490563144\n\n\n1\nCayadi Wasita\nMale\n1960-12-22\n082656090723\n\n\n2\nKasiran Kurniawan\nFemale\n1942-10-29\n080482834773\n\n\n3\nBahuwarna Wibowo\nMale\n1990-01-24\n084811798843\n\n\n4\nGasti Purwanti\nMale\n1951-12-16\n088166710966\n\n\n...\n...\n...\n...\n...\n\n\n95\nUli Hartati\nFemale\n1962-12-19\n081993230755\n\n\n96\nLulut Aryani\nMale\n2006-06-08\n083668184629\n\n\n97\nAsman Manullang\nFemale\n1980-04-15\n086253589902\n\n\n98\nEndah Simbolon\nMale\n1937-04-22\n086571070906\n\n\n99\nDina Mulyani\nFemale\n1973-03-05\n081212534333\n\n\n\n\n100 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nCode\ndef create_doctors(seed=202, n=25, hospitals=None, specializations=None):\n  \"\"\"\n  Generate a dummy dataset of doctors.\n\n  Args:\n      seed (int, optional): Random seed for reproducibility. Default is 202.\n      n (int, optional): Number of doctors to generate. Default is 25.\n      hospitals (pandas.DataFrame): DataFrame containing hospital records to link each doctor.\n      specializations (pandas.DataFrame): DataFrame containing specialization records to link each doctor.\n\n  Returns:\n      pandas.DataFrame: A DataFrame containing the following columns:\n          - 'hospital_id': ID of the hospital the doctor is affiliated with.\n          - 'specialization_id': ID of the doctor’s medical specialization.\n          - 'doctor_name': Full name of the doctor.\n  \"\"\"\n  fake.seed_instance(seed)\n  random.seed(seed)\n  names = [fake.first_name() + \" \" + fake.last_name() for i in range (n)]\n\n  hospital_id = [random.randint(1, len(hospitals)) for i in range (n)]\n  specialization_id = [random.randint(1, len(specializations)) for i in range (n)]\n\n  doctors_dict = {'hospital_id':hospital_id,\n                   'specialization_id': specialization_id,\n                   'doctor_name': names}\n\n  data = pd.DataFrame(doctors_dict)\n  return data\n\n\n\n\nCode\ndoctors = create_doctors(hospitals=hospitals, specializations=specializations)\ndoctors\n\n\n\n  \n    \n\n\n\n\n\n\nhospital_id\nspecialization_id\ndoctor_name\n\n\n\n\n0\n2\n4\nPrasetya Yuliarti\n\n\n1\n2\n4\nWakiman Wasita\n\n\n2\n2\n4\nHarto Napitupulu\n\n\n3\n1\n3\nVicky Widodo\n\n\n4\n1\n1\nBalidin Aryani\n\n\n5\n2\n2\nTimbul Hartati\n\n\n6\n1\n1\nBagus Nuraini\n\n\n7\n2\n1\nMartana Pangestu\n\n\n8\n2\n4\nJaswadi Lailasari\n\n\n9\n2\n3\nRini Suryatmi\n\n\n10\n1\n1\nSyahrini Nurdiyanti\n\n\n11\n2\n2\nHarjo Wacana\n\n\n12\n1\n2\nMartani Kusmawati\n\n\n13\n1\n2\nEdi Anggraini\n\n\n14\n2\n1\nWadi Wahyuni\n\n\n15\n1\n3\nBagya Nurdiyanti\n\n\n16\n2\n2\nJamal Rajasa\n\n\n17\n2\n2\nTugiman Halimah\n\n\n18\n2\n2\nAzalea Saputra\n\n\n19\n1\n4\nBakijan Nugroho\n\n\n20\n2\n2\nRatih Januar\n\n\n21\n2\n3\nBahuwarna Wibowo\n\n\n22\n1\n3\nKarma Anggriawan\n\n\n23\n1\n2\nGarang Prabowo\n\n\n24\n2\n3\nHadi Maheswara\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nCode\ndef create_doctor_schedule(doctors, seed=123):\n  \"\"\"\n  Generate a dummy work schedule for each doctor.\n\n  Args:\n      doctors (pandas.DataFrame): DataFrame containing doctor records\n          (each row representing one doctor).\n      seed (int, optional): Random seed for reproducibility. Default is 123.\n\n  Returns:\n      pandas.DataFrame: A DataFrame containing the following columns:\n          - 'doctor_id': Unique identifier of the doctor.\n          - 'day_of_week': Day the doctor is scheduled to work.\n          - 'start_time': Starting time of the doctor's shift.\n          - 'end_time': Ending time of the doctor's shift.\n  \"\"\"\n  random.seed(seed)\n\n  days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n  start_end_pairs = [\n      (time(8, 0), time(12, 0)),\n      (time(13, 0), time(17, 0)),\n      (time(9, 0), time(13, 0)),\n      (time(10, 0), time(14, 0))\n  ]\n\n  records = []\n\n  for index, doctor in doctors.iterrows():\n      doctor_id = index + 1\n\n      # pick 3 unique days\n      work_days = random.sample(days, 3)\n\n      for day in work_days:\n          start_time, end_time = random.choice(start_end_pairs)\n          records.append({\n              'doctor_id': doctor_id,\n              'day_of_week': day,\n              'start_time': start_time,\n              'end_time': end_time\n          })\n\n  df = pd.DataFrame(records)\n  return df\n\n\n\n\nCode\ndoctor_schedule = create_doctor_schedule(doctors)\ndoctor_schedule\n\n\n\n  \n    \n\n\n\n\n\n\ndoctor_id\nday_of_week\nstart_time\nend_time\n\n\n\n\n0\n1\nMonday\n10:00:00\n14:00:00\n\n\n1\n1\nWednesday\n09:00:00\n13:00:00\n\n\n2\n1\nSaturday\n08:00:00\n12:00:00\n\n\n3\n2\nMonday\n09:00:00\n13:00:00\n\n\n4\n2\nThursday\n08:00:00\n12:00:00\n\n\n...\n...\n...\n...\n...\n\n\n70\n24\nMonday\n10:00:00\n14:00:00\n\n\n71\n24\nTuesday\n08:00:00\n12:00:00\n\n\n72\n25\nSaturday\n08:00:00\n12:00:00\n\n\n73\n25\nTuesday\n09:00:00\n13:00:00\n\n\n74\n25\nThursday\n10:00:00\n14:00:00\n\n\n\n\n75 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nCode\ndef create_appointment_slots(doctor_schedule, slot_duration_minutes=30):\n    \"\"\"\n    Generate appointment time slots based on each doctor's schedule.\n\n    Args:\n        doctor_schedule (pandas.DataFrame): DataFrame containing doctors' work schedules,\n            including 'start_time' and 'end_time' for each day.\n        slot_duration_minutes (int, optional): Duration of each appointment slot in minutes.\n            Default is 30 minutes.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the following columns:\n            - 'schedule_id': Identifier referencing the doctor's schedule.\n            - 'slot_time_start': Start time of the appointment slot.\n            - 'slot_time_end': End time of the appointment slot.\n    \"\"\"\n    slots = []\n\n    for index, sched in doctor_schedule.iterrows():\n        start = sched['start_time']\n        end = sched['end_time']\n        current = datetime.combine(datetime.today(), start)\n\n        while current.time() &lt; end:\n            slot_start = current.time()\n            slot_end = (current + timedelta(minutes=slot_duration_minutes)).time()\n\n            if slot_end &gt; end:\n                break\n\n            slots.append({\n                'schedule_id': index + 1,\n                'slot_time_start': slot_start,\n                'slot_time_end': slot_end\n            })\n\n            current += timedelta(minutes=slot_duration_minutes)\n\n    return pd.DataFrame(slots)\n\n\n\n\nCode\nappointment_slots = create_appointment_slots(doctor_schedule)\nappointment_slots\n\n\n\n  \n    \n\n\n\n\n\n\nschedule_id\nslot_time_start\nslot_time_end\n\n\n\n\n0\n1\n10:00:00\n10:30:00\n\n\n1\n1\n10:30:00\n11:00:00\n\n\n2\n1\n11:00:00\n11:30:00\n\n\n3\n1\n11:30:00\n12:00:00\n\n\n4\n1\n12:00:00\n12:30:00\n\n\n...\n...\n...\n...\n\n\n595\n75\n11:30:00\n12:00:00\n\n\n596\n75\n12:00:00\n12:30:00\n\n\n597\n75\n12:30:00\n13:00:00\n\n\n598\n75\n13:00:00\n13:30:00\n\n\n599\n75\n13:30:00\n14:00:00\n\n\n\n\n600 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nCode\ndef create_appointments(patients, appointment_slots, n=50, seed=123,\n                        past_ratio=0.7, past_days=60, future_days=30):\n    \"\"\"\n    Membuat dummy data untuk tabel appointments.\n\n    Args:\n        patients (DataFrame): tabel pasien.\n        appointment_slots (DataFrame): tabel slot dokter.\n        n (int): jumlah total appointment yang dihasilkan.\n        seed (int): untuk reproducibility.\n        past_ratio (float): proporsi appointment masa lalu.\n        past_days (int): rentang hari ke belakang.\n        future_days (int): rentang hari ke depan.\n\n    Returns:\n        DataFrame: appointments dengan kolom patient_id, slot_id, appointment_date, appointment_status.\n    \"\"\"\n    random.seed(seed)\n\n    appointments = []\n    patient_ids = list(range(1, len(patients) + 1))\n    slot_ids = list(range(1, len(appointment_slots) + 1))\n\n    for _ in range(n):\n        patient_id = random.choice(patient_ids)\n        slot_id = random.choice(slot_ids)\n\n        # determine past or future\n        if random.random() &lt; past_ratio:\n            delta_days = -random.randint(1, past_days)  # past\n        else:\n            delta_days = random.randint(1, future_days)  # future\n\n        appointment_date = datetime.today().date() + timedelta(days=delta_days)\n\n        # status by date\n        if appointment_date &lt; datetime.today().date():\n            status = random.choices(['Completed', 'Cancelled'], weights=[0.8, 0.2])[0]\n        else:\n            status = random.choices(['Scheduled', 'Cancelled'], weights=[0.9, 0.1])[0]\n\n        appointments.append({\n            'patient_id': patient_id,\n            'slot_id': slot_id,\n            'appointment_date': appointment_date,\n            'appointment_status': status\n        })\n\n    # delete duplicate slot_id and date\n    df = pd.DataFrame(appointments).drop_duplicates(subset=['slot_id', 'appointment_date'])\n    return df.reset_index(drop=True)\n\n\n\n\nCode\nappointments = create_appointments(patients, appointment_slots)\nappointments\n\n\n\n  \n    \n\n\n\n\n\n\npatient_id\nslot_id\nappointment_date\nappointment_status\n\n\n\n\n0\n7\n275\n2025-10-16\nCompleted\n\n\n1\n5\n389\n2025-10-21\nCompleted\n\n\n2\n7\n164\n2025-10-07\nCompleted\n\n\n3\n32\n168\n2025-10-15\nCompleted\n\n\n4\n77\n387\n2025-10-22\nCompleted\n\n\n5\n14\n45\n2025-11-02\nCompleted\n\n\n6\n3\n299\n2025-12-01\nScheduled\n\n\n7\n61\n38\n2025-11-23\nScheduled\n\n\n8\n62\n212\n2025-12-03\nScheduled\n\n\n9\n41\n13\n2025-09-23\nCompleted\n\n\n10\n56\n552\n2025-09-30\nCompleted\n\n\n11\n63\n534\n2025-10-19\nCompleted\n\n\n12\n97\n186\n2025-11-15\nCancelled\n\n\n13\n85\n269\n2025-10-21\nCompleted\n\n\n14\n10\n468\n2025-09-23\nCompleted\n\n\n15\n25\n91\n2025-11-24\nScheduled\n\n\n16\n46\n235\n2025-10-10\nCompleted\n\n\n17\n8\n295\n2025-12-07\nScheduled\n\n\n18\n30\n558\n2025-10-31\nCompleted\n\n\n19\n69\n456\n2025-10-13\nCompleted\n\n\n20\n18\n468\n2025-10-26\nCompleted\n\n\n21\n88\n279\n2025-12-03\nScheduled\n\n\n22\n14\n65\n2025-09-27\nCompleted\n\n\n23\n56\n9\n2025-11-25\nScheduled\n\n\n24\n89\n414\n2025-10-22\nCompleted\n\n\n25\n61\n34\n2025-11-23\nScheduled\n\n\n26\n4\n168\n2025-11-26\nScheduled\n\n\n27\n87\n148\n2025-10-27\nCompleted\n\n\n28\n81\n59\n2025-11-25\nScheduled\n\n\n29\n19\n299\n2025-11-25\nScheduled\n\n\n30\n69\n346\n2025-10-02\nCompleted\n\n\n31\n76\n396\n2025-09-25\nCompleted\n\n\n32\n86\n22\n2025-09-17\nCompleted\n\n\n33\n93\n462\n2025-09-22\nCompleted\n\n\n34\n21\n535\n2025-10-09\nCompleted\n\n\n35\n18\n352\n2025-10-08\nCompleted\n\n\n36\n29\n584\n2025-10-25\nCancelled\n\n\n37\n32\n209\n2025-11-21\nScheduled\n\n\n38\n65\n83\n2025-11-29\nScheduled\n\n\n39\n3\n199\n2025-10-28\nCompleted\n\n\n40\n41\n86\n2025-10-08\nCompleted\n\n\n41\n69\n139\n2025-11-28\nScheduled\n\n\n42\n51\n29\n2025-09-19\nCancelled\n\n\n43\n63\n349\n2025-10-28\nCancelled\n\n\n44\n17\n7\n2025-12-08\nScheduled\n\n\n45\n64\n352\n2025-11-04\nCompleted\n\n\n46\n47\n472\n2025-09-30\nCancelled\n\n\n47\n41\n368\n2025-10-14\nCompleted\n\n\n48\n97\n198\n2025-11-18\nScheduled\n\n\n49\n8\n425\n2025-10-26\nCancelled\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nCode\ndef create_medical_records(appointments):\n  \"\"\"\n  Generate dummy medical record data based on completed appointments.\n\n  Args:\n      appointments (pandas.DataFrame): DataFrame containing appointment information,\n          including the column 'appointment_status'.\n\n  Returns:\n      pandas.DataFrame: A DataFrame containing:\n          - 'appointment_id': The corresponding appointment identifier.\n          - 'diagnosis': Randomly generated diagnosis text.\n          - 'treatment': Randomly generated treatment text.\n  \"\"\"\n  records = []\n\n  # Filter only completed appointment\n  completed_appointments = appointments[appointments[\"appointment_status\"] == \"Completed\"]\n\n  for index, appt in completed_appointments.iterrows():\n    records.append({\n          \"appointment_id\": index + 1,\n          \"diagnosis\": fake.text(max_nb_chars=50),   # random words\n          \"treatment\": fake.text(max_nb_chars=60),\n          })\n\n  return pd.DataFrame(records)\n\n\n\n\nCode\nmedical_records = create_medical_records(appointments)\nmedical_records\n\n\n\n  \n    \n\n\n\n\n\n\nappointment_id\ndiagnosis\ntreatment\n\n\n\n\n0\n1\nConsectetur dolore ratione dicta voluptatem est.\nNostrum recusandae hic quis.\n\n\n1\n2\nSint perspiciatis vero esse omnis.\nCupiditate quas ipsam.\n\n\n2\n3\nLibero ab deserunt perspiciatis.\nLibero quos doloremque pariatur iusto nobis.\n\n\n3\n4\nAccusantium in velit officia magni.\nOptio molestias blanditiis nostrum voluptate.\n\n\n4\n5\nPlaceat odio deserunt dicta deserunt ullam.\nQuas sapiente accusamus aliquam.\n\n\n5\n6\nVelit eaque incidunt cupiditate.\nIste occaecati dolores dicta accusantium maxime.\n\n\n6\n10\nNihil molestiae deserunt quibusdam fuga quaerat.\nNesciunt laborum a veniam architecto saepe.\n\n\n7\n11\nQuas tempora quae.\nDeserunt odit officia accusantium.\n\n\n8\n12\nSaepe molestias nulla optio error laborum.\nLaborum animi aperiam maiores nihil minima.\n\n\n9\n14\nSimilique aliquid assumenda in aperiam tenetur.\nSimilique ad aspernatur molestias officia veri...\n\n\n10\n15\nAb nam velit dolores dolores.\nExplicabo est fuga ad tempore mollitia digniss...\n\n\n11\n17\nSequi ipsa velit qui quo aliquid.\nEst error nisi inventore.\n\n\n12\n19\nHarum neque expedita sapiente quis quas pariatur.\nEos laborum aliquid quos.\n\n\n13\n20\nCumque natus dolorem ullam.\nIncidunt hic recusandae reiciendis ducimus lab...\n\n\n14\n21\nPariatur nisi molestiae.\nOmnis aut voluptas impedit. Sit error nisi omn...\n\n\n15\n23\nOfficiis voluptatibus culpa hic.\nEst maiores quod culpa voluptate odit.\n\n\n16\n25\nNesciunt nisi earum sapiente nulla esse quo nam.\nAccusantium blanditiis occaecati aut ab iusto ...\n\n\n17\n28\nOdio dolore ipsam deleniti mollitia.\nDoloremque blanditiis aliquid sunt sint.\n\n\n18\n31\nLaboriosam quos ea quos eum.\nEst non vero laborum. Labore quidem totam sequi.\n\n\n19\n32\nTemporibus aut ea.\nImpedit voluptatem illum tempora nostrum excep...\n\n\n20\n33\nOfficia magni natus quibusdam nisi architecto.\nEst nobis a pariatur nam sit necessitatibus.\n\n\n21\n34\nIpsam quasi unde vel accusamus delectus quo.\nVeritatis ipsam iure consequatur optio odit.\n\n\n22\n35\nIllo quam enim necessitatibus totam.\nDignissimos molestias quis ipsum tempore.\n\n\n23\n36\nHarum culpa molestias suscipit maxime.\nReprehenderit numquam earum incidunt molestias...\n\n\n24\n40\nRepellendus labore hic quidem.\nAperiam voluptatem quidem voluptatem fugiat ex...\n\n\n25\n41\nVitae facere facilis sunt velit eum veniam nam.\nEius debitis facilis ea harum dicta provident.\n\n\n26\n46\nAtque magni ipsam doloribus.\nTenetur corporis eum nostrum reprehenderit.\n\n\n27\n48\nDebitis illo architecto voluptatem.\nError sint natus magnam consequuntur eos solut...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nCode\npatients.to_csv(\"patients.csv\", index=False)\nhospitals.to_csv(\"hospitals.csv\", index=False)\nspecializations.to_csv(\"specializations.csv\", index=False)\ndoctors.to_csv(\"doctors.csv\", index=False)\ndoctor_schedule.to_csv(\"doctor_schedule.csv\", index=False)\nappointment_slots.to_csv(\"appointment_slots.csv\", index=False)\nappointments.to_csv(\"appointments.csv\", index=False)\nmedical_records.to_csv(\"medical_records.csv\", index=False)"
  },
  {
    "objectID": "posts/02-RFM-analysis/index.html",
    "href": "posts/02-RFM-analysis/index.html",
    "title": "Customer Segmentation using RFM Analysis and K-Means Clustering",
    "section": "",
    "text": "Important\n\n\n\nThe Project 100% completed and fully functional but the detailed case study and walkthrough on this Quarto site are being finalized — coming in the next 1–2 days."
  },
  {
    "objectID": "posts/02-RFM-analysis/index.html#objective",
    "href": "posts/02-RFM-analysis/index.html#objective",
    "title": "Customer Segmentation using RFM Analysis and K-Means Clustering",
    "section": "Objective",
    "text": "Objective\nTransform raw transaction data into actionable customer segments for targeted marketing campaigns."
  },
  {
    "objectID": "posts/02-RFM-analysis/index.html#methodology",
    "href": "posts/02-RFM-analysis/index.html#methodology",
    "title": "Customer Segmentation using RFM Analysis and K-Means Clustering",
    "section": "Methodology",
    "text": "Methodology\n\nCalculated Recency, Frequency, Monetary (RFM) scores\n\nApplied log transformation + standardization to handle skewness\n\nDetermined optimal number of clusters using Elbow Method and Silhouette Score\n\nTrained K-Means and interpreted resulting segments"
  },
  {
    "objectID": "posts/02-RFM-analysis/index.html#results",
    "href": "posts/02-RFM-analysis/index.html#results",
    "title": "Customer Segmentation using RFM Analysis and K-Means Clustering",
    "section": "Results",
    "text": "Results\n\nOptimal number of clusters: 6\n\nClear, business-friendly segment profiles identified"
  },
  {
    "objectID": "posts/02-RFM-analysis/index.html#customer-segments",
    "href": "posts/02-RFM-analysis/index.html#customer-segments",
    "title": "Customer Segmentation using RFM Analysis and K-Means Clustering",
    "section": "Customer Segments",
    "text": "Customer Segments\n\n\n\n\n\n\n\n\n\nSegment\n% of Customers\nKey Traits\nRecommended Action\n\n\n\n\nChampions\n8 %\nRecent, frequent, high spend\nReward, cross-sell\n\n\nLoyal Customers\n22 %\nFrequent, medium-high spend\nUpsell, loyalty program\n\n\nAt-Risk\n15 %\nHigh past spend, long time inactive\nWin-back campaign\n\n\nHibernating\n30 %\nLow frequency & monetary\nRe-engagement offers\n\n\nNew Customers\n12 %\nVery recent, low frequency\nOnboarding, education\n\n\nLost\n13 %\nVery long inactive, low spend\nMinimal effort or ignore\n\n\n\nTech Stack: Python • Pandas • Scikit-learn • Seaborn • Plotly"
  },
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "Daffa Daniel Rivaldi",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nChurn Prediction: 12-week Observation window with 1-Week Gap\n\n\nBuilt a customer churn prediction model using RFM-T feature as predictors that is extracted from the past 12-week observation windows with a 1-week gap. Achieved realistic…\n\n\n\nNov 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Retention Cohort Analysis\n\n\nAnalyzing Customer Cohorts to Improve Retention & Value\n\n\n\nNov 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesigning and Creating Relational Database for Doctor-Patient Appointments\n\n\nIn this post, I will design a simple database system for hospital app to manage patient medical records and doctor appointments using postgresql. Through this project, I…\n\n\n\nNov 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Segmentation using RFM Analysis and K-Means Clustering\n\n\nSegmented xx customers into x actionable groups using RFM + K-Means, unlocking clear retention and re-engagement strategies.\n\n\n\nOct 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsed Car Price Prediction\n\n\nAn end-to-end machine learning project to predict used car prices based on features like brand, model, year, mileage, fuel type, transmission, etc. Includes data cleaning…\n\n\n\nOct 1, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Daffa Daniel Rivaldi",
    "section": "",
    "text": "this is where the cv will be"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Daffa Daniel Rivaldi",
    "section": "",
    "text": "Highly motivated and detail-oriented student with interest in data analysis, statistical modeling, and data science. Proficient in utilizing tools such as Python, Git, PostgreSQL, and Tableau to solve complex problems and deliver actionable insights. Strong foundation in mathematics with a proven abilty to collaborate and execute projects effectively. Currently focused on improving data analysis skill, mastering machine learning, and improving databases management skills."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Daffa Daniel Rivaldi",
    "section": "Education",
    "text": "Education\n\nUniversitas Indonesia\nBachelor’s degree, Mathematics | Aug 2021 – Sept 2025\nSMA Negeri 1 Tangerang\nMathematics and Natural Science | Jul 2017 – May 2020"
  },
  {
    "objectID": "about.html#certifications-training",
    "href": "about.html#certifications-training",
    "title": "Daffa Daniel Rivaldi",
    "section": "Certifications & Training",
    "text": "Certifications & Training\n\nMachine Learning APIs & Machine Learning Deployments\nPacmann Academy | Issued August 2025\n Credential  \nCustomer, Marketing, Product Analytics & Experimentation\nPacmann Academy | Issued April 2025\n Credential  \nBasic & Advanced Machine Learning Algorithm\nPacmann Academy | Issued January 2025\n Credential  \nSQL & Relational Database\nPacmann Academy | Issued November 2024\n Credential  \nData Visualization & Wrangling\nPacmann Academy | Issued September 2024\n Credential  \nBasic Python Programming & Python for Software Engineering\nPacmann Academy | Issued July 2024\n Credential"
  },
  {
    "objectID": "about.html#language",
    "href": "about.html#language",
    "title": "Daffa Daniel Rivaldi",
    "section": "Language",
    "text": "Language\n\nIndonesian (Native)\nEnglish\nDuolingo English Test | Issued Jan 2024 – Expires Jan 2026\n Credential"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daffa Daniel Rivaldi",
    "section": "",
    "text": "Fresh Mathematics graduate from Universitas Indonesia (2025) actively looking for Data Science or Analytics roles. I turn raw data into predictive models and actionable dashboards using Python, SQL, and modern ML tools. Ready to contribute real impact from day one.\nCheckout My Post →"
  },
  {
    "objectID": "posts/01-used-car-price/index.html",
    "href": "posts/01-used-car-price/index.html",
    "title": "Used Car Price Prediction – End-to-End Machine Learning Project",
    "section": "",
    "text": "This is a complete end-to-end machine learning project that predicts used car prices based on a finished Kaggle competition dataset (Used Car Price Dataset ~188k rows). The goal of this repository is to showcase a production-ready ML workflow from raw data to a deployed web app:\n\nExploratory Data Analysis (EDA)\nPreprocessing & feature engineering\nModel selection & hyperparameter tuning\nFinal model training with proper train/validation split\nSimple deployment using Streamlit\n\n\n\nSource: Kaggle Playground Competition Series 4, Episode 9 (competition has ended)\nOriginal name: Regression of Used Car Prices\n~188,000 rows of used car listings with features such as brand, model, year, mileage, fuel type, transmission, etc.\nLink to original competition (for reference): Kaggle competition\n\n\n\n```{bash}\n├───data\n│   ├───processed\n│   │   └───\n│   └───raw\n│       └───test.csv\n│       └───sample_submission.csv\n│       └───train.csv\n│       └───X_train.pkl\n│       └───X_valid.pkl\n│       └───y_train.pkl\n│       └───y_valid.pkl\n├───models\n│   └───imputer.pkl\n│   └───lgb_rand.pkl\n│   └───OHencoder.pkl\n│   └───scaler.pkl\n├───notebooks\n│       └───data_preparation.ipynb\n│       └───EDA.ipynb\n│       └───data_preprocessing.ipynb\n│       └───modelling.ipynb\n├───src\n│   └───modelling.py\n│   └───preprocessing.pkl    \n│   └───utils.pkl \n└───requirements.txt\n```\n\n\n\n\nThe variables accident, clean_title, and fuel_type have missing values that must be handled.\nThe variables model_year, mileage, and price have outliers.\nThere are 1897 car models.\nThe variable fuel_type has missing values stored as -, not supported, and nan, and the category hydrogen fuel needs to be added.\nThe engine variable can be extracted into several columns to be more descriptive, such as:\n\nhorsepower : float64 # example: 369.0, 214.0, 482.0\nengine_size : float64 # example: 3.0, 2.5, 4.4\ncylinder : int64 # example: 4, 6, 8, 10, 12\nis_electric : int64 # 0 = no, 1 = yes\nis_turbo : int64 # 0 = no, 1 = yes\nfuel_system : category # example: MPFI, GDI, PDI, DOHC, SOHC, etc.\n\nFuel type can be extracted from engine if fuel_type is nan, -, ‘ ’, or not supported.\nThe transmission variable can be simplified into 4 categories, namely A/T, M/T, and Unknown.\nThe ext_col and int_col variables can be simplified into basic color categories such as blue, red, black, silver, white, gold, orange, purple, and beige, while other colors can be simplified into other or unknown.\nThe variable accident have only 2 type of category with 2452 rows missing.\n\n\n\n\n\nConsistent handling of missing/inconsistent categorical values\nReplaced ambiguous entries in fuel_type and transmission with standardized categories (Unknown, a/t, m/t) using regex-based cleaning.\nParsing of the free-text engine column\nUsed regular expressions for robust extraction even from highly inconsistent formats. Extracted structured features:\n\nHorsepower (HP)\nEngine displacement (Liters)\n\nNumber of cylinders\n\nTurbocharged / supercharged flag\n\nElectric motor flag\n\nFuel system type\n\nRecovery of fuel_type\nFilled missing/unknown fuel_type values using information parsed from the engine string (e.g., “Electric Motor”, “Hybrid”, “Diesel”).\nContext-aware imputation\n\nApplied median imputation for horsepower, engine size, and cylinders only on non-electric vehicles\nSet horsepower = 0 and cylinders = 0 for confirmed electric vehicles\nFilled accident and clean_title with an explicit “Unknown” category.\n\nColor simplification\nReduced hundreds of unique exterior & interior colors into 11 meaningful groups (blue, red, black, silver, white, gold, orange, purple, beige, other, unknown) to reduce cardinality and noise.\nRobust numerical scaling\nApplied StandardScaler to: model_year, milage, horsepower, engine_size, cylinder.\nCategorical encoding\nUsed One-Hot Encoding on cleaned categorical columns: brand, fuel_type, transmission, ext_col, int_col, accident, clean_title, fuel_system.\nDropped redundant/high-cardinality raw columns\nRemoved original engine and model columns after successful feature extraction.\nModular & reusable pipeline\nEntire workflow wrapped into a single preprocessing_pipeline() function that accepts pre-fitted imputer, scaler, and encoder → ensures zero data leakage between train/validation/test sets.\n\nAll steps were carefully designed to handle the highly messy real-world nature of the used-car dataset while maintaining reproducibility and production-readiness.\n\n\n\nSeveral models were evaluated using 5-fold CV and a hold-out validation set:\n\n\n\nModel\nRMSE(Validation)\n\n\n\n\nLGBM_rand\n68090\n\n\nCatBoost_rand\n68257\n\n\nLGBM\n68344\n\n\nXGBoost\n68352\n\n\nCatBoost\n68356\n\n\n\nLightGBM achieved the lowest RMSE of 68090 on the validation set and was therefore selected as the final model."
  },
  {
    "objectID": "posts/01-used-car-price/index.html#project-overview",
    "href": "posts/01-used-car-price/index.html#project-overview",
    "title": "Used Car Price Prediction – End-to-End Machine Learning Project",
    "section": "",
    "text": "This is a complete end-to-end machine learning project that predicts used car prices based on a finished Kaggle competition dataset (Used Car Price Dataset ~188k rows). The goal of this repository is to showcase a production-ready ML workflow from raw data to a deployed web app:\n\nExploratory Data Analysis (EDA)\nPreprocessing & feature engineering\nModel selection & hyperparameter tuning\nFinal model training with proper train/validation split\nSimple deployment using Streamlit\n\n\n\nSource: Kaggle Playground Competition Series 4, Episode 9 (competition has ended)\nOriginal name: Regression of Used Car Prices\n~188,000 rows of used car listings with features such as brand, model, year, mileage, fuel type, transmission, etc.\nLink to original competition (for reference): Kaggle competition\n\n\n\n```{bash}\n├───data\n│   ├───processed\n│   │   └───\n│   └───raw\n│       └───test.csv\n│       └───sample_submission.csv\n│       └───train.csv\n│       └───X_train.pkl\n│       └───X_valid.pkl\n│       └───y_train.pkl\n│       └───y_valid.pkl\n├───models\n│   └───imputer.pkl\n│   └───lgb_rand.pkl\n│   └───OHencoder.pkl\n│   └───scaler.pkl\n├───notebooks\n│       └───data_preparation.ipynb\n│       └───EDA.ipynb\n│       └───data_preprocessing.ipynb\n│       └───modelling.ipynb\n├───src\n│   └───modelling.py\n│   └───preprocessing.pkl    \n│   └───utils.pkl \n└───requirements.txt\n```\n\n\n\n\nThe variables accident, clean_title, and fuel_type have missing values that must be handled.\nThe variables model_year, mileage, and price have outliers.\nThere are 1897 car models.\nThe variable fuel_type has missing values stored as -, not supported, and nan, and the category hydrogen fuel needs to be added.\nThe engine variable can be extracted into several columns to be more descriptive, such as:\n\nhorsepower : float64 # example: 369.0, 214.0, 482.0\nengine_size : float64 # example: 3.0, 2.5, 4.4\ncylinder : int64 # example: 4, 6, 8, 10, 12\nis_electric : int64 # 0 = no, 1 = yes\nis_turbo : int64 # 0 = no, 1 = yes\nfuel_system : category # example: MPFI, GDI, PDI, DOHC, SOHC, etc.\n\nFuel type can be extracted from engine if fuel_type is nan, -, ‘ ’, or not supported.\nThe transmission variable can be simplified into 4 categories, namely A/T, M/T, and Unknown.\nThe ext_col and int_col variables can be simplified into basic color categories such as blue, red, black, silver, white, gold, orange, purple, and beige, while other colors can be simplified into other or unknown.\nThe variable accident have only 2 type of category with 2452 rows missing.\n\n\n\n\n\nConsistent handling of missing/inconsistent categorical values\nReplaced ambiguous entries in fuel_type and transmission with standardized categories (Unknown, a/t, m/t) using regex-based cleaning.\nParsing of the free-text engine column\nUsed regular expressions for robust extraction even from highly inconsistent formats. Extracted structured features:\n\nHorsepower (HP)\nEngine displacement (Liters)\n\nNumber of cylinders\n\nTurbocharged / supercharged flag\n\nElectric motor flag\n\nFuel system type\n\nRecovery of fuel_type\nFilled missing/unknown fuel_type values using information parsed from the engine string (e.g., “Electric Motor”, “Hybrid”, “Diesel”).\nContext-aware imputation\n\nApplied median imputation for horsepower, engine size, and cylinders only on non-electric vehicles\nSet horsepower = 0 and cylinders = 0 for confirmed electric vehicles\nFilled accident and clean_title with an explicit “Unknown” category.\n\nColor simplification\nReduced hundreds of unique exterior & interior colors into 11 meaningful groups (blue, red, black, silver, white, gold, orange, purple, beige, other, unknown) to reduce cardinality and noise.\nRobust numerical scaling\nApplied StandardScaler to: model_year, milage, horsepower, engine_size, cylinder.\nCategorical encoding\nUsed One-Hot Encoding on cleaned categorical columns: brand, fuel_type, transmission, ext_col, int_col, accident, clean_title, fuel_system.\nDropped redundant/high-cardinality raw columns\nRemoved original engine and model columns after successful feature extraction.\nModular & reusable pipeline\nEntire workflow wrapped into a single preprocessing_pipeline() function that accepts pre-fitted imputer, scaler, and encoder → ensures zero data leakage between train/validation/test sets.\n\nAll steps were carefully designed to handle the highly messy real-world nature of the used-car dataset while maintaining reproducibility and production-readiness.\n\n\n\nSeveral models were evaluated using 5-fold CV and a hold-out validation set:\n\n\n\nModel\nRMSE(Validation)\n\n\n\n\nLGBM_rand\n68090\n\n\nCatBoost_rand\n68257\n\n\nLGBM\n68344\n\n\nXGBoost\n68352\n\n\nCatBoost\n68356\n\n\n\nLightGBM achieved the lowest RMSE of 68090 on the validation set and was therefore selected as the final model."
  },
  {
    "objectID": "posts/01-used-car-price/index.html#full-code-notebooks",
    "href": "posts/01-used-car-price/index.html#full-code-notebooks",
    "title": "Used Car Price Prediction – End-to-End Machine Learning Project",
    "section": "Full Code & Notebooks",
    "text": "Full Code & Notebooks\nAll notebooks (from data preparation to modelling) are available in the GitHub repository:\nGitHub Repository: link\nOr You can explore the notebooks directly:\n\nData Preparation\n\nEDA Notebook\n\nData Preprocessing & feature engineering Notebook\n\nModelling Notebook"
  },
  {
    "objectID": "posts/03-Database Design/index.html",
    "href": "posts/03-Database Design/index.html",
    "title": "Designing and Creating Relational Database for Doctor-Patient Appointments",
    "section": "",
    "text": "This project showcases my capabilities in relational database design, data modeling, and SQL schema implementation. The primary goal was to design an efficient, consistent, and scalable database schema to support the operations of a hospital management system.\nThe resulting tables has been normalized up to the Third Normal Form (3NF) to minimize data redundancy and ensure data integrity.\nGithub: Link\n\n\n\nEntity-Relationship Modeling (ERD): Creation of a comprehensive relationship diagram to visualize the data structure.\nData Normalization: Application of normalization rules to achieve storage efficiency and integrity.\nSQL Schema Definition: Implementation of complete SQL DDL (CREATE TABLE) scripts, including constraints (PRIMARY KEY, FOREIGN KEY, UNIQUE, CHECK).\nData Documentation: Detailed explanation of each table, column, and relationship.\nData Population (Dummy Data): Generation of realistic and relevant dummy data using the Faker library in Python to populate the tables, ensuring the schema is thoroughly tested with substantial, simulated data.\n\n\n\n\n\n\n\nhospital appointment system erd"
  },
  {
    "objectID": "posts/03-Database Design/index.html#key-features",
    "href": "posts/03-Database Design/index.html#key-features",
    "title": "Designing and Creating Relational Database for Doctor-Patient Appointments",
    "section": "",
    "text": "Entity-Relationship Modeling (ERD): Creation of a comprehensive relationship diagram to visualize the data structure.\nData Normalization: Application of normalization rules to achieve storage efficiency and integrity.\nSQL Schema Definition: Implementation of complete SQL DDL (CREATE TABLE) scripts, including constraints (PRIMARY KEY, FOREIGN KEY, UNIQUE, CHECK).\nData Documentation: Detailed explanation of each table, column, and relationship.\nData Population (Dummy Data): Generation of realistic and relevant dummy data using the Faker library in Python to populate the tables, ensuring the schema is thoroughly tested with substantial, simulated data."
  },
  {
    "objectID": "posts/03-Database Design/index.html#entity-relationship-diagram",
    "href": "posts/03-Database Design/index.html#entity-relationship-diagram",
    "title": "Designing and Creating Relational Database for Doctor-Patient Appointments",
    "section": "",
    "text": "hospital appointment system erd"
  },
  {
    "objectID": "posts/03-Database Design/index.html#populating-the-database",
    "href": "posts/03-Database Design/index.html#populating-the-database",
    "title": "Designing and Creating Relational Database for Doctor-Patient Appointments",
    "section": "Populating the Database",
    "text": "Populating the Database\nAll data in this project were synthetically generated using Faker and random library in Python."
  },
  {
    "objectID": "posts/03-Database Design/index.html#sample-sql-queries",
    "href": "posts/03-Database Design/index.html#sample-sql-queries",
    "title": "Designing and Creating Relational Database for Doctor-Patient Appointments",
    "section": "Sample SQL Queries",
    "text": "Sample SQL Queries\nThe following are sample SQL queries you can do on hospital_app-db.\n\n1. Doctors with their hospitals and specializations\nSELECT doctor_name, hospital_name\nFROM doctors\nLEFT JOIN hospitals\n    ON  doctors.hospital_id = hospitals.hospital_id\nLEFT JOIN specializations\n    ON doctors.specialization_id = specializations.specialization_id\nOutput: \n\n\n2. Patient’s scheduled appointment\nCREATE VIEW full_appointments AS\nSELECT\n    a.appointment_id,\n    a.appointment_date,\n    a.appointment_status,\n    p.patient_id,\n    p.patient_name,\n    spec.specialization_id,\n    spec.specialization_name,\n    d.doctor_id,\n    d.doctor_name,\n    h.hospital_name,\n    h.hospital_address,\n    s.start_time AS slot_start_time,\n    s.end_time AS slot_end_time\nFROM appointments a\nJOIN patients p\n    ON a.patient_id = p.patient_id\nJOIN appointment_slots s\n    ON a.slot_id = s.slot_id\nJOIN doctor_schedule ds\n    ON s.schedule_id = ds.schedule_id\nJOIN doctors d\n    ON ds.doctor_id = d.doctor_id\nJOIN hospitals h\n    ON d.hospital_id = h.hospital_id\nJOIN specializations spec\n    ON d.specialization_id = spec.specialization_id;\n\nSELECT *\nFROM full_appointments\nOutput: \nSELECT *\nFROM full_appointments\nWHERE appointment_status = 'Scheduled'\nOutput: \n\n\n3. Patient’s Medical Records\nSELECT appointment_date, patient_name, diagnosis, treatment\nFROM medical_records\nJOIN appointments\n    on medical_records.appointment_id = appointments.appointment_id\nJOIN patients\n    ON  appointments.patient_id = patients.patient_id\nOutput: \n\n\n4. Creating new appointment\nINSERT INTO appointments (patient_id, slot_id, appointment_date, appointment_status)\nVALUES (7, 25, '2025-11-15', 'Scheduled');\n\n\n5. Doctors with the most appointment\nSELECT\n    doctor_name, \n    COUNT(doctor_name) AS appoinment_count,\n    specialization_name\nFROM full_appointments\nGROUP BY doctor_name, specialization_name\nORDER BY appoinment_count DESC\nLIMIT 5\nOutput: \n\n\n6. Doctors with the most ‘completed’ appointment\nSELECT\n    doctor_name, \n    COUNT(doctor_name) AS appoinment_count,\n    specialization_name\nFROM full_appointments\nWHERE appointment_status = 'Completed'\nGROUP BY doctor_name, specialization_name\nORDER BY appoinment_count DESC\nLIMIT 5\nOutput: \n\n\n7. Specializations with the most cancelled appointment\nSELECT\n    specialization_name,\n    COUNT(specialization_name) AS spec_count\nFROM full_appointments\nWHERE appointment_status = 'Cancelled'\nGROUP BY specialization_name\nORDER BY spec_count DESC\nLIMIT 5\nOutput: \n\n\n8. Specializations with the most patients\nSELECT\n    specialization_name,\n    COUNT(DISTINCT patient_name) AS patient_count\nFROM full_appointments\nGROUP BY specialization_name\nORDER BY patient_count DESC\nLIMIT 5\nOutput: \n\n\n9. Number of Specialist in each hospital\nSELECT\n    hospital_name,\n    specialization_name,\n    COUNT(specialization_name) AS spec_count\nFROM doctors\nJOIN hospitals\n    ON doctors.hospital_id = hospitals.hospital_id\nJOIN specializations\n    ON doctors.specialization_id = specializations.specialization_id\nGROUP BY hospital_name, specialization_name\nORDER BY hospital_name, specialization_name, spec_count\nOutput:"
  },
  {
    "objectID": "posts/04-Cohort Analysis/index.html",
    "href": "posts/04-Cohort Analysis/index.html",
    "title": "Customer Retention Cohort Analysis",
    "section": "",
    "text": "This project analyzes customer behavior using cohort analysis, focusing on acquisition patterns, retention trends, and cumulative customer value over time. The goal is to understand how different cohorts perform and derive actionable business insights to improve acquisition quality, retention strategy, and customer lifetime value.\nThis project answers key business questions such as:\n\nWhich period brought the highest number of newly acquired customers?\nAre there specific months where customer acquisition significantly declined?\nHow does customer retention trend over time across cohorts?\nWhich cohorts contributed the highest customer value?\nAre there cohorts that consistently bring lower value customers?\nDoes the quality of acquired customers remain consistent across months?\nIs there alignment between acquisition volume and customer value/retention?\n\nGithub Repositories: Link\nDataset: Online Retail II\nReference: wunderdata - cohort analysis\n\n\n\nCohort Definition: Customers are grouped based on the month of their first transaction.\nRetention Measurement: Retention is calculated as the percentage of customers from each cohort who remain active in subsequent periods.\nActivity Definition: A customer is considered active if they record at least one transaction in the given period.\nCumulative Sales Analysis: Computed average cumulative sales per customer for each cohort to evaluate long-term customer value and spending patterns.\nVisualization Techniques: Insights presented using heatmaps, line charts, and cohort tables for retention and revenue behavior.\n\n\n\n\n\nDec-2009 acquired the largest number of customers.\nThere is a significant drop in customer acquisition from April to September.\nRetention improves noticeably in September–November 2010.\nRetention declines during June–August, indicating seasonal or engagement issues.\nHigh-value customers were acquired in Dec-2009 and Sep-2010.\nCustomers acquired between April and August generate lower value than other cohorts.\n\n\n\n\n\n\n\n\n\n\n\nInsight\nRecommended Action\n\n\n\n\nDecember 2009 acquired the highest number of customers\nReplicate the acquisition strategies used during this period.\n\n\nSignificant drop in customer acquisition Apr–Sep\nInvestigate marketing activity and strengthen acquisition campaigns.\n\n\nRetention increases in Sep–Nov 2010\nApply the successful retention tactics more consistently.\n\n\nRetention declines in Jun–Aug\nLaunch re-engagement initiatives and analyze seasonal/operational drivers.\n\n\nHigh-value customers acquired in Dec 2009 & Sep 2010\nScale the channels that brought these valuable segments.\n\n\nLow-value customers acquired Apr–Aug\nImprove onboarding, upselling, and cross-selling for these cohorts.\n\n\n\n\n\n\n\nPython: pandas, numpy, matplotlib, seaborn\nJupyter Notebook / Quarto for analysis and reporting\n\n\n\n\n├── data/\n│   └── \n├── notebooks/\n│   └── cohort_analysis.ipynb\n│   └──\n├── reports/\n│   └── cohort_analysis.qmd\n├── src/\n│   └── data_processing.py\n│   └── cohort_functions.py\n└── README.md"
  },
  {
    "objectID": "posts/04-Cohort Analysis/index.html#project-overview",
    "href": "posts/04-Cohort Analysis/index.html#project-overview",
    "title": "Customer Retention Cohort Analysis",
    "section": "",
    "text": "This project analyzes customer behavior using cohort analysis, focusing on acquisition patterns, retention trends, and cumulative customer value over time. The goal is to understand how different cohorts perform and derive actionable business insights to improve acquisition quality, retention strategy, and customer lifetime value.\nThis project answers key business questions such as:\n\nWhich period brought the highest number of newly acquired customers?\nAre there specific months where customer acquisition significantly declined?\nHow does customer retention trend over time across cohorts?\nWhich cohorts contributed the highest customer value?\nAre there cohorts that consistently bring lower value customers?\nDoes the quality of acquired customers remain consistent across months?\nIs there alignment between acquisition volume and customer value/retention?\n\nGithub Repositories: Link\nDataset: Online Retail II\nReference: wunderdata - cohort analysis\n\n\n\nCohort Definition: Customers are grouped based on the month of their first transaction.\nRetention Measurement: Retention is calculated as the percentage of customers from each cohort who remain active in subsequent periods.\nActivity Definition: A customer is considered active if they record at least one transaction in the given period.\nCumulative Sales Analysis: Computed average cumulative sales per customer for each cohort to evaluate long-term customer value and spending patterns.\nVisualization Techniques: Insights presented using heatmaps, line charts, and cohort tables for retention and revenue behavior.\n\n\n\n\n\nDec-2009 acquired the largest number of customers.\nThere is a significant drop in customer acquisition from April to September.\nRetention improves noticeably in September–November 2010.\nRetention declines during June–August, indicating seasonal or engagement issues.\nHigh-value customers were acquired in Dec-2009 and Sep-2010.\nCustomers acquired between April and August generate lower value than other cohorts.\n\n\n\n\n\n\n\n\n\n\n\nInsight\nRecommended Action\n\n\n\n\nDecember 2009 acquired the highest number of customers\nReplicate the acquisition strategies used during this period.\n\n\nSignificant drop in customer acquisition Apr–Sep\nInvestigate marketing activity and strengthen acquisition campaigns.\n\n\nRetention increases in Sep–Nov 2010\nApply the successful retention tactics more consistently.\n\n\nRetention declines in Jun–Aug\nLaunch re-engagement initiatives and analyze seasonal/operational drivers.\n\n\nHigh-value customers acquired in Dec 2009 & Sep 2010\nScale the channels that brought these valuable segments.\n\n\nLow-value customers acquired Apr–Aug\nImprove onboarding, upselling, and cross-selling for these cohorts.\n\n\n\n\n\n\n\nPython: pandas, numpy, matplotlib, seaborn\nJupyter Notebook / Quarto for analysis and reporting\n\n\n\n\n├── data/\n│   └── \n├── notebooks/\n│   └── cohort_analysis.ipynb\n│   └──\n├── reports/\n│   └── cohort_analysis.qmd\n├── src/\n│   └── data_processing.py\n│   └── cohort_functions.py\n└── README.md"
  },
  {
    "objectID": "posts/04-Cohort Analysis/index.html#full-code-details",
    "href": "posts/04-Cohort Analysis/index.html#full-code-details",
    "title": "Customer Retention Cohort Analysis",
    "section": "Full Code & Details",
    "text": "Full Code & Details\nThis project consist of 2 Notebooks:\n\nData Cleaning & EDA Notebook\nHere I perform some basic EDA and do a little cleaning.\nCohort Analysis Notebook\nHere is where I do the actual analysis for the dataset.\n\n\nData Cleaning & EDA Notebook"
  }
]