[
  {
    "objectID": "posts/project-2/index.html",
    "href": "posts/project-2/index.html",
    "title": "Mentoring 3",
    "section": "",
    "text": "Dataset Link: https://www.kaggle.com/datasets/laotse/credit-risk-dataset\nDataset berisi data terkait peminjam uang. Data ini memiliki 12 variabel dengan loan_status sebagai variabel dependen (output) dan sisanya sebagai variabel independen (input).\nBerikut adalah deskripsi mengenai arti dari setiap kolom pada data:\n\n\n\n\nFeature Name\nDescription\n\n\n\n\nperson_age\nAge\n\n\nperson_income\nAnnual Income\n\n\nperson_home_ownership\nHome ownership\n\n\nperson_emp_length\nEmployment length (in years)\n\n\nloan_intent\nLoan intent\n\n\nloan_grade\nExpected Risk Grade\n\n\nloan_amnt\nLoan amount\n\n\nloan_int_rate\nInterest rate\n\n\nloan_status\n0 : non default, 1: default\n\n\nloan_percent_income\nPercent income\n\n\ncb_person_default_on_file\nHistorical default\n\n\ncb_preson_cred_hist_length\nCredit history length\n\n\n\n\nCatatan: - Tidak terdapat keterangan mengenai mata uang yang digunakan pada person_income dan loan_amnt - loan_grade adalah klasifikasi ekspektasi risiko pemberian pinjaman, yaitu A sampai G untuk pinjaman dengan risiko default (gagal bayar) yang rendah hingga tinggi. (sumber:https://blog.groundfloor.com/groundfloorblog/about-loan-grading)",
    "crumbs": [
      "Portofolio",
      "Project 2: Mentoring 3"
    ]
  },
  {
    "objectID": "posts/project-2/index.html#task-1-data-preparation",
    "href": "posts/project-2/index.html#task-1-data-preparation",
    "title": "Mentoring 3",
    "section": "Task 1: Data Preparation",
    "text": "Task 1: Data Preparation\n\nimport pandas as pd\nimport numpy as np\n\n\nLoad the Data\n\ndef read_data(fname):\n  filename = '/content/' + fname\n\n  # read csv as pandas dataframe\n  dataset = pd.read_csv(filename)\n  dataset['id'] = dataset.index\n  dataset.set_index('id', inplace = True)\n  print('Data shape raw               : ', dataset.shape)\n\n  # drop duplicates\n  print('Number of duplicates         : ', dataset.duplicated().sum())\n\n  dataset.drop_duplicates(keep = 'last', inplace = True)\n\n  # print data shape\n  print('Data shape after dropping    : ', dataset.shape)\n\n  return dataset\n\n\ndf = read_data(fname= 'credit_risk_dataset.csv')\n\n\ndf.head()\n\n\n\nData Splitting\n\nfrom sklearn.model_selection import train_test_split\n\n\n# Splitting the input and output columns\n\ndef split_input_output(data, target_col):\n  X = data.drop((target_col), axis = 1)\n  y = data[target_col]\n  print(\"X shape: \" +str(X.shape))\n  print(\"y shape: \" + str(y.shape))\n  return X, y\n\n\nX, y = split_input_output(data=df,\n                          target_col='loan_status')\n\n\nX.head()\n\n\ny.head()\n\n\ny.value_counts(normalize = True)\n\nVariabel y memiliki kelas yang imbalance sehingga perlu dilakukan stratified splitting pada train-test split.\n\n# Train test split\ndef split_train_test(X,y, test_size = 0.2, seed = 123):\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, stratify = y, random_state=seed)\n  print(\"X train shape  : \", X_train.shape)\n  print(\"y train shape  : \", y_train.shape)\n  print(\"X test shape   : \", X_test.shape)\n  print(\"y test shape   : \", y_test.shape)\n  print(\"\\n\")\n\n  return X_train, X_test, y_train, y_test\n\n\n# Training set\nX_train, X_test, y_train, y_test = split_train_test(X,y)\n\n\nprint(len(X_train)/len(X))\nprint(len(X_test)/len(X))\n\n\n# Memastikan data test dan train memiliki proporsi kelas yang sama\nprint(y_train.value_counts(normalize= True))\nprint(y_test.value_counts(normalize= True))\n\n\n\nEDA\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nX_train.info()\n\nKeterangan: - Kolom person_home_ownership, loan_intent, loan_grade, dan cb_person_default_on_file bertipe kategorik. - Terdapat missing value pada person_emp_length(numerik) dan loan_int_rate (numerik)\n\nX_train.isna().sum()\n\n\n# Lakukan splitting Variabel x numerik dengan kategorik\ndef split_num_cat(data, num_cols, cat_cols):\n  data_num = data[num_cols]\n  data_cat = data[cat_cols]\n  print(\"Numeric Data shape: \"+ str(data_num.shape))\n  print(\"Categoric Data shape: \"+ str(data_cat.shape))\n\n  return data_num, data_cat\n\n\nnum_columns = ['person_age', 'person_income', 'person_emp_length',\n                'loan_amnt', 'loan_int_rate', 'loan_percent_income',\n                'cb_person_cred_hist_length' ]\n\ncat_columns = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']\n\nX_train_num, X_train_cat = split_num_cat(X_train, num_columns, cat_columns)\n\n\nStatistik Deskriptif Data Numerik\n\nX_train_num.describe().T\n\nKeterangan: - Terdapat indikasi outlier pada kolom person_age, person_income, dan person_emp_length. - Kolom numerik memiliki distribusi nilai yang sangat berbeda satu sama lain.\n\n# Membuat subplot\nfig, axes = plt.subplots(len(num_columns), 2, figsize=(12, 16))\n\n# Membuat plot untuk setiap kolom numerik\nfor i, column in enumerate(num_columns):\n   # Histogram dengan KDE\n   sns.histplot(X_train_num[column], kde=True, ax=axes[i, 0], color='skyblue')\n   axes[i, 0].set_title(f'Histogram of {column}')\n   axes[i, 0].set_xlabel(column)\n   axes[i, 0].set_ylabel('Frequency')\n\n   # Boxplot\n   sns.boxplot(x=X_train_num[column], ax=axes[i, 1], color='lightgreen')\n   axes[i, 1].set_title(f'Boxplot of {column}')\n   axes[i, 1].set_xlabel(column)\n\nplt.tight_layout()\nplt.show()\n\nperson_age, person_income, dan person_emp_length memiliki anomali\n\n\nPemeriksaan Anomali Data Numerik\n\nX_train_num[X_train_num['person_age']&gt; 100]\n\n\nX_train_num[ X_train_num['person_income'] &gt;= 1_000_000]\n\n\nX_train_num[X_train_num['person_emp_length'] &gt; 60]\n\n\nX_train_num[X_train_num['person_emp_length'] == 0]\n\n\nX_train_num[X_train_num['person_emp_length'] == 0].shape\n\nCukup banyak baris yang memiliki kolom person_emp_length = 0, saya akan berasumsi bahwa hal ini berarti objek observasi tidak pernah bekerja untuk orang lain tetapi memiliki penghasilan misalnya pengusaha dan bukanlah sebuah anomali pada data.\n\nX_train_num[X_train_num['loan_percent_income'] == 0]\n\n\n\nPengecekan Data Kategorik\n\nX_train_cat['person_home_ownership'].value_counts()\n\n\nX_train_cat['loan_intent'].value_counts()\n\n\nX_train_cat['loan_grade'].value_counts()\n\n\nX_train_cat['cb_person_default_on_file'].value_counts()\n\nKolom cb_person_default_on_file perlu diubah menjadi boolean\n\n\nData Preprocessing Plan:\n\nHapus data anomali pada kolom numerik.\nHandle missing value pada person_emp_length dan loan_int_rate dengan imputasi nilai median\nLakukan Standarisasi pada kolom numerik.\nUbah cb_person_default_on_file menjadi boolean\nLakukan ordinal encoding pada kolom loan_grade\nlakukan one hot encoding pada person_home_ownership dan loan_intent\n\n\n\n\nData Preprocessing\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\nPenanganan Anomali pada Data\n\nage_anomaly = X_train_num[X_train_num['person_age']&gt; 100].index.tolist()\n\nincome_anomaly = X_train_num[X_train_num['person_income'] &gt;= 1_000_000].index.tolist()\n\nemp_length_anomaly = X_train_num[X_train_num['person_emp_length'] &gt; 60].index.tolist()\n\nidx_to_drop = set(emp_length_anomaly + income_anomaly + age_anomaly)\n\n\nprint(f'Number of index to drop:', len(idx_to_drop))\nidx_to_drop\n\n\nX_train_num_dropped = X_train_num.drop(index = idx_to_drop)\ny_train_dropped = y_train.drop(index = idx_to_drop)\n\n\nprint('Shape of X train after dropped:', X_train_num_dropped.shape)\nX_train_num_dropped.head()\n\n\n\nPenanganan Missing Value\n\ndef num_imputer_fit(data):\n  imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')\n\n  imputer.fit(data)\n\n  return imputer\n\ndef num_imputer_transform(data, imputer):\n  imputer.set_output(transform = \"pandas\")\n  data = imputer.transform(data)\n  return data\n\n\n# Get the numerical imputer\nnum_imputer = num_imputer_fit(X_train_num_dropped)\n\n# Transform the data\nX_train_imputed = num_imputer_transform(X_train_num_dropped, num_imputer)\n\n\n# Validasi hasil\nX_train_imputed.isna().sum()\n\n\n\nStandardisasi\n\ndef fit_scaler(data):\n  scaler = StandardScaler()\n  scaler.fit(data)\n  return scaler\n\ndef transform_scaler(data, scaler):\n  scaler.set_output(transform = 'pandas')\n  data = scaler.transform(data)\n  return data\n\n\nscaler = fit_scaler(X_train_imputed)\n\nX_train_num_scaled = transform_scaler(X_train_imputed, scaler)\n\n\nX_train_num_scaled.head()\n\n\n# Validasi hasil\nX_train_num_scaled.describe().round(4)\n\n\n\nPenanganan Inkonsistensi Format Data Kategorik\n\nX_train_cat.loc[:,'cb_person_default_on_file'] = (X_train_cat['cb_person_default_on_file'].replace({'N' : 0, 'Y' : 1 }).astype(int))\n\n\nX_train_cat.head()\n\n\n\nEncoding Data Kategorik\n\ndef cat_OHencode_fit(data):\n  OHencoder = OneHotEncoder(sparse_output=False, handle_unknown = 'ignore')\n  OHencoder.fit(data[['person_home_ownership',  'loan_intent']])\n  return OHencoder\n\ndef cat_ORDencode_fit(data):\n  ORDencoder = OrdinalEncoder()\n  ORDencoder.fit(data[['loan_grade']])\n  return ORDencoder\n\n\ndef cat_encoder_transform(data, onehot_encoder, ordinal_encoder):\n  OHencoded_data = onehot_encoder.transform(data[['person_home_ownership',  'loan_intent']])\n  ORDencoded_data = ordinal_encoder.transform(data[['loan_grade']])\n\n  # simpan index 'id' sebagai kolom\n  df_reset = data.reset_index()\n\n  # Ubah data menjadi Dataframe\n  df_OHencoded = pd.DataFrame(OHencoded_data, columns= onehot_encoder.get_feature_names_out(['person_home_ownership',   'loan_intent']))\n  df_ORDencoded = pd.DataFrame(ORDencoded_data, columns=['loan_grade'])\n\n  # Gabungkan sesuai index\n  df_encoded_combined_indexed = pd.concat([df_reset.drop(columns=['person_home_ownership',  'loan_intent','loan_grade']),df_OHencoded, df_ORDencoded], axis=1)\n  result_df = df_encoded_combined_indexed.set_index('id')\n\n  return result_df\n\n\n# Perform categorical imputation\ncat_OHencoder = cat_OHencode_fit(X_train_cat)\ncat_ORDencoder = cat_ORDencode_fit(X_train_cat)\n\n# Transform\nX_train_cat_encoded = cat_encoder_transform(X_train_cat, cat_OHencoder, cat_ORDencoder)\n\n\nX_train_cat_encoded\n\n\n\nPenggabungan Data Numerik dan Kategorik\n\ndef concat_data(num_data, cat_data):\n  print('\\nCleaned Numerical data shape: ' + str(num_data.shape))\n  print('Cleaned Categorical data shape: ' + str(cat_data.shape))\n\n  concated_data = num_data.join(cat_data, how = 'inner')\n  print('Concated data shape: ' + str(concated_data.shape))\n\n  return concated_data\n\n\nX_train_concat = concat_data(X_train_num_scaled ,X_train_cat_encoded)\n\n\nX_train_concat\n\n\n\nMembuat Fungsi Preprocessing\n\ndef preprocess_data(data,num_cols, cat_cols,  num_imputer, scaler, cat_onehot_encoder, cat_ordinal_encoder):\n\n  #Splitting Numerik dan kategorik\n  X_num, X_cat = split_num_cat(data, num_cols, cat_cols)\n\n  #Penanganan Missing Value\n  X_num_imputed = num_imputer_transform(X_num, num_imputer)\n\n  #Standardisasi\n  X_train_num_clean = transform_scaler(X_num_imputed, scaler)\n\n  #Penganganan Inkonsistensi\n  X_cat.loc[:,'cb_person_default_on_file'] = (X_cat['cb_person_default_on_file'].replace({'N' : 0, 'Y' : 1 }).astype(int))\n\n  #Encoding Data Kategorik\n  X_cat_encoded = cat_encoder_transform(X_cat, cat_onehot_encoder, cat_ordinal_encoder)\n\n  #Gabungkan Data\n  cleaned_data = concat_data(X_train_num_clean,X_cat_encoded)\n\n  return cleaned_data",
    "crumbs": [
      "Portofolio",
      "Project 2: Mentoring 3"
    ]
  },
  {
    "objectID": "posts/project-2/index.html#task-2-modeling",
    "href": "posts/project-2/index.html#task-2-modeling",
    "title": "Mentoring 3",
    "section": "Task 2: Modeling",
    "text": "Task 2: Modeling\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\nfrom scipy.stats import randint,uniform\n\n\n# Preprocessing Data Test\nX_test_clean = preprocess_data(X_test,num_columns, cat_columns,  num_imputer, scaler, cat_OHencoder, cat_ORDencoder)\n\n\ny_test.shape\n\n\nMetrics\nUntuk meminimalkan kejadian False Negative akan digunakan Recall sebagai evaluation metric utama.\n\n\nBaseline Model\n\ndummy_clf = dummy_clf = DummyClassifier(strategy = \"most_frequent\")\ndummy_clf.fit(X = X_train_concat,\n              y = y_train_dropped)\n\n\ny_pred_dummy = dummy_clf.predict(X_train_concat)\n\nreport_dummy_model = classification_report(y_train_dropped, y_pred_dummy)\n\nprint(report_dummy_model)\n\n\n\nBest Model Search\n\nkNN Classifier Model\n\nknn_clf = KNeighborsClassifier()\n\nparam_grid = {'n_neighbors': range(1, 21), 'metric': ['euclidean', 'manhattan']}\ngrid_search = GridSearchCV(knn_clf, param_grid, cv=5,scoring='recall')\ngrid_search.fit(X_train_concat, y_train_dropped)\n\n\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Mean cross-validated score of the best_estimator:\", grid_search.best_score_)\n\n\nknn_best_model = grid_search.best_estimator_\ny_pred_knn = knn_best_model.predict(X_test_clean)\n\nreport_knn_model = classification_report(y_test, y_pred_knn)\nprint(report_knn_model)\n\n\n\nDecision Tree\n\n#Decision Tree\ndtree_clf = DecisionTreeClassifier(random_state = 42)\n\nparam_distributions = {\n    'max_depth': np.arange(1, 50),\n    'min_samples_split': np.arange(2, 20),\n    'min_samples_leaf': np.arange(1, 20),\n    'max_features': [None, 'sqrt', 'log2'],\n    'criterion': ['gini', 'entropy']\n}\n\nrandomcv_dtree = RandomizedSearchCV(\n    estimator=dtree_clf,\n    param_distributions=param_distributions,\n    n_iter=50,\n    cv=5,\n    scoring = \"recall\",\n    random_state=42,\n    n_jobs=-1\n)\n\nrandomcv_dtree.fit(X = X_train_concat,\n              y = y_train_dropped)\n\n\n# Menampilkan Hasil Hyperparameter Terbaik\nprint(\"Best Parameters:\", randomcv_dtree.best_params_)\nprint(\"Mean cross-validated score of the best_estimator:\", randomcv_dtree.best_score_)\n\n\nDT_best_model = randomcv_dtree.best_estimator_\ny_pred_DT = DT_best_model.predict(X_test_clean)\n\nreport_DT_model = classification_report(y_test, y_pred_DT)\nprint(report_DT_model)\n\n\n\nLogistic Regresion\n\n#Logistic Regression\nlogreg_clf = LogisticRegression()\n\nparam_grid = {'max_iter': [100, 500, 1000],\n              'penalty' : [ 'l1', 'l2',None],\n              'solver' : ['liblinear', 'saga'],\n              'C': [0.01, 0.1, 1, 10, 100]\n              }\n\ngridcv_logreg = GridSearchCV(estimator= logreg_clf, param_grid=param_grid, cv=5, scoring='recall')\n\ngridcv_logreg.fit(X = X_train_concat,\n              y = y_train_dropped)\n\n\nprint(\"Best Parameters:\", gridcv_logreg.best_params_)\nprint(\"Mean cross-validated score of the best_estimator:\", gridcv_logreg.best_score_)\n\n\nlogreg_best_model = gridcv_logreg.best_estimator_\ny_pred_logreg = logreg_best_model.predict(X_test_clean)\n\nreport_logreg_model = classification_report(y_test, y_pred_logreg)\nprint(report_logreg_model)\n\n\n\nSupport Vector Classifier Model\n\nsvc = SVC()\n\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'kernel': ['linear', 'rbf', 'poly'],\n    'gamma': ['scale', 'auto'],\n    'degree': [2, 3, 4]\n}\n\nrandomcv_svc = RandomizedSearchCV(\n    estimator=svc,\n    param_distributions=param_grid,\n    scoring='recall',\n    cv=5,\n    verbose=1,\n    n_jobs=-1\n)\n\nrandomcv_svc.fit(X = X_train_concat,\n              y = y_train_dropped)\n\n\nprint(\"Best Parameters:\", randomcv_svc.best_params_)\nprint(\"Mean cross-validated score of the best_estimator:\", randomcv_svc.best_score_)\n\n\ny_pred_svc = grid_search.best_estimator_.predict(X_test_clean)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_svc))\n\n\n\nBagging Decision Tree\n\ntree = DecisionTreeClassifier(random_state=123)\nbagging_model = BaggingClassifier(estimator= tree, random_state=123)\n\n# Parameter grid\nparam_distributions = {\n    'n_estimators': randint(10, 100),\n    'max_samples': [0.5, 0.7, 1.0],\n    'max_features': [0.5, 0.7, 1.0],\n    'bootstrap': [True, False],\n    'bootstrap_features': [True, False],\n    'estimator__max_depth': randint(5, 30),\n    'estimator__min_samples_split': randint(2, 20),\n}\n\n# RandomizedSearchCV\nrandom_search_bagging = RandomizedSearchCV(\n    estimator=bagging_model,\n    param_distributions=param_distributions,\n    n_iter=20,\n    cv=5,\n    scoring='recall',\n    n_jobs=-1,\n    random_state=123\n)\n\nrandom_search_bagging.fit(X = X_train_concat, y = y_train_dropped)\n\n\nprint(\"Best Parameters:\", random_search_bagging.best_params_)\nprint(\"Mean cross-validated score of the best_estimator:\", random_search_bagging.best_score_)\n\n\nbagging_best_model = random_search_bagging.best_estimator_\ny_pred_bagging = bagging_best_model.predict(X_test_clean)\n\nreport_bagging_model = classification_report(y_test, y_pred_bagging)\nprint(report_bagging_model)\n\n\n\nRandom Forest\n\nrf_clf = RandomForestClassifier(random_state=42)\n\nparam_dist = {\n    'n_estimators': randint(50, 200),  # Jumlah trees antara 50-200\n    'max_depth': [None, 10, 20, 30],  # Kedalaman maksimal tree\n    'min_samples_split': randint(2, 10),  # Minimal sampel untuk split\n    'min_samples_leaf': randint(1, 5),   # Minimal sampel di leaf node\n    'criterion': ['gini', 'entropy'],    # Fungsi untuk split\n}\n\nrandomcv_rf = RandomizedSearchCV(rf_clf, param_distributions=param_dist, n_iter=20, cv=5, scoring='recall', random_state = 42)\nrandomcv_rf.fit(X_train_concat, y_train_dropped)\n\n\nprint(\"Best Parameters:\", randomcv_rf.best_params_)\nprint(\"Best Cross-Validated recall:\", randomcv_rf.best_score_)\n\n\nrf_best_model = randomcv_rf.best_estimator_\ny_pred_rf = rf_best_model.predict(X_test_clean)\n\n\nreport_rf_model = classification_report(y_test, y_pred_rf)\nprint(report_rf_model)\n\n\n\nAdaboost Model\n\ntree = DecisionTreeClassifier(random_state=123)\nadaboost_model = AdaBoostClassifier(estimator= tree, random_state=123)\n\nparam_distributions = {\n    'n_estimators': randint(50, 200),\n    'learning_rate': uniform(0.01, 1.0),\n    'estimator__max_depth': randint(1, 5),\n    'estimator__min_samples_split': randint(2, 20),\n    'estimator__min_samples_leaf': randint(1, 10),\n}\n\nrandom_search_adaboost = RandomizedSearchCV(\n    estimator=adaboost_model,\n    param_distributions=param_distributions,\n    n_iter=20,\n    cv=5,\n    scoring='recall',\n    n_jobs=-1,\n    random_state=123\n)\n\n\nrandom_search_adaboost.fit(X_train_concat, y_train_dropped)\n\n\nprint(\"Best Parameters:\", random_search_adaboost.best_params_)\nprint(\"Best Cross-Validated recall:\", random_search_adaboost.best_score_)\n\n\nadaboost_best_model = random_search_adaboost.best_estimator_\ny_pred_adaboost = adaboost_best_model.predict(X_test_clean)\n\nreport_adaboost = classification_report(y_test, y_pred_adaboost)\nprint(report_adaboost)\n\n\n\n\nFinal Best Model\n\n# Best Model: Decision Tree\nbest_model = randomcv_dtree.best_estimator_",
    "crumbs": [
      "Portofolio",
      "Project 2: Mentoring 3"
    ]
  },
  {
    "objectID": "posts/project-2/index.html#task-3-model-evaluation",
    "href": "posts/project-2/index.html#task-3-model-evaluation",
    "title": "Mentoring 3",
    "section": "Task 3: Model Evaluation",
    "text": "Task 3: Model Evaluation\n\nScore on test data\n\ny_pred_dt = best_model.predict(X_test_clean)\nreport_dt = classification_report(y_test, y_pred_dt)\nprint(report_dt)\n\n\n\nFinancial Impact Comparison\n\n# False Negative potential loss : 35juta\n# False positive potential loss : 10 juta\n\ny_pred_dummy = dummy_clf.predict(X_test)\n\nmodel_predictions = {\n    'dummy': y_pred_dummy,\n    'knn': y_pred_knn,\n    'dt': y_pred_dt,\n    'logreg': y_pred_logreg,\n    'svc': y_pred_svc,\n    'bagging': y_pred_bagging,\n    'rf': y_pred_rf,\n    'boost': y_pred_adaboost\n}\n\nconfusion_matrices = {}\n\nfor model_name, y_pred in model_predictions.items():\n    confusion_matrices[model_name] = confusion_matrix(y_test, y_pred).ravel()\n\n#Financial Loss\ncost_fn = 35  # dalam juta\ncost_fp = 10\n\nfinancial_losses = {}\n\n# Hitung Total Loss\nfor model_name, cm_values in confusion_matrices.items():\n    fn, fp = cm_values[2], cm_values[1]  # FN dan FP berada di indeks 2 dan 1\n    financial_losses[model_name] = fn * cost_fn + fp * cost_fp\n\n\ncomparison_data = []\n\n# Loop untuk mengisi data financial comparison\nfor model_name, cm_values in confusion_matrices.items():\n    fn, fp = cm_values[2], cm_values[1]  # FN dan FP berada di indeks 2 dan 1\n    total_loss = fn * cost_fn + fp * cost_fp\n    comparison_data.append({\n        'Model': model_name.capitalize(),        # Nama model dengan huruf kapital\n        'False Negative (FN)': fn,              # Jumlah FN\n        'False Positive (FP)': fp,              # Jumlah FP\n        'Total Loss (Rp juta)': total_loss      # Total loss dalam juta\n    })\n\n# Buat DataFrame dari data yang terkumpul\nfinancial_comparison = pd.DataFrame(comparison_data)\n\n# Tampilkan DataFrame\nfinancial_comparison\n\n\nfinancial_comparison_sorted = financial_comparison.sort_values(\n    by='Total Loss (Rp juta)',ascending = False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    data=financial_comparison_sorted,\n    x='Model',\n    y='Total Loss (Rp juta)',\n    palette='Blues_d'  # Pilih palet warna\n)\n\n# Tambahkan label dan judul\nplt.title('Financial Loss by Model', fontsize=16)\nplt.xlabel('Model', fontsize=12)\nplt.ylabel('Total Loss (Rp juta)', fontsize=12)\n\n\n\n# Tampilkan plot\nplt.tight_layout()\nplt.show()\n\nModel Bagging memiliki potential loss paling rendah",
    "crumbs": [
      "Portofolio",
      "Project 2: Mentoring 3"
    ]
  },
  {
    "objectID": "portofolio.html",
    "href": "portofolio.html",
    "title": "Daffa Daniel Rivaldi",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nMentoring 3\n\n\n\nML\n\n\nPortofolio\n\n\n\n\nJun 28, 2024\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Portofolio"
    ]
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Daffa Daniel Rivaldi",
    "section": "",
    "text": "this is where the cv will be"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Daffa Daniel Rivaldi",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     Medium\n  \n\n      \nHighly motivated and detail-oriented student with interest in data analysis, statistical modeling, and data science. Proficient in utilizing tools such as Python, Git, PostgreSQL, and Tableau to solve complex problems and deliver actionable insights. Strong foundation in mathematics with a proven abilty to collaborate and execute projects effectively. Currently focused on improving data analysis skill, mastering machine learning, and improving databases management skills.\n\n\n\n\nBachelor’s degree, Mathematics | Aug 2021 – Jun 2025\n\n\n\nMathematics and Natural Science | Jul 2017 – May 2020\n\n\n\n\n\n\nPacmann | Issued Nov 2024\n Credential  \n\n\n\nPacmann | Issued Sep 2024\n Credential  \n\n\n\nPacmann | Issued Jul 2024\n Credential  \n\n\n\nDuolingo English Test | Issued Jan 2024 – Expires Jan 2026\n Credential"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Daffa Daniel Rivaldi",
    "section": "",
    "text": "Bachelor’s degree, Mathematics | Aug 2021 – Jun 2025\n\n\n\nMathematics and Natural Science | Jul 2017 – May 2020"
  },
  {
    "objectID": "about.html#certifications-training",
    "href": "about.html#certifications-training",
    "title": "Daffa Daniel Rivaldi",
    "section": "",
    "text": "Pacmann | Issued Nov 2024\n Credential  \n\n\n\nPacmann | Issued Sep 2024\n Credential  \n\n\n\nPacmann | Issued Jul 2024\n Credential  \n\n\n\nDuolingo English Test | Issued Jan 2024 – Expires Jan 2026\n Credential"
  },
  {
    "objectID": "posts/project-2/DaffaDanielR_Mentoring_3_Introduction_to_Machine_Learning.html",
    "href": "posts/project-2/DaffaDanielR_Mentoring_3_Introduction_to_Machine_Learning.html",
    "title": "Mentoring 3",
    "section": "",
    "text": "Dataset Link: https://www.kaggle.com/datasets/laotse/credit-risk-dataset\nDataset berisi data terkait peminjam uang. Data ini memiliki 12 variabel dengan loan_status sebagai variabel dependen (output) dan sisanya sebagai variabel independen (input).\nBerikut adalah deskripsi mengenai arti dari setiap kolom pada data:\n\n\n\n\nFeature Name\nDescription\n\n\n\n\nperson_age\nAge\n\n\nperson_income\nAnnual Income\n\n\nperson_home_ownership\nHome ownership\n\n\nperson_emp_length\nEmployment length (in years)\n\n\nloan_intent\nLoan intent\n\n\nloan_grade\nExpected Risk Grade\n\n\nloan_amnt\nLoan amount\n\n\nloan_int_rate\nInterest rate\n\n\nloan_status\n0 : non default, 1: default\n\n\nloan_percent_income\nPercent income\n\n\ncb_person_default_on_file\nHistorical default\n\n\ncb_preson_cred_hist_length\nCredit history length\n\n\n\n\nCatatan: - Tidak terdapat keterangan mengenai mata uang yang digunakan pada person_income dan loan_amnt - loan_grade adalah klasifikasi ekspektasi risiko pemberian pinjaman, yaitu A sampai G untuk pinjaman dengan risiko default (gagal bayar) yang rendah hingga tinggi. (sumber:https://blog.groundfloor.com/groundfloorblog/about-loan-grading)"
  },
  {
    "objectID": "posts/project-2/DaffaDanielR_Mentoring_3_Introduction_to_Machine_Learning.html#task-1-data-preparation",
    "href": "posts/project-2/DaffaDanielR_Mentoring_3_Introduction_to_Machine_Learning.html#task-1-data-preparation",
    "title": "Mentoring 3",
    "section": "Task 1: Data Preparation",
    "text": "Task 1: Data Preparation\n\nimport pandas as pd\nimport numpy as np\n\n\nLoad the Data\n\ndef read_data(fname):\n  filename = '/content/' + fname\n\n  # read csv as pandas dataframe\n  dataset = pd.read_csv(filename)\n  dataset['id'] = dataset.index\n  dataset.set_index('id', inplace = True)\n  print('Data shape raw               : ', dataset.shape)\n\n  # drop duplicates\n  print('Number of duplicates         : ', dataset.duplicated().sum())\n\n  dataset.drop_duplicates(keep = 'last', inplace = True)\n\n  # print data shape\n  print('Data shape after dropping    : ', dataset.shape)\n\n  return dataset\n\n\ndf = read_data(fname= 'credit_risk_dataset.csv')\n\nData shape raw               :  (32581, 12)\nNumber of duplicates         :  165\nData shape after dropping    :  (32416, 12)\n\n\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n22\n59000\nRENT\n123.0\nPERSONAL\nD\n35000\n16.02\n1\n0.59\nY\n3\n\n\n1\n21\n9600\nOWN\n5.0\nEDUCATION\nB\n1000\n11.14\n0\n0.10\nN\n2\n\n\n2\n25\n9600\nMORTGAGE\n1.0\nMEDICAL\nC\n5500\n12.87\n1\n0.57\nN\n3\n\n\n3\n23\n65500\nRENT\n4.0\nMEDICAL\nC\n35000\n15.23\n1\n0.53\nN\n2\n\n\n4\n24\n54400\nRENT\n8.0\nMEDICAL\nC\n35000\n14.27\n1\n0.55\nY\n4\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nData Splitting\n\nfrom sklearn.model_selection import train_test_split\n\n\n# Splitting the input and output columns\n\ndef split_input_output(data, target_col):\n  X = data.drop((target_col), axis = 1)\n  y = data[target_col]\n  print(\"X shape: \" +str(X.shape))\n  print(\"y shape: \" + str(y.shape))\n  return X, y\n\n\nX, y = split_input_output(data=df,\n                          target_col='loan_status')\n\nX shape: (32416, 11)\ny shape: (32416,)\n\n\n\nX.head()\n\n\n  \n    \n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n22\n59000\nRENT\n123.0\nPERSONAL\nD\n35000\n16.02\n0.59\nY\n3\n\n\n1\n21\n9600\nOWN\n5.0\nEDUCATION\nB\n1000\n11.14\n0.10\nN\n2\n\n\n2\n25\n9600\nMORTGAGE\n1.0\nMEDICAL\nC\n5500\n12.87\n0.57\nN\n3\n\n\n3\n23\n65500\nRENT\n4.0\nMEDICAL\nC\n35000\n15.23\n0.53\nN\n2\n\n\n4\n24\n54400\nRENT\n8.0\nMEDICAL\nC\n35000\n14.27\n0.55\nY\n4\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ny.head()\n\n\n\n\n\n\n\n\nloan_status\n\n\nid\n\n\n\n\n\n0\n1\n\n\n1\n0\n\n\n2\n1\n\n\n3\n1\n\n\n4\n1\n\n\n\n\ndtype: int64\n\n\n\ny.value_counts(normalize = True)\n\n\n\n\n\n\n\n\nproportion\n\n\nloan_status\n\n\n\n\n\n0\n0.781312\n\n\n1\n0.218688\n\n\n\n\ndtype: float64\n\n\nVariabel y memiliki kelas yang imbalance sehingga perlu dilakukan stratified splitting pada train-test split.\n\n# Train test split\ndef split_train_test(X,y, test_size = 0.2, seed = 123):\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, stratify = y, random_state=seed)\n  print(\"X train shape  : \", X_train.shape)\n  print(\"y train shape  : \", y_train.shape)\n  print(\"X test shape   : \", X_test.shape)\n  print(\"y test shape   : \", y_test.shape)\n  print(\"\\n\")\n\n  return X_train, X_test, y_train, y_test\n\n\n# Training set\nX_train, X_test, y_train, y_test = split_train_test(X,y)\n\nX train shape  :  (25932, 11)\ny train shape  :  (25932,)\nX test shape   :  (6484, 11)\ny test shape   :  (6484,)\n\n\n\n\n\nprint(len(X_train)/len(X))\nprint(len(X_test)/len(X))\n\n0.7999753208292202\n0.20002467917077987\n\n\n\n# Memastikan data test dan train memiliki proporsi kelas yang sama\nprint(y_train.value_counts(normalize= True))\nprint(y_test.value_counts(normalize= True))\n\nloan_status\n0    0.781313\n1    0.218687\nName: proportion, dtype: float64\nloan_status\n0    0.781308\n1    0.218692\nName: proportion, dtype: float64\n\n\n\n\nEDA\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nX_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 25932 entries, 30 to 21613\nData columns (total 11 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   person_age                  25932 non-null  int64  \n 1   person_income               25932 non-null  int64  \n 2   person_home_ownership       25932 non-null  object \n 3   person_emp_length           25219 non-null  float64\n 4   loan_intent                 25932 non-null  object \n 5   loan_grade                  25932 non-null  object \n 6   loan_amnt                   25932 non-null  int64  \n 7   loan_int_rate               23433 non-null  float64\n 8   loan_percent_income         25932 non-null  float64\n 9   cb_person_default_on_file   25932 non-null  object \n 10  cb_person_cred_hist_length  25932 non-null  int64  \ndtypes: float64(3), int64(4), object(4)\nmemory usage: 2.4+ MB\n\n\nKeterangan: - Kolom person_home_ownership, loan_intent, loan_grade, dan cb_person_default_on_file bertipe kategorik. - Terdapat missing value pada person_emp_length(numerik) dan loan_int_rate (numerik)\n\nX_train.isna().sum()\n\n\n\n\n\n\n\n\n0\n\n\n\n\nperson_age\n0\n\n\nperson_income\n0\n\n\nperson_home_ownership\n0\n\n\nperson_emp_length\n713\n\n\nloan_intent\n0\n\n\nloan_grade\n0\n\n\nloan_amnt\n0\n\n\nloan_int_rate\n2499\n\n\nloan_percent_income\n0\n\n\ncb_person_default_on_file\n0\n\n\ncb_person_cred_hist_length\n0\n\n\n\n\ndtype: int64\n\n\n\n# Lakukan splitting Variabel x numerik dengan kategorik\ndef split_num_cat(data, num_cols, cat_cols):\n  data_num = data[num_cols]\n  data_cat = data[cat_cols]\n  print(\"Numeric Data shape: \"+ str(data_num.shape))\n  print(\"Categoric Data shape: \"+ str(data_cat.shape))\n\n  return data_num, data_cat\n\n\nnum_columns = ['person_age', 'person_income', 'person_emp_length',\n                'loan_amnt', 'loan_int_rate', 'loan_percent_income',\n                'cb_person_cred_hist_length' ]\n\ncat_columns = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']\n\nX_train_num, X_train_cat = split_num_cat(X_train, num_columns, cat_columns)\n\nNumeric Data shape: (25932, 7)\nCategoric Data shape: (25932, 4)\n\n\n\nStatistik Deskriptif Data Numerik\n\nX_train_num.describe().T\n\n\n  \n    \n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nperson_age\n25932.0\n27.721734\n6.290102\n20.00\n23.00\n26.00\n30.00\n144.00\n\n\nperson_income\n25932.0\n65694.299090\n51863.661672\n4000.00\n38400.00\n55000.00\n79000.00\n2039784.00\n\n\nperson_emp_length\n25219.0\n4.789405\n4.164592\n0.00\n2.00\n4.00\n7.00\n123.00\n\n\nloan_amnt\n25932.0\n9586.400008\n6316.672920\n500.00\n5000.00\n8000.00\n12250.00\n35000.00\n\n\nloan_int_rate\n23433.0\n11.013494\n3.238078\n5.42\n7.90\n10.99\n13.47\n23.22\n\n\nloan_percent_income\n25932.0\n0.170452\n0.106667\n0.00\n0.09\n0.15\n0.23\n0.83\n\n\ncb_person_cred_hist_length\n25932.0\n5.791108\n4.041546\n2.00\n3.00\n4.00\n8.00\n30.00\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nKeterangan: - Terdapat indikasi outlier pada kolom person_age, person_income, dan person_emp_length. - Kolom numerik memiliki distribusi nilai yang sangat berbeda satu sama lain.\n\n# Membuat subplot\nfig, axes = plt.subplots(len(num_columns), 2, figsize=(12, 16))\n\n# Membuat plot untuk setiap kolom numerik\nfor i, column in enumerate(num_columns):\n   # Histogram dengan KDE\n   sns.histplot(X_train_num[column], kde=True, ax=axes[i, 0], color='skyblue')\n   axes[i, 0].set_title(f'Histogram of {column}')\n   axes[i, 0].set_xlabel(column)\n   axes[i, 0].set_ylabel('Frequency')\n\n   # Boxplot\n   sns.boxplot(x=X_train_num[column], ax=axes[i, 1], color='lightgreen')\n   axes[i, 1].set_title(f'Boxplot of {column}')\n   axes[i, 1].set_xlabel(column)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nperson_age, person_income, dan person_emp_length memiliki anomali\n\n\nPemeriksaan Anomali Data Numerik\n\nX_train_num[X_train_num['person_age']&gt; 100]\n\n\n  \n    \n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n81\n144\n250000\n4.0\n4800\n13.57\n0.02\n3\n\n\n747\n123\n78000\n7.0\n20000\nNaN\n0.26\n4\n\n\n575\n123\n80004\n2.0\n20400\n10.25\n0.25\n3\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nX_train_num[ X_train_num['person_income'] &gt;= 1_000_000]\n\n\n  \n    \n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n32546\n60\n1900000\n5.0\n1500\nNaN\n0.00\n21\n\n\n31922\n47\n1362000\n9.0\n6600\n7.74\n0.00\n17\n\n\n29119\n36\n1200000\n16.0\n10000\n6.54\n0.01\n11\n\n\n30049\n42\n2039784\n0.0\n8450\n12.29\n0.00\n15\n\n\n17833\n32\n1200000\n1.0\n12000\n7.51\n0.01\n8\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nX_train_num[X_train_num['person_emp_length'] &gt; 60]\n\n\n  \n    \n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n0\n22\n59000\n123.0\n35000\n16.02\n0.59\n3\n\n\n210\n21\n192000\n123.0\n20000\n6.54\n0.10\n4\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nX_train_num[X_train_num['person_emp_length'] == 0]\n\n\n  \n    \n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n32528\n65\n120000\n0.0\n12000\n11.48\n0.10\n21\n\n\n1657\n24\n14400\n0.0\n1600\n18.25\n0.11\n3\n\n\n9934\n24\n65000\n0.0\n8000\n10.99\n0.12\n3\n\n\n18352\n29\n58000\n0.0\n20000\n9.91\n0.34\n10\n\n\n3441\n26\n45000\n0.0\n10800\n6.17\n0.24\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7958\n22\n54000\n0.0\n7000\n13.61\n0.13\n4\n\n\n2102\n24\n28800\n0.0\n2400\n14.26\n0.08\n4\n\n\n9267\n22\n60400\n0.0\n24000\n6.54\n0.40\n3\n\n\n6117\n22\n48000\n0.0\n3500\n6.54\n0.07\n3\n\n\n4656\n22\n72000\n0.0\n9975\n10.99\n0.14\n3\n\n\n\n\n3279 rows × 7 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nX_train_num[X_train_num['person_emp_length'] == 0].shape\n\n(3279, 7)\n\n\nCukup banyak baris yang memiliki kolom person_emp_length = 0, saya akan berasumsi bahwa hal ini berarti objek observasi tidak pernah bekerja untuk orang lain tetapi memiliki penghasilan misalnya pengusaha dan bukanlah sebuah anomali pada data.\n\nX_train_num[X_train_num['loan_percent_income'] == 0]\n\n\n  \n    \n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n17834\n34\n948000\n18.0\n2000\n9.99\n0.0\n7\n\n\n32546\n60\n1900000\n5.0\n1500\nNaN\n0.0\n21\n\n\n31916\n43\n780000\n2.0\n1000\n8.94\n0.0\n11\n\n\n31922\n47\n1362000\n9.0\n6600\n7.74\n0.0\n17\n\n\n30049\n42\n2039784\n0.0\n8450\n12.29\n0.0\n15\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nPengecekan Data Kategorik\n\nX_train_cat['person_home_ownership'].value_counts()\n\n\n\n\n\n\n\n\ncount\n\n\nperson_home_ownership\n\n\n\n\n\nRENT\n13159\n\n\nMORTGAGE\n10655\n\n\nOWN\n2033\n\n\nOTHER\n85\n\n\n\n\ndtype: int64\n\n\n\nX_train_cat['loan_intent'].value_counts()\n\n\n\n\n\n\n\n\ncount\n\n\nloan_intent\n\n\n\n\n\nEDUCATION\n5141\n\n\nMEDICAL\n4794\n\n\nVENTURE\n4561\n\n\nPERSONAL\n4407\n\n\nDEBTCONSOLIDATION\n4169\n\n\nHOMEIMPROVEMENT\n2860\n\n\n\n\ndtype: int64\n\n\n\nX_train_cat['loan_grade'].value_counts()\n\n\n\n\n\n\n\n\ncount\n\n\nloan_grade\n\n\n\n\n\nA\n8562\n\n\nB\n8338\n\n\nC\n5135\n\n\nD\n2881\n\n\nE\n759\n\n\nF\n206\n\n\nG\n51\n\n\n\n\ndtype: int64\n\n\n\nX_train_cat['cb_person_default_on_file'].value_counts()\n\n\n\n\n\n\n\n\ncount\n\n\ncb_person_default_on_file\n\n\n\n\n\nN\n21374\n\n\nY\n4558\n\n\n\n\ndtype: int64\n\n\nKolom cb_person_default_on_file perlu diubah menjadi boolean\n\n\nData Preprocessing Plan:\n\nHapus data anomali pada kolom numerik.\nHandle missing value pada person_emp_length dan loan_int_rate dengan imputasi nilai median\nLakukan Standarisasi pada kolom numerik.\nUbah cb_person_default_on_file menjadi boolean\nLakukan ordinal encoding pada kolom loan_grade\nlakukan one hot encoding pada person_home_ownership dan loan_intent\n\n\n\n\nData Preprocessing\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\nPenanganan Anomali pada Data\n\nage_anomaly = X_train_num[X_train_num['person_age']&gt; 100].index.tolist()\n\nincome_anomaly = X_train_num[X_train_num['person_income'] &gt;= 1_000_000].index.tolist()\n\nemp_length_anomaly = X_train_num[X_train_num['person_emp_length'] &gt; 60].index.tolist()\n\nidx_to_drop = set(emp_length_anomaly + income_anomaly + age_anomaly)\n\n\nprint(f'Number of index to drop:', len(idx_to_drop))\nidx_to_drop\n\nNumber of index to drop: 10\n\n\n{0, 81, 210, 575, 747, 17833, 29119, 30049, 31922, 32546}\n\n\n\nX_train_num_dropped = X_train_num.drop(index = idx_to_drop)\ny_train_dropped = y_train.drop(index = idx_to_drop)\n\n\nprint('Shape of X train after dropped:', X_train_num_dropped.shape)\nX_train_num_dropped.head()\n\nShape of X train after dropped: (25922, 7)\n\n\n\n  \n    \n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n30\n21\n11520\n5.0\n2000\n11.12\n0.17\n3\n\n\n25221\n28\n81120\n5.0\n5200\n11.49\n0.06\n7\n\n\n32528\n65\n120000\n0.0\n12000\n11.48\n0.10\n21\n\n\n9322\n26\n62000\n7.0\n8000\n12.99\n0.13\n4\n\n\n7010\n26\n51000\n2.0\n4000\n5.99\n0.08\n4\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nPenanganan Missing Value\n\ndef num_imputer_fit(data):\n  imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')\n\n  imputer.fit(data)\n\n  return imputer\n\ndef num_imputer_transform(data, imputer):\n  imputer.set_output(transform = \"pandas\")\n  data = imputer.transform(data)\n  return data\n\n\n# Get the numerical imputer\nnum_imputer = num_imputer_fit(X_train_num_dropped)\n\n# Transform the data\nX_train_imputed = num_imputer_transform(X_train_num_dropped, num_imputer)\n\n\n# Validasi hasil\nX_train_imputed.isna().sum()\n\n\n\n\n\n\n\n\n0\n\n\n\n\nperson_age\n0\n\n\nperson_income\n0\n\n\nperson_emp_length\n0\n\n\nloan_amnt\n0\n\n\nloan_int_rate\n0\n\n\nloan_percent_income\n0\n\n\ncb_person_cred_hist_length\n0\n\n\n\n\ndtype: int64\n\n\n\n\nStandardisasi\n\ndef fit_scaler(data):\n  scaler = StandardScaler()\n  scaler.fit(data)\n  return scaler\n\ndef transform_scaler(data, scaler):\n  scaler.set_output(transform = 'pandas')\n  data = scaler.transform(data)\n  return data\n\n\nscaler = fit_scaler(X_train_imputed)\n\nX_train_num_scaled = transform_scaler(X_train_imputed, scaler)\n\n\nX_train_num_scaled.head()\n\n\n  \n    \n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n30\n-1.083974\n-1.137217\n0.060787\n-1.201171\n0.035241\n-0.004407\n-0.690598\n\n\n25221\n0.047295\n0.331873\n0.060787\n-0.694398\n0.155448\n-1.036096\n0.299539\n\n\n32528\n6.026857\n1.152537\n-1.196967\n0.382496\n0.152199\n-0.660936\n3.765020\n\n\n9322\n-0.275925\n-0.071705\n0.563888\n-0.250971\n0.642772\n-0.379567\n-0.443064\n\n\n7010\n-0.275925\n-0.303889\n-0.693865\n-0.884438\n-1.631408\n-0.848516\n-0.443064\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Validasi hasil\nX_train_num_scaled.describe().round(4)\n\n\n  \n    \n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\n\n\n\n\ncount\n25922.0000\n25922.0000\n25922.0000\n25922.0000\n25922.0000\n25922.0000\n25922.0000\n\n\nmean\n-0.0000\n0.0000\n-0.0000\n0.0000\n0.0000\n-0.0000\n-0.0000\n\n\nstd\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\nmin\n-1.2456\n-1.2959\n-1.1970\n-1.4387\n-1.8166\n-1.5988\n-0.9381\n\n\n25%\n-0.7608\n-0.5698\n-0.6939\n-0.7261\n-0.8192\n-0.7547\n-0.6906\n\n\n50%\n-0.2759\n-0.2195\n-0.1908\n-0.2510\n-0.0070\n-0.1920\n-0.4431\n\n\n75%\n0.3705\n0.2871\n0.5639\n0.4221\n0.6818\n0.5583\n0.5471\n\n\nmax\n8.4510\n18.6296\n9.1166\n4.0249\n3.9663\n6.1857\n5.9928\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nPenanganan Inkonsistensi Format Data Kategorik\n\nX_train_cat.loc[:,'cb_person_default_on_file'] = (X_train_cat['cb_person_default_on_file'].replace({'N' : 0, 'Y' : 1 }).astype(int))\n\nFutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  X_train_cat.loc[:,'cb_person_default_on_file'] = (X_train_cat['cb_person_default_on_file'].replace({'N' : 0, 'Y' : 1 }).astype(int))\n\n\n\nX_train_cat.head()\n\n\n  \n    \n\n\n\n\n\n\nperson_home_ownership\nloan_intent\nloan_grade\ncb_person_default_on_file\n\n\nid\n\n\n\n\n\n\n\n\n30\nOWN\nMEDICAL\nB\n0\n\n\n25221\nMORTGAGE\nHOMEIMPROVEMENT\nB\n0\n\n\n32528\nMORTGAGE\nPERSONAL\nB\n0\n\n\n9322\nRENT\nPERSONAL\nC\n0\n\n\n7010\nMORTGAGE\nMEDICAL\nA\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nEncoding Data Kategorik\n\ndef cat_OHencode_fit(data):\n  OHencoder = OneHotEncoder(sparse_output=False, handle_unknown = 'ignore')\n  OHencoder.fit(data[['person_home_ownership',  'loan_intent']])\n  return OHencoder\n\ndef cat_ORDencode_fit(data):\n  ORDencoder = OrdinalEncoder()\n  ORDencoder.fit(data[['loan_grade']])\n  return ORDencoder\n\n\ndef cat_encoder_transform(data, onehot_encoder, ordinal_encoder):\n  OHencoded_data = onehot_encoder.transform(data[['person_home_ownership',  'loan_intent']])\n  ORDencoded_data = ordinal_encoder.transform(data[['loan_grade']])\n\n  # simpan index 'id' sebagai kolom\n  df_reset = data.reset_index()\n\n  # Ubah data menjadi Dataframe\n  df_OHencoded = pd.DataFrame(OHencoded_data, columns= onehot_encoder.get_feature_names_out(['person_home_ownership',   'loan_intent']))\n  df_ORDencoded = pd.DataFrame(ORDencoded_data, columns=['loan_grade'])\n\n  # Gabungkan sesuai index\n  df_encoded_combined_indexed = pd.concat([df_reset.drop(columns=['person_home_ownership',  'loan_intent','loan_grade']),df_OHencoded, df_ORDencoded], axis=1)\n  result_df = df_encoded_combined_indexed.set_index('id')\n\n  return result_df\n\n\n# Perform categorical imputation\ncat_OHencoder = cat_OHencode_fit(X_train_cat)\ncat_ORDencoder = cat_ORDencode_fit(X_train_cat)\n\n# Transform\nX_train_cat_encoded = cat_encoder_transform(X_train_cat, cat_OHencoder, cat_ORDencoder)\n\n\nX_train_cat_encoded\n\n\n  \n    \n\n\n\n\n\n\ncb_person_default_on_file\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\nloan_grade\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n30\n0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n25221\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n32528\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n9322\n0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n2.0\n\n\n7010\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n32519\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n27809\n0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n16801\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n95\n0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n21613\n0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n25932 rows × 12 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nPenggabungan Data Numerik dan Kategorik\n\ndef concat_data(num_data, cat_data):\n  print('\\nCleaned Numerical data shape: ' + str(num_data.shape))\n  print('Cleaned Categorical data shape: ' + str(cat_data.shape))\n\n  concated_data = num_data.join(cat_data, how = 'inner')\n  print('Concated data shape: ' + str(concated_data.shape))\n\n  return concated_data\n\n\nX_train_concat = concat_data(X_train_num_scaled ,X_train_cat_encoded)\n\n\nCleaned Numerical data shape: (25922, 7)\nCleaned Categorical data shape: (25932, 12)\nConcated data shape: (25922, 19)\n\n\n\nX_train_concat\n\n\n  \n    \n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\ncb_person_default_on_file\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\nloan_grade\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n30\n-1.083974\n-1.137217\n0.060787\n-1.201171\n0.035241\n-0.004407\n-0.690598\n0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n25221\n0.047295\n0.331873\n0.060787\n-0.694398\n0.155448\n-1.036096\n0.299539\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n32528\n6.026857\n1.152537\n-1.196967\n0.382496\n0.152199\n-0.660936\n3.765020\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n9322\n-0.275925\n-0.071705\n0.563888\n-0.250971\n0.642772\n-0.379567\n-0.443064\n0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n2.0\n\n\n7010\n-0.275925\n-0.303889\n-0.693865\n-0.884438\n-1.631408\n-0.848516\n-0.443064\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n32519\n3.764320\n1.065996\n2.827844\n0.382496\n-1.017379\n-0.660936\n5.745294\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n27809\n0.047295\n3.474375\n1.821641\n0.065762\n-0.019989\n-1.223676\n0.052005\n0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n16801\n-0.760754\n-0.219458\n-0.190764\n-0.250971\n-1.144083\n-0.191987\n-0.938132\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n95\n-0.760754\n1.152537\n-0.945416\n2.536283\n0.545307\n0.370753\n-0.690598\n0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n21613\n0.532124\n-0.335550\n-0.442315\n0.065762\n-0.084965\n0.276963\n0.052005\n0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n25922 rows × 19 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nMembuat Fungsi Preprocessing\n\ndef preprocess_data(data,num_cols, cat_cols,  num_imputer, scaler, cat_onehot_encoder, cat_ordinal_encoder):\n\n  #Splitting Numerik dan kategorik\n  X_num, X_cat = split_num_cat(data, num_cols, cat_cols)\n\n  #Penanganan Missing Value\n  X_num_imputed = num_imputer_transform(X_num, num_imputer)\n\n  #Standardisasi\n  X_train_num_clean = transform_scaler(X_num_imputed, scaler)\n\n  #Penganganan Inkonsistensi\n  X_cat.loc[:,'cb_person_default_on_file'] = (X_cat['cb_person_default_on_file'].replace({'N' : 0, 'Y' : 1 }).astype(int))\n\n  #Encoding Data Kategorik\n  X_cat_encoded = cat_encoder_transform(X_cat, cat_onehot_encoder, cat_ordinal_encoder)\n\n  #Gabungkan Data\n  cleaned_data = concat_data(X_train_num_clean,X_cat_encoded)\n\n  return cleaned_data"
  },
  {
    "objectID": "posts/project-2/DaffaDanielR_Mentoring_3_Introduction_to_Machine_Learning.html#task-2-modeling",
    "href": "posts/project-2/DaffaDanielR_Mentoring_3_Introduction_to_Machine_Learning.html#task-2-modeling",
    "title": "Mentoring 3",
    "section": "Task 2: Modeling",
    "text": "Task 2: Modeling\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\nfrom scipy.stats import randint,uniform\n\n\n# Preprocessing Data Test\nX_test_clean = preprocess_data(X_test,num_columns, cat_columns,  num_imputer, scaler, cat_OHencoder, cat_ORDencoder)\n\nNumeric Data shape: (6484, 7)\nCategoric Data shape: (6484, 4)\n\nCleaned Numerical data shape: (6484, 7)\nCleaned Categorical data shape: (6484, 12)\nConcated data shape: (6484, 19)\n\n\nFutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  X_cat.loc[:,'cb_person_default_on_file'] = (X_cat['cb_person_default_on_file'].replace({'N' : 0, 'Y' : 1 }).astype(int))\n\n\n\ny_test.shape\n\n(6484,)\n\n\n\nMetrics\nUntuk meminimalkan kejadian False Negative akan digunakan Recall sebagai evaluation metric utama.\n\n\nBaseline Model\n\ndummy_clf = dummy_clf = DummyClassifier(strategy = \"most_frequent\")\ndummy_clf.fit(X = X_train_concat,\n              y = y_train_dropped)\n\nDummyClassifier(strategy='most_frequent')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DummyClassifier?Documentation for DummyClassifieriFittedDummyClassifier(strategy='most_frequent') \n\n\n\ny_pred_dummy = dummy_clf.predict(X_train_concat)\n\nreport_dummy_model = classification_report(y_train_dropped, y_pred_dummy)\n\nprint(report_dummy_model)\n\n              precision    recall  f1-score   support\n\n           0       0.78      1.00      0.88     20252\n           1       0.00      0.00      0.00      5670\n\n    accuracy                           0.78     25922\n   macro avg       0.39      0.50      0.44     25922\nweighted avg       0.61      0.78      0.69     25922\n\n\n\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n\n\n\n\nBest Model Search\n\nkNN Classifier Model\n\nknn_clf = KNeighborsClassifier()\n\nparam_grid = {'n_neighbors': range(1, 21), 'metric': ['euclidean', 'manhattan']}\ngrid_search = GridSearchCV(knn_clf, param_grid, cv=5,scoring='recall')\ngrid_search.fit(X_train_concat, y_train_dropped)\n\nGridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid={'metric': ['euclidean', 'manhattan'],\n                         'n_neighbors': range(1, 21)},\n             scoring='recall')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid={'metric': ['euclidean', 'manhattan'],\n                         'n_neighbors': range(1, 21)},\n             scoring='recall') best_estimator_: KNeighborsClassifierKNeighborsClassifier(metric='manhattan', n_neighbors=1) KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier(metric='manhattan', n_neighbors=1) \n\n\n\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Mean cross-validated score of the best_estimator:\", grid_search.best_score_)\n\nBest Parameters: {'metric': 'manhattan', 'n_neighbors': 1}\nMean cross-validated score of the best_estimator: 0.6421516754850087\n\n\n\nknn_best_model = grid_search.best_estimator_\ny_pred_knn = knn_best_model.predict(X_test_clean)\n\nreport_knn_model = classification_report(y_test, y_pred_knn)\nprint(report_knn_model)\n\n              precision    recall  f1-score   support\n\n           0       0.90      0.92      0.91      5066\n           1       0.70      0.63      0.66      1418\n\n    accuracy                           0.86      6484\n   macro avg       0.80      0.78      0.79      6484\nweighted avg       0.86      0.86      0.86      6484\n\n\n\n\n\nDecision Tree\n\n#Decision Tree\ndtree_clf = DecisionTreeClassifier(random_state = 42)\n\nparam_distributions = {\n    'max_depth': np.arange(1, 50),\n    'min_samples_split': np.arange(2, 20),\n    'min_samples_leaf': np.arange(1, 20),\n    'max_features': [None, 'sqrt', 'log2'],\n    'criterion': ['gini', 'entropy']\n}\n\nrandomcv_dtree = RandomizedSearchCV(\n    estimator=dtree_clf,\n    param_distributions=param_distributions,\n    n_iter=50,\n    cv=5,\n    scoring = \"recall\",\n    random_state=42,\n    n_jobs=-1\n)\n\nrandomcv_dtree.fit(X = X_train_concat,\n              y = y_train_dropped)\n\nRandomizedSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42),\n                   n_iter=50, n_jobs=-1,\n                   param_distributions={'criterion': ['gini', 'entropy'],\n                                        'max_depth': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]),\n                                        'max_features': [None, 'sqrt', 'log2'],\n                                        'min_samples_leaf': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19]),\n                                        'min_samples_split': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n       19])},\n                   random_state=42, scoring='recall')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCV?Documentation for RandomizedSearchCViFittedRandomizedSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42),\n                   n_iter=50, n_jobs=-1,\n                   param_distributions={'criterion': ['gini', 'entropy'],\n                                        'max_depth': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]),\n                                        'max_features': [None, 'sqrt', 'log2'],\n                                        'min_samples_leaf': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19]),\n                                        'min_samples_split': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n       19])},\n                   random_state=42, scoring='recall') best_estimator_: DecisionTreeClassifierDecisionTreeClassifier(max_depth=26, min_samples_split=10, random_state=42) DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier(max_depth=26, min_samples_split=10, random_state=42) \n\n\n\n# Menampilkan Hasil Hyperparameter Terbaik\nprint(\"Best Parameters:\", randomcv_dtree.best_params_)\nprint(\"Mean cross-validated score of the best_estimator:\", randomcv_dtree.best_score_)\n\nBest Parameters: {'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 26, 'criterion': 'gini'}\nMean cross-validated score of the best_estimator: 0.7562610229276896\n\n\n\nDT_best_model = randomcv_dtree.best_estimator_\ny_pred_DT = DT_best_model.predict(X_test_clean)\n\nreport_DT_model = classification_report(y_test, y_pred_DT)\nprint(report_DT_model)\n\n              precision    recall  f1-score   support\n\n           0       0.93      0.96      0.94      5066\n           1       0.83      0.74      0.78      1418\n\n    accuracy                           0.91      6484\n   macro avg       0.88      0.85      0.86      6484\nweighted avg       0.91      0.91      0.91      6484\n\n\n\n\n\nLogistic Regresion\n\n#Logistic Regression\nlogreg_clf = LogisticRegression()\n\nparam_grid = {'max_iter': [100, 500, 1000],\n              'penalty' : [ 'l1', 'l2',None],\n              'solver' : ['liblinear', 'saga'],\n              'C': [0.01, 0.1, 1, 10, 100]\n              }\n\ngridcv_logreg = GridSearchCV(estimator= logreg_clf, param_grid=param_grid, cv=5, scoring='recall')\n\ngridcv_logreg.fit(X = X_train_concat,\n              y = y_train_dropped)\n\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n75 fits failed out of a total of 450.\nThe score on these train-test partitions for these parameters will be set to nan.\nIf these failures are not expected, you can try to debug them by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n75 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n    solver = _check_solver(self.solver, self.penalty, self.dual)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 76, in _check_solver\n    raise ValueError(\"penalty=None is not supported for the liblinear solver\")\nValueError: penalty=None is not supported for the liblinear solver\n\n  warnings.warn(some_fits_failed_message, FitFailedWarning)\n/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1107: UserWarning: One or more of the test scores are non-finite: [0.45837743 0.46525573 0.47813051 0.48201058        nan 0.50670194\n 0.45855379 0.46525573 0.47813051 0.48201058        nan 0.50670194\n 0.45855379 0.46525573 0.47813051 0.48201058        nan 0.50670194\n 0.5015873  0.50194004 0.5015873  0.5021164         nan 0.50670194\n 0.5015873  0.50194004 0.5015873  0.5021164         nan 0.50670194\n 0.5015873  0.50194004 0.5015873  0.5021164         nan 0.50670194\n 0.50617284 0.50599647 0.50617284 0.50599647        nan 0.50670194\n 0.50617284 0.50634921 0.50617284 0.50634921        nan 0.50670194\n 0.50617284 0.50634921 0.50617284 0.50634921        nan 0.50670194\n 0.50652557 0.50652557 0.50652557 0.50652557        nan 0.50670194\n 0.50652557 0.50652557 0.50652557 0.50652557        nan 0.50670194\n 0.50652557 0.50652557 0.50652557 0.50652557        nan 0.50670194\n 0.50652557 0.50652557 0.50670194 0.50652557        nan 0.50670194\n 0.50652557 0.50652557 0.50670194 0.50652557        nan 0.50670194\n 0.50652557 0.50652557 0.50670194 0.50652557        nan 0.50670194]\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n\n\nGridSearchCV(cv=5, estimator=LogisticRegression(),\n             param_grid={'C': [0.01, 0.1, 1, 10, 100],\n                         'max_iter': [100, 500, 1000],\n                         'penalty': ['l1', 'l2', None],\n                         'solver': ['liblinear', 'saga']},\n             scoring='recall')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5, estimator=LogisticRegression(),\n             param_grid={'C': [0.01, 0.1, 1, 10, 100],\n                         'max_iter': [100, 500, 1000],\n                         'penalty': ['l1', 'l2', None],\n                         'solver': ['liblinear', 'saga']},\n             scoring='recall') best_estimator_: LogisticRegressionLogisticRegression(C=0.01, penalty=None, solver='saga') LogisticRegression?Documentation for LogisticRegressionLogisticRegression(C=0.01, penalty=None, solver='saga') \n\n\n\nprint(\"Best Parameters:\", gridcv_logreg.best_params_)\nprint(\"Mean cross-validated score of the best_estimator:\", gridcv_logreg.best_score_)\n\nBest Parameters: {'C': 0.01, 'max_iter': 100, 'penalty': None, 'solver': 'saga'}\nMean cross-validated score of the best_estimator: 0.5067019400352735\n\n\n\nlogreg_best_model = gridcv_logreg.best_estimator_\ny_pred_logreg = logreg_best_model.predict(X_test_clean)\n\nreport_logreg_model = classification_report(y_test, y_pred_logreg)\nprint(report_logreg_model)\n\n              precision    recall  f1-score   support\n\n           0       0.88      0.95      0.91      5066\n           1       0.74      0.52      0.61      1418\n\n    accuracy                           0.85      6484\n   macro avg       0.81      0.73      0.76      6484\nweighted avg       0.85      0.85      0.84      6484\n\n\n\n\n\nSupport Vector Classifier Model\n\nsvc = SVC()\n\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'kernel': ['linear', 'rbf', 'poly'],\n    'gamma': ['scale', 'auto'],\n    'degree': [2, 3, 4]\n}\n\nrandomcv_svc = RandomizedSearchCV(\n    estimator=svc,\n    param_distributions=param_grid,\n    scoring='recall',\n    cv=5,\n    verbose=1,\n    n_jobs=-1\n)\n\nrandomcv_svc.fit(X = X_train_concat,\n              y = y_train_dropped)\n\nFitting 5 folds for each of 10 candidates, totalling 50 fits\n\n\nRandomizedSearchCV(cv=5, estimator=SVC(), n_jobs=-1,\n                   param_distributions={'C': [0.1, 1, 10, 100],\n                                        'degree': [2, 3, 4],\n                                        'gamma': ['scale', 'auto'],\n                                        'kernel': ['linear', 'rbf', 'poly']},\n                   scoring='recall', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCV?Documentation for RandomizedSearchCViFittedRandomizedSearchCV(cv=5, estimator=SVC(), n_jobs=-1,\n                   param_distributions={'C': [0.1, 1, 10, 100],\n                                        'degree': [2, 3, 4],\n                                        'gamma': ['scale', 'auto'],\n                                        'kernel': ['linear', 'rbf', 'poly']},\n                   scoring='recall', verbose=1) best_estimator_: SVCSVC(C=10, kernel='poly') SVC?Documentation for SVCSVC(C=10, kernel='poly') \n\n\n\nprint(\"Best Parameters:\", randomcv_svc.best_params_)\nprint(\"Mean cross-validated score of the best_estimator:\", randomcv_svc.best_score_)\n\nBest Parameters: {'kernel': 'poly', 'gamma': 'scale', 'degree': 3, 'C': 10}\nMean cross-validated score of the best_estimator: 0.6520282186948854\n\n\n\ny_pred_svc = grid_search.best_estimator_.predict(X_test_clean)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_svc))\n\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.90      0.92      0.91      5066\n           1       0.70      0.63      0.66      1418\n\n    accuracy                           0.86      6484\n   macro avg       0.80      0.78      0.79      6484\nweighted avg       0.86      0.86      0.86      6484\n\n\n\n\n\nBagging Decision Tree\n\n\ntree = DecisionTreeClassifier(random_state=123)\nbagging_model = BaggingClassifier(estimator= tree, random_state=123)\n\n# Parameter grid\nparam_distributions = {\n    'n_estimators': randint(10, 100),\n    'max_samples': [0.5, 0.7, 1.0],\n    'max_features': [0.5, 0.7, 1.0],\n    'bootstrap': [True, False],\n    'bootstrap_features': [True, False],\n    'estimator__max_depth': randint(5, 30),\n    'estimator__min_samples_split': randint(2, 20),\n}\n\n# RandomizedSearchCV\nrandom_search_bagging = RandomizedSearchCV(\n    estimator=bagging_model,\n    param_distributions=param_distributions,\n    n_iter=20,\n    cv=5,\n    scoring='recall',\n    n_jobs=-1,\n    random_state=123\n)\n\nrandom_search_bagging.fit(X = X_train_concat, y = y_train_dropped)\n\nRandomizedSearchCV(cv=5,\n                   estimator=BaggingClassifier(estimator=DecisionTreeClassifier(random_state=123),\n                                               random_state=123),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'bootstrap': [True, False],\n                                        'bootstrap_features': [True, False],\n                                        'estimator__max_depth': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeed9eae10&gt;,\n                                        'estimator__min_samples_split': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeeda2e950&gt;,\n                                        'max_features': [0.5, 0.7, 1.0],\n                                        'max_samples': [0.5, 0.7, 1.0],\n                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeeda36390&gt;},\n                   random_state=123, scoring='recall')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCV?Documentation for RandomizedSearchCViFittedRandomizedSearchCV(cv=5,\n                   estimator=BaggingClassifier(estimator=DecisionTreeClassifier(random_state=123),\n                                               random_state=123),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'bootstrap': [True, False],\n                                        'bootstrap_features': [True, False],\n                                        'estimator__max_depth': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeed9eae10&gt;,\n                                        'estimator__min_samples_split': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeeda2e950&gt;,\n                                        'max_features': [0.5, 0.7, 1.0],\n                                        'max_samples': [0.5, 0.7, 1.0],\n                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeeda36390&gt;},\n                   random_state=123, scoring='recall') best_estimator_: BaggingClassifierBaggingClassifier(bootstrap=False,\n                  estimator=DecisionTreeClassifier(max_depth=24,\n                                                   min_samples_split=8,\n                                                   random_state=123),\n                  max_features=0.7, n_estimators=93, random_state=123) estimator: DecisionTreeClassifierDecisionTreeClassifier(max_depth=24, min_samples_split=8, random_state=123) DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier(max_depth=24, min_samples_split=8, random_state=123) \n\n\n\nprint(\"Best Parameters:\", random_search_bagging.best_params_)\nprint(\"Mean cross-validated score of the best_estimator:\", random_search_bagging.best_score_)\n\nBest Parameters: {'bootstrap': False, 'bootstrap_features': False, 'estimator__max_depth': 24, 'estimator__min_samples_split': 8, 'max_features': 0.7, 'max_samples': 1.0, 'n_estimators': 93}\nMean cross-validated score of the best_estimator: 0.7331569664902998\n\n\n\nbagging_best_model = random_search_bagging.best_estimator_\ny_pred_bagging = bagging_best_model.predict(X_test_clean)\n\nreport_bagging_model = classification_report(y_test, y_pred_bagging)\nprint(report_bagging_model)\n\n              precision    recall  f1-score   support\n\n           0       0.93      0.99      0.96      5066\n           1       0.96      0.71      0.82      1418\n\n    accuracy                           0.93      6484\n   macro avg       0.95      0.85      0.89      6484\nweighted avg       0.93      0.93      0.93      6484\n\n\n\n\n\nRandom Forest\n\nrf_clf = RandomForestClassifier(random_state=42)\n\nparam_dist = {\n    'n_estimators': randint(50, 200),  # Jumlah trees antara 50-200\n    'max_depth': [None, 10, 20, 30],  # Kedalaman maksimal tree\n    'min_samples_split': randint(2, 10),  # Minimal sampel untuk split\n    'min_samples_leaf': randint(1, 5),   # Minimal sampel di leaf node\n    'criterion': ['gini', 'entropy'],    # Fungsi untuk split\n}\n\nrandomcv_rf = RandomizedSearchCV(rf_clf, param_distributions=param_dist, n_iter=20, cv=5, scoring='recall', random_state = 42)\nrandomcv_rf.fit(X_train_concat, y_train_dropped)\n\nRandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n                   n_iter=20,\n                   param_distributions={'criterion': ['gini', 'entropy'],\n                                        'max_depth': [None, 10, 20, 30],\n                                        'min_samples_leaf': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeed9ee550&gt;,\n                                        'min_samples_split': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeeda33190&gt;,\n                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeed9b5490&gt;},\n                   random_state=42, scoring='recall')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCV?Documentation for RandomizedSearchCViFittedRandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n                   n_iter=20,\n                   param_distributions={'criterion': ['gini', 'entropy'],\n                                        'max_depth': [None, 10, 20, 30],\n                                        'min_samples_leaf': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeed9ee550&gt;,\n                                        'min_samples_split': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeeda33190&gt;,\n                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeed9b5490&gt;},\n                   random_state=42, scoring='recall') best_estimator_: RandomForestClassifierRandomForestClassifier(max_depth=30, min_samples_split=8, n_estimators=156,\n                       random_state=42) RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(max_depth=30, min_samples_split=8, n_estimators=156,\n                       random_state=42) \n\n\n\nprint(\"Best Parameters:\", randomcv_rf.best_params_)\nprint(\"Best Cross-Validated recall:\", randomcv_rf.best_score_)\n\nBest Parameters: {'criterion': 'gini', 'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 8, 'n_estimators': 156}\nBest Cross-Validated recall: 0.718342151675485\n\n\n\nrf_best_model = randomcv_rf.best_estimator_\ny_pred_rf = rf_best_model.predict(X_test_clean)\n\n\nreport_rf_model = classification_report(y_test, y_pred_rf)\nprint(report_rf_model)\n\n              precision    recall  f1-score   support\n\n           0       0.92      1.00      0.96      5066\n           1       0.98      0.70      0.82      1418\n\n    accuracy                           0.93      6484\n   macro avg       0.95      0.85      0.89      6484\nweighted avg       0.94      0.93      0.93      6484\n\n\n\n\n\nAdaboost Model\n\n\ntree = DecisionTreeClassifier(random_state=123)\nadaboost_model = AdaBoostClassifier(estimator= tree, random_state=123)\n\nparam_distributions = {\n    'n_estimators': randint(50, 200),\n    'learning_rate': uniform(0.01, 1.0),\n    'estimator__max_depth': randint(1, 5),\n    'estimator__min_samples_split': randint(2, 20),\n    'estimator__min_samples_leaf': randint(1, 10),\n}\n\nrandom_search_adaboost = RandomizedSearchCV(\n    estimator=adaboost_model,\n    param_distributions=param_distributions,\n    n_iter=20,\n    cv=5,\n    scoring='recall',\n    n_jobs=-1,\n    random_state=123\n)\n\n\nrandom_search_adaboost.fit(X_train_concat, y_train_dropped)\n\nRandomizedSearchCV(cv=5,\n                   estimator=AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=123),\n                                                random_state=123),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'estimator__max_depth': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeed07a290&gt;,\n                                        'estimator__min_samples_leaf': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeed591b50&gt;,\n                                        'estimator__min_samples_split': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeed07a050&gt;,\n                                        'learning_rate': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7ebeeda15990&gt;,\n                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeeda14890&gt;},\n                   random_state=123, scoring='recall')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCV?Documentation for RandomizedSearchCViFittedRandomizedSearchCV(cv=5,\n                   estimator=AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=123),\n                                                random_state=123),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'estimator__max_depth': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeed07a290&gt;,\n                                        'estimator__min_samples_leaf': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeed591b50&gt;,\n                                        'estimator__min_samples_split': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeed07a050&gt;,\n                                        'learning_rate': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7ebeeda15990&gt;,\n                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x7ebeeda14890&gt;},\n                   random_state=123, scoring='recall') best_estimator_: AdaBoostClassifierAdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3,\n                                                    min_samples_leaf=3,\n                                                    min_samples_split=3,\n                                                    random_state=123),\n                   learning_rate=0.9979952865166672, n_estimators=181,\n                   random_state=123) estimator: DecisionTreeClassifierDecisionTreeClassifier(max_depth=3, min_samples_leaf=3, min_samples_split=3,\n                       random_state=123) DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier(max_depth=3, min_samples_leaf=3, min_samples_split=3,\n                       random_state=123) \n\n\n\nprint(\"Best Parameters:\", random_search_adaboost.best_params_)\nprint(\"Best Cross-Validated recall:\", random_search_adaboost.best_score_)\n\nBest Parameters: {'estimator__max_depth': 3, 'estimator__min_samples_leaf': 3, 'estimator__min_samples_split': 3, 'learning_rate': 0.9979952865166672, 'n_estimators': 181}\nBest Cross-Validated recall: 0.7283950617283951\n\n\n\nadaboost_best_model = random_search_adaboost.best_estimator_\ny_pred_adaboost = adaboost_best_model.predict(X_test_clean)\n\nreport_adaboost = classification_report(y_test, y_pred_adaboost)\nprint(report_adaboost)\n\n              precision    recall  f1-score   support\n\n           0       0.92      0.98      0.95      5066\n           1       0.90      0.71      0.79      1418\n\n    accuracy                           0.92      6484\n   macro avg       0.91      0.84      0.87      6484\nweighted avg       0.92      0.92      0.92      6484\n\n\n\n\n\n\nFinal Best Model\n\n# Best Model: Decision Tree\nbest_model = randomcv_dtree.best_estimator_"
  },
  {
    "objectID": "posts/project-2/DaffaDanielR_Mentoring_3_Introduction_to_Machine_Learning.html#task-3-model-evaluation",
    "href": "posts/project-2/DaffaDanielR_Mentoring_3_Introduction_to_Machine_Learning.html#task-3-model-evaluation",
    "title": "Mentoring 3",
    "section": "Task 3: Model Evaluation",
    "text": "Task 3: Model Evaluation\n\nScore on test data\n\ny_pred_dt = best_model.predict(X_test_clean)\nreport_dt = classification_report(y_test, y_pred_dt)\nprint(report_dt)\n\n              precision    recall  f1-score   support\n\n           0       0.93      0.96      0.94      5066\n           1       0.83      0.74      0.78      1418\n\n    accuracy                           0.91      6484\n   macro avg       0.88      0.85      0.86      6484\nweighted avg       0.91      0.91      0.91      6484\n\n\n\n\n\nFinancial Impact Comparison\n\n# False Negative potential loss : 35juta\n# False positive potential loss : 10 juta\n\ny_pred_dummy = dummy_clf.predict(X_test)\n\nmodel_predictions = {\n    'dummy': y_pred_dummy,\n    'knn': y_pred_knn,\n    'dt': y_pred_dt,\n    'logreg': y_pred_logreg,\n    'svc': y_pred_svc,\n    'bagging': y_pred_bagging,\n    'rf': y_pred_rf,\n    'boost': y_pred_adaboost\n}\n\nconfusion_matrices = {}\n\nfor model_name, y_pred in model_predictions.items():\n    confusion_matrices[model_name] = confusion_matrix(y_test, y_pred).ravel()\n\n#Financial Loss\ncost_fn = 35  # dalam juta\ncost_fp = 10\n\nfinancial_losses = {}\n\n# Hitung Total Loss\nfor model_name, cm_values in confusion_matrices.items():\n    fn, fp = cm_values[2], cm_values[1]  # FN dan FP berada di indeks 2 dan 1\n    financial_losses[model_name] = fn * cost_fn + fp * cost_fp\n\n\ncomparison_data = []\n\n# Loop untuk mengisi data financial comparison\nfor model_name, cm_values in confusion_matrices.items():\n    fn, fp = cm_values[2], cm_values[1]  # FN dan FP berada di indeks 2 dan 1\n    total_loss = fn * cost_fn + fp * cost_fp\n    comparison_data.append({\n        'Model': model_name.capitalize(),        # Nama model dengan huruf kapital\n        'False Negative (FN)': fn,              # Jumlah FN\n        'False Positive (FP)': fp,              # Jumlah FP\n        'Total Loss (Rp juta)': total_loss      # Total loss dalam juta\n    })\n\n# Buat DataFrame dari data yang terkumpul\nfinancial_comparison = pd.DataFrame(comparison_data)\n\n# Tampilkan DataFrame\nfinancial_comparison\n\n\n  \n    \n\n\n\n\n\n\nModel\nFalse Negative (FN)\nFalse Positive (FP)\nTotal Loss (Rp juta)\n\n\n\n\n0\nDummy\n1418\n0\n49630\n\n\n1\nKnn\n522\n383\n22100\n\n\n2\nDt\n373\n217\n15225\n\n\n3\nLogreg\n686\n255\n26560\n\n\n4\nSvc\n522\n383\n22100\n\n\n5\nBagging\n405\n37\n14545\n\n\n6\nRf\n424\n18\n15020\n\n\n7\nBoost\n415\n110\n15625\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nfinancial_comparison_sorted = financial_comparison.sort_values(\n    by='Total Loss (Rp juta)',ascending = False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    data=financial_comparison_sorted,\n    x='Model',\n    y='Total Loss (Rp juta)',\n    palette='Blues_d'  # Pilih palet warna\n)\n\n# Tambahkan label dan judul\nplt.title('Financial Loss by Model', fontsize=16)\nplt.xlabel('Model', fontsize=12)\nplt.ylabel('Total Loss (Rp juta)', fontsize=12)\n\n\n\n# Tampilkan plot\nplt.tight_layout()\nplt.show()\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(\n\n\n\n\n\n\n\n\n\nModel Bagging memiliki potential loss paling rendah"
  }
]