{
  "hash": "e0596ec18e0edbe0252af9cec338669e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Mentoring 3\ntoc: true\ntoc-depth: 4\njupyter: python3\nexecute:\n  eval: false\n---\n\n\n\n\n# Dataset Description\n\nDataset Link: https://www.kaggle.com/datasets/laotse/credit-risk-dataset\n\nDataset berisi data terkait peminjam uang.\nData ini memiliki 12 variabel dengan `loan_status` sebagai variabel dependen (output) dan sisanya sebagai variabel independen (input).\n\nBerikut adalah deskripsi mengenai arti dari setiap kolom pada data:\n\n<div style=\"text-align: center;\">\n\n|Feature Name|\tDescription|\n|:--|:--|\n|person_age\t|Age|\n|person_income|\tAnnual Income|\n|person_home_ownership|\tHome ownership|\n|person_emp_length|\tEmployment length (in years)|\n|loan_intent|\tLoan intent|\n|loan_grade|\tExpected Risk Grade|\n|loan_amnt|\tLoan amount|\n|loan_int_rate|\tInterest rate|\n|loan_status|\t0 : non default, 1: default|\n|loan_percent_income|\tPercent income|\n|cb_person_default_on_file|\tHistorical default|\n|cb_preson_cred_hist_length|\tCredit history length|\n\n</div>\n\nCatatan:\n- Tidak terdapat keterangan mengenai mata uang yang digunakan pada `person_income` dan `loan_amnt`\n- `loan_grade` adalah klasifikasi ekspektasi risiko pemberian pinjaman, yaitu A sampai G untuk pinjaman dengan risiko default (gagal bayar) yang rendah hingga tinggi. (sumber:https://blog.groundfloor.com/groundfloorblog/about-loan-grading)\n\n# Modeling Workflow\n\nTujuan: Membuat model Classifier (default, non default) untuk meminimalkan potensi kerugian.\n\n## Task 1: Data Preparation\n\n::: {#45a4a2cc .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n```\n:::\n\n\n### Load the Data\n\n::: {#56cbb20f .cell execution_count=2}\n``` {.python .cell-code}\ndef read_data(fname):\n  filename = '/content/' + fname\n\n  # read csv as pandas dataframe\n  dataset = pd.read_csv(filename)\n  dataset['id'] = dataset.index\n  dataset.set_index('id', inplace = True)\n  print('Data shape raw               : ', dataset.shape)\n\n  # drop duplicates\n  print('Number of duplicates         : ', dataset.duplicated().sum())\n\n  dataset.drop_duplicates(keep = 'last', inplace = True)\n\n  # print data shape\n  print('Data shape after dropping    : ', dataset.shape)\n\n  return dataset\n```\n:::\n\n\n::: {#962e418c .cell execution_count=3}\n``` {.python .cell-code}\ndf = read_data(fname= 'credit_risk_dataset.csv')\n```\n:::\n\n\n::: {#f9ac199a .cell execution_count=4}\n``` {.python .cell-code}\ndf.head()\n```\n:::\n\n\n### Data Splitting\n\n::: {#7b28eb4b .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\n```\n:::\n\n\n::: {#ee46dab7 .cell execution_count=6}\n``` {.python .cell-code}\n# Splitting the input and output columns\n\ndef split_input_output(data, target_col):\n  X = data.drop((target_col), axis = 1)\n  y = data[target_col]\n  print(\"X shape: \" +str(X.shape))\n  print(\"y shape: \" + str(y.shape))\n  return X, y\n```\n:::\n\n\n::: {#6858532a .cell execution_count=7}\n``` {.python .cell-code}\nX, y = split_input_output(data=df,\n                          target_col='loan_status')\n```\n:::\n\n\n::: {#fcc79fe4 .cell execution_count=8}\n``` {.python .cell-code}\nX.head()\n```\n:::\n\n\n::: {#37c81b2d .cell execution_count=9}\n``` {.python .cell-code}\ny.head()\n```\n:::\n\n\n::: {#2e214966 .cell execution_count=10}\n``` {.python .cell-code}\ny.value_counts(normalize = True)\n```\n:::\n\n\nVariabel y memiliki kelas yang imbalance sehingga perlu dilakukan stratified splitting pada train-test split.\n\n::: {#61664f66 .cell execution_count=11}\n``` {.python .cell-code}\n# Train test split\ndef split_train_test(X,y, test_size = 0.2, seed = 123):\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, stratify = y, random_state=seed)\n  print(\"X train shape  : \", X_train.shape)\n  print(\"y train shape  : \", y_train.shape)\n  print(\"X test shape   : \", X_test.shape)\n  print(\"y test shape   : \", y_test.shape)\n  print(\"\\n\")\n\n  return X_train, X_test, y_train, y_test\n```\n:::\n\n\n::: {#eb4ae1ab .cell execution_count=12}\n``` {.python .cell-code}\n# Training set\nX_train, X_test, y_train, y_test = split_train_test(X,y)\n```\n:::\n\n\n::: {#766843b1 .cell execution_count=13}\n``` {.python .cell-code}\nprint(len(X_train)/len(X))\nprint(len(X_test)/len(X))\n```\n:::\n\n\n::: {#ee37933d .cell execution_count=14}\n``` {.python .cell-code}\n# Memastikan data test dan train memiliki proporsi kelas yang sama\nprint(y_train.value_counts(normalize= True))\nprint(y_test.value_counts(normalize= True))\n```\n:::\n\n\n### EDA\n\n::: {#bde63612 .cell execution_count=15}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n```\n:::\n\n\n::: {#c61a9896 .cell execution_count=16}\n``` {.python .cell-code}\nX_train.info()\n```\n:::\n\n\nKeterangan:\n- Kolom `person_home_ownership`, `loan_intent`, `loan_grade`, dan  `cb_person_default_on_file` bertipe kategorik.\n- Terdapat missing value pada `person_emp_length`(numerik) dan `loan_int_rate` (numerik)\n\n::: {#7e422f0c .cell execution_count=17}\n``` {.python .cell-code}\nX_train.isna().sum()\n```\n:::\n\n\n::: {#4dbc894f .cell execution_count=18}\n``` {.python .cell-code}\n# Lakukan splitting Variabel x numerik dengan kategorik\ndef split_num_cat(data, num_cols, cat_cols):\n  data_num = data[num_cols]\n  data_cat = data[cat_cols]\n  print(\"Numeric Data shape: \"+ str(data_num.shape))\n  print(\"Categoric Data shape: \"+ str(data_cat.shape))\n\n  return data_num, data_cat\n```\n:::\n\n\n::: {#2dfa93f5 .cell execution_count=19}\n``` {.python .cell-code}\nnum_columns = ['person_age', 'person_income', 'person_emp_length',\n                'loan_amnt', 'loan_int_rate', 'loan_percent_income',\n                'cb_person_cred_hist_length' ]\n\ncat_columns = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']\n\nX_train_num, X_train_cat = split_num_cat(X_train, num_columns, cat_columns)\n```\n:::\n\n\n#### Statistik Deskriptif Data Numerik\n\n::: {#4b5d91d3 .cell execution_count=20}\n``` {.python .cell-code}\nX_train_num.describe().T\n```\n:::\n\n\nKeterangan:\n- Terdapat indikasi outlier pada kolom `person_age`, `person_income`, dan  `person_emp_length`.\n- Kolom numerik memiliki distribusi nilai yang sangat berbeda satu sama lain.\n\n::: {#d9f953fb .cell execution_count=21}\n``` {.python .cell-code}\n# Membuat subplot\nfig, axes = plt.subplots(len(num_columns), 2, figsize=(12, 16))\n\n# Membuat plot untuk setiap kolom numerik\nfor i, column in enumerate(num_columns):\n   # Histogram dengan KDE\n   sns.histplot(X_train_num[column], kde=True, ax=axes[i, 0], color='skyblue')\n   axes[i, 0].set_title(f'Histogram of {column}')\n   axes[i, 0].set_xlabel(column)\n   axes[i, 0].set_ylabel('Frequency')\n\n   # Boxplot\n   sns.boxplot(x=X_train_num[column], ax=axes[i, 1], color='lightgreen')\n   axes[i, 1].set_title(f'Boxplot of {column}')\n   axes[i, 1].set_xlabel(column)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n`person_age`, `person_income`, dan `person_emp_length` memiliki anomali\n\n#### Pemeriksaan Anomali Data Numerik\n\n::: {#615049e2 .cell execution_count=22}\n``` {.python .cell-code}\nX_train_num[X_train_num['person_age']> 100]\n```\n:::\n\n\n::: {#d4f5fae8 .cell execution_count=23}\n``` {.python .cell-code}\nX_train_num[ X_train_num['person_income'] >= 1_000_000]\n```\n:::\n\n\n::: {#346d8883 .cell execution_count=24}\n``` {.python .cell-code}\nX_train_num[X_train_num['person_emp_length'] > 60]\n```\n:::\n\n\n::: {#bc893a23 .cell execution_count=25}\n``` {.python .cell-code}\nX_train_num[X_train_num['person_emp_length'] == 0]\n```\n:::\n\n\n::: {#286ad154 .cell execution_count=26}\n``` {.python .cell-code}\nX_train_num[X_train_num['person_emp_length'] == 0].shape\n```\n:::\n\n\nCukup banyak baris yang memiliki kolom `person_emp_length` = 0, saya akan berasumsi bahwa hal ini berarti objek observasi tidak pernah bekerja untuk orang lain tetapi memiliki penghasilan misalnya pengusaha dan bukanlah sebuah anomali pada data.  \n\n::: {#f107bbcc .cell execution_count=27}\n``` {.python .cell-code}\nX_train_num[X_train_num['loan_percent_income'] == 0]\n```\n:::\n\n\n#### Pengecekan Data Kategorik\n\n::: {#1cc43ae3 .cell execution_count=28}\n``` {.python .cell-code}\nX_train_cat['person_home_ownership'].value_counts()\n```\n:::\n\n\n::: {#ed516ca3 .cell execution_count=29}\n``` {.python .cell-code}\nX_train_cat['loan_intent'].value_counts()\n```\n:::\n\n\n::: {#5d49b84b .cell execution_count=30}\n``` {.python .cell-code}\nX_train_cat['loan_grade'].value_counts()\n```\n:::\n\n\n::: {#f0bae193 .cell execution_count=31}\n``` {.python .cell-code}\nX_train_cat['cb_person_default_on_file'].value_counts()\n```\n:::\n\n\nKolom `cb_person_default_on_file` perlu diubah menjadi boolean\n\n#### Data Preprocessing Plan:\n- Hapus data anomali pada kolom numerik.\n- Handle missing value pada `person_emp_length` dan `loan_int_rate` dengan imputasi nilai median\n- Lakukan Standarisasi pada kolom numerik.\n- Ubah `cb_person_default_on_file` menjadi boolean\n- Lakukan ordinal encoding pada kolom `loan_grade`\n- lakukan one hot encoding pada `person_home_ownership` dan `loan_intent`\n\n### Data Preprocessing\n\n::: {#3a7ae17b .cell execution_count=32}\n``` {.python .cell-code}\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\n```\n:::\n\n\n####  Penanganan Anomali pada Data\n\n::: {#27774b52 .cell execution_count=33}\n``` {.python .cell-code}\nage_anomaly = X_train_num[X_train_num['person_age']> 100].index.tolist()\n\nincome_anomaly = X_train_num[X_train_num['person_income'] >= 1_000_000].index.tolist()\n\nemp_length_anomaly = X_train_num[X_train_num['person_emp_length'] > 60].index.tolist()\n\nidx_to_drop = set(emp_length_anomaly + income_anomaly + age_anomaly)\n```\n:::\n\n\n::: {#b0ce27d4 .cell execution_count=34}\n``` {.python .cell-code}\nprint(f'Number of index to drop:', len(idx_to_drop))\nidx_to_drop\n```\n:::\n\n\n::: {#729259de .cell execution_count=35}\n``` {.python .cell-code}\nX_train_num_dropped = X_train_num.drop(index = idx_to_drop)\ny_train_dropped = y_train.drop(index = idx_to_drop)\n```\n:::\n\n\n::: {#2d403d36 .cell execution_count=36}\n``` {.python .cell-code}\nprint('Shape of X train after dropped:', X_train_num_dropped.shape)\nX_train_num_dropped.head()\n```\n:::\n\n\n#### Penanganan Missing Value\n\n::: {#228c9951 .cell execution_count=37}\n``` {.python .cell-code}\ndef num_imputer_fit(data):\n  imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')\n\n  imputer.fit(data)\n\n  return imputer\n\ndef num_imputer_transform(data, imputer):\n  imputer.set_output(transform = \"pandas\")\n  data = imputer.transform(data)\n  return data\n```\n:::\n\n\n::: {#7603e257 .cell execution_count=38}\n``` {.python .cell-code}\n# Get the numerical imputer\nnum_imputer = num_imputer_fit(X_train_num_dropped)\n\n# Transform the data\nX_train_imputed = num_imputer_transform(X_train_num_dropped, num_imputer)\n```\n:::\n\n\n::: {#3f226280 .cell execution_count=39}\n``` {.python .cell-code}\n# Validasi hasil\nX_train_imputed.isna().sum()\n```\n:::\n\n\n#### Standardisasi\n\n::: {#475cb45d .cell execution_count=40}\n``` {.python .cell-code}\ndef fit_scaler(data):\n  scaler = StandardScaler()\n  scaler.fit(data)\n  return scaler\n\ndef transform_scaler(data, scaler):\n  scaler.set_output(transform = 'pandas')\n  data = scaler.transform(data)\n  return data\n```\n:::\n\n\n::: {#b3730b84 .cell execution_count=41}\n``` {.python .cell-code}\nscaler = fit_scaler(X_train_imputed)\n\nX_train_num_scaled = transform_scaler(X_train_imputed, scaler)\n```\n:::\n\n\n::: {#f6182c38 .cell execution_count=42}\n``` {.python .cell-code}\nX_train_num_scaled.head()\n```\n:::\n\n\n::: {#520eed36 .cell execution_count=43}\n``` {.python .cell-code}\n# Validasi hasil\nX_train_num_scaled.describe().round(4)\n```\n:::\n\n\n#### Penanganan Inkonsistensi Format Data Kategorik\n\n::: {#36d7d88e .cell execution_count=44}\n``` {.python .cell-code}\nX_train_cat.loc[:,'cb_person_default_on_file'] = (X_train_cat['cb_person_default_on_file'].replace({'N' : 0, 'Y' : 1 }).astype(int))\n```\n:::\n\n\n::: {#ae5cc458 .cell execution_count=45}\n``` {.python .cell-code}\nX_train_cat.head()\n```\n:::\n\n\n#### Encoding Data Kategorik\n\n::: {#9bd3624a .cell execution_count=46}\n``` {.python .cell-code}\ndef cat_OHencode_fit(data):\n  OHencoder = OneHotEncoder(sparse_output=False, handle_unknown = 'ignore')\n  OHencoder.fit(data[['person_home_ownership',\t'loan_intent']])\n  return OHencoder\n\ndef cat_ORDencode_fit(data):\n  ORDencoder = OrdinalEncoder()\n  ORDencoder.fit(data[['loan_grade']])\n  return ORDencoder\n\n\ndef cat_encoder_transform(data, onehot_encoder, ordinal_encoder):\n  OHencoded_data = onehot_encoder.transform(data[['person_home_ownership',\t'loan_intent']])\n  ORDencoded_data = ordinal_encoder.transform(data[['loan_grade']])\n\n  # simpan index 'id' sebagai kolom\n  df_reset = data.reset_index()\n\n  # Ubah data menjadi Dataframe\n  df_OHencoded = pd.DataFrame(OHencoded_data, columns= onehot_encoder.get_feature_names_out(['person_home_ownership',\t'loan_intent']))\n  df_ORDencoded = pd.DataFrame(ORDencoded_data, columns=['loan_grade'])\n\n  # Gabungkan sesuai index\n  df_encoded_combined_indexed = pd.concat([df_reset.drop(columns=['person_home_ownership',\t'loan_intent','loan_grade']),df_OHencoded, df_ORDencoded], axis=1)\n  result_df = df_encoded_combined_indexed.set_index('id')\n\n  return result_df\n```\n:::\n\n\n::: {#aa44502a .cell execution_count=47}\n``` {.python .cell-code}\n# Perform categorical imputation\ncat_OHencoder = cat_OHencode_fit(X_train_cat)\ncat_ORDencoder = cat_ORDencode_fit(X_train_cat)\n\n# Transform\nX_train_cat_encoded = cat_encoder_transform(X_train_cat, cat_OHencoder, cat_ORDencoder)\n```\n:::\n\n\n::: {#25f263fb .cell execution_count=48}\n``` {.python .cell-code}\nX_train_cat_encoded\n```\n:::\n\n\n#### Penggabungan Data Numerik dan Kategorik\n\n::: {#63d8d20f .cell execution_count=49}\n``` {.python .cell-code}\ndef concat_data(num_data, cat_data):\n  print('\\nCleaned Numerical data shape: ' + str(num_data.shape))\n  print('Cleaned Categorical data shape: ' + str(cat_data.shape))\n\n  concated_data = num_data.join(cat_data, how = 'inner')\n  print('Concated data shape: ' + str(concated_data.shape))\n\n  return concated_data\n```\n:::\n\n\n::: {#ce84a2f1 .cell execution_count=50}\n``` {.python .cell-code}\nX_train_concat = concat_data(X_train_num_scaled ,X_train_cat_encoded)\n```\n:::\n\n\n::: {#3416c7a5 .cell execution_count=51}\n``` {.python .cell-code}\nX_train_concat\n```\n:::\n\n\n#### Membuat Fungsi Preprocessing\n\n::: {#43ad9c89 .cell execution_count=52}\n``` {.python .cell-code}\ndef preprocess_data(data,num_cols, cat_cols,  num_imputer, scaler, cat_onehot_encoder, cat_ordinal_encoder):\n\n  #Splitting Numerik dan kategorik\n  X_num, X_cat = split_num_cat(data, num_cols, cat_cols)\n\n  #Penanganan Missing Value\n  X_num_imputed = num_imputer_transform(X_num, num_imputer)\n\n  #Standardisasi\n  X_train_num_clean = transform_scaler(X_num_imputed, scaler)\n\n  #Penganganan Inkonsistensi\n  X_cat.loc[:,'cb_person_default_on_file'] = (X_cat['cb_person_default_on_file'].replace({'N' : 0, 'Y' : 1 }).astype(int))\n\n  #Encoding Data Kategorik\n  X_cat_encoded = cat_encoder_transform(X_cat, cat_onehot_encoder, cat_ordinal_encoder)\n\n  #Gabungkan Data\n  cleaned_data = concat_data(X_train_num_clean,X_cat_encoded)\n\n  return cleaned_data\n```\n:::\n\n\n## Task 2:  Modeling\n\n::: {#1ebff5ad .cell execution_count=53}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\nfrom scipy.stats import randint,uniform\n```\n:::\n\n\n::: {#f2f976fc .cell execution_count=54}\n``` {.python .cell-code}\n# Preprocessing Data Test\nX_test_clean = preprocess_data(X_test,num_columns, cat_columns,  num_imputer, scaler, cat_OHencoder, cat_ORDencoder)\n```\n:::\n\n\n::: {#d1c1cd93 .cell execution_count=55}\n``` {.python .cell-code}\ny_test.shape\n```\n:::\n\n\n### Metrics\n\n\nUntuk meminimalkan kejadian **False Negative** akan digunakan `Recall` sebagai evaluation metric utama.\n\n### Baseline Model\n\n::: {#ef95c682 .cell execution_count=56}\n``` {.python .cell-code}\ndummy_clf = dummy_clf = DummyClassifier(strategy = \"most_frequent\")\ndummy_clf.fit(X = X_train_concat,\n              y = y_train_dropped)\n```\n:::\n\n\n::: {#4a7fd3be .cell execution_count=57}\n``` {.python .cell-code}\ny_pred_dummy = dummy_clf.predict(X_train_concat)\n\nreport_dummy_model = classification_report(y_train_dropped, y_pred_dummy)\n\nprint(report_dummy_model)\n```\n:::\n\n\n### Best Model Search\n\n#### kNN Classifier Model\n\n::: {#8fe0b21d .cell execution_count=58}\n``` {.python .cell-code}\nknn_clf = KNeighborsClassifier()\n\nparam_grid = {'n_neighbors': range(1, 21), 'metric': ['euclidean', 'manhattan']}\ngrid_search = GridSearchCV(knn_clf, param_grid, cv=5,scoring='recall')\ngrid_search.fit(X_train_concat, y_train_dropped)\n```\n:::\n\n\n::: {#c6669a90 .cell execution_count=59}\n``` {.python .cell-code}\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Mean cross-validated score of the best_estimator:\", grid_search.best_score_)\n```\n:::\n\n\n::: {#17435ee5 .cell execution_count=60}\n``` {.python .cell-code}\nknn_best_model = grid_search.best_estimator_\ny_pred_knn = knn_best_model.predict(X_test_clean)\n\nreport_knn_model = classification_report(y_test, y_pred_knn)\nprint(report_knn_model)\n```\n:::\n\n\n#### Decision Tree\n\n::: {#36ff4db7 .cell execution_count=61}\n``` {.python .cell-code}\n#Decision Tree\ndtree_clf = DecisionTreeClassifier(random_state = 42)\n\nparam_distributions = {\n    'max_depth': np.arange(1, 50),\n    'min_samples_split': np.arange(2, 20),\n    'min_samples_leaf': np.arange(1, 20),\n    'max_features': [None, 'sqrt', 'log2'],\n    'criterion': ['gini', 'entropy']\n}\n\nrandomcv_dtree = RandomizedSearchCV(\n    estimator=dtree_clf,\n    param_distributions=param_distributions,\n    n_iter=50,\n    cv=5,\n    scoring = \"recall\",\n    random_state=42,\n    n_jobs=-1\n)\n\nrandomcv_dtree.fit(X = X_train_concat,\n              y = y_train_dropped)\n```\n:::\n\n\n::: {#90887502 .cell execution_count=62}\n``` {.python .cell-code}\n# Menampilkan Hasil Hyperparameter Terbaik\nprint(\"Best Parameters:\", randomcv_dtree.best_params_)\nprint(\"Mean cross-validated score of the best_estimator:\", randomcv_dtree.best_score_)\n```\n:::\n\n\n::: {#4a26bb83 .cell execution_count=63}\n``` {.python .cell-code}\nDT_best_model = randomcv_dtree.best_estimator_\ny_pred_DT = DT_best_model.predict(X_test_clean)\n\nreport_DT_model = classification_report(y_test, y_pred_DT)\nprint(report_DT_model)\n```\n:::\n\n\n#### Logistic Regresion\n\n::: {#f4419d39 .cell execution_count=64}\n``` {.python .cell-code}\n#Logistic Regression\nlogreg_clf = LogisticRegression()\n\nparam_grid = {'max_iter': [100, 500, 1000],\n              'penalty' : [ 'l1', 'l2',None],\n              'solver' : ['liblinear', 'saga'],\n              'C': [0.01, 0.1, 1, 10, 100]\n              }\n\ngridcv_logreg = GridSearchCV(estimator= logreg_clf, param_grid=param_grid, cv=5, scoring='recall')\n\ngridcv_logreg.fit(X = X_train_concat,\n              y = y_train_dropped)\n```\n:::\n\n\n::: {#dcbcc795 .cell execution_count=65}\n``` {.python .cell-code}\nprint(\"Best Parameters:\", gridcv_logreg.best_params_)\nprint(\"Mean cross-validated score of the best_estimator:\", gridcv_logreg.best_score_)\n```\n:::\n\n\n::: {#4bbc6801 .cell execution_count=66}\n``` {.python .cell-code}\nlogreg_best_model = gridcv_logreg.best_estimator_\ny_pred_logreg = logreg_best_model.predict(X_test_clean)\n\nreport_logreg_model = classification_report(y_test, y_pred_logreg)\nprint(report_logreg_model)\n```\n:::\n\n\n#### Support Vector Classifier Model\n\n::: {#d72df246 .cell execution_count=67}\n``` {.python .cell-code}\nsvc = SVC()\n\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'kernel': ['linear', 'rbf', 'poly'],\n    'gamma': ['scale', 'auto'],\n    'degree': [2, 3, 4]\n}\n\nrandomcv_svc = RandomizedSearchCV(\n    estimator=svc,\n    param_distributions=param_grid,\n    scoring='recall',\n    cv=5,\n    verbose=1,\n    n_jobs=-1\n)\n\nrandomcv_svc.fit(X = X_train_concat,\n              y = y_train_dropped)\n```\n:::\n\n\n::: {#4d99752f .cell execution_count=68}\n``` {.python .cell-code}\nprint(\"Best Parameters:\", randomcv_svc.best_params_)\nprint(\"Mean cross-validated score of the best_estimator:\", randomcv_svc.best_score_)\n```\n:::\n\n\n::: {#fcc49e78 .cell execution_count=69}\n``` {.python .cell-code}\ny_pred_svc = grid_search.best_estimator_.predict(X_test_clean)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_svc))\n```\n:::\n\n\n#### Bagging Decision Tree\n\n::: {#1f7e286e .cell execution_count=70}\n``` {.python .cell-code}\ntree = DecisionTreeClassifier(random_state=123)\nbagging_model = BaggingClassifier(estimator= tree, random_state=123)\n\n# Parameter grid\nparam_distributions = {\n    'n_estimators': randint(10, 100),\n    'max_samples': [0.5, 0.7, 1.0],\n    'max_features': [0.5, 0.7, 1.0],\n    'bootstrap': [True, False],\n    'bootstrap_features': [True, False],\n    'estimator__max_depth': randint(5, 30),\n    'estimator__min_samples_split': randint(2, 20),\n}\n\n# RandomizedSearchCV\nrandom_search_bagging = RandomizedSearchCV(\n    estimator=bagging_model,\n    param_distributions=param_distributions,\n    n_iter=20,\n    cv=5,\n    scoring='recall',\n    n_jobs=-1,\n    random_state=123\n)\n\nrandom_search_bagging.fit(X = X_train_concat, y = y_train_dropped)\n```\n:::\n\n\n::: {#6d54e715 .cell execution_count=71}\n``` {.python .cell-code}\nprint(\"Best Parameters:\", random_search_bagging.best_params_)\nprint(\"Mean cross-validated score of the best_estimator:\", random_search_bagging.best_score_)\n```\n:::\n\n\n::: {#0a479c71 .cell execution_count=72}\n``` {.python .cell-code}\nbagging_best_model = random_search_bagging.best_estimator_\ny_pred_bagging = bagging_best_model.predict(X_test_clean)\n\nreport_bagging_model = classification_report(y_test, y_pred_bagging)\nprint(report_bagging_model)\n```\n:::\n\n\n#### Random Forest\n\n::: {#c71b4eea .cell execution_count=73}\n``` {.python .cell-code}\nrf_clf = RandomForestClassifier(random_state=42)\n\nparam_dist = {\n    'n_estimators': randint(50, 200),  # Jumlah trees antara 50-200\n    'max_depth': [None, 10, 20, 30],  # Kedalaman maksimal tree\n    'min_samples_split': randint(2, 10),  # Minimal sampel untuk split\n    'min_samples_leaf': randint(1, 5),   # Minimal sampel di leaf node\n    'criterion': ['gini', 'entropy'],    # Fungsi untuk split\n}\n\nrandomcv_rf = RandomizedSearchCV(rf_clf, param_distributions=param_dist, n_iter=20, cv=5, scoring='recall', random_state = 42)\nrandomcv_rf.fit(X_train_concat, y_train_dropped)\n```\n:::\n\n\n::: {#0fc31759 .cell execution_count=74}\n``` {.python .cell-code}\nprint(\"Best Parameters:\", randomcv_rf.best_params_)\nprint(\"Best Cross-Validated recall:\", randomcv_rf.best_score_)\n```\n:::\n\n\n::: {#a599d5b9 .cell execution_count=75}\n``` {.python .cell-code}\nrf_best_model = randomcv_rf.best_estimator_\ny_pred_rf = rf_best_model.predict(X_test_clean)\n\n\nreport_rf_model = classification_report(y_test, y_pred_rf)\nprint(report_rf_model)\n```\n:::\n\n\n#### Adaboost Model\n\n::: {#32a9591e .cell execution_count=76}\n``` {.python .cell-code}\ntree = DecisionTreeClassifier(random_state=123)\nadaboost_model = AdaBoostClassifier(estimator= tree, random_state=123)\n\nparam_distributions = {\n    'n_estimators': randint(50, 200),\n    'learning_rate': uniform(0.01, 1.0),\n    'estimator__max_depth': randint(1, 5),\n    'estimator__min_samples_split': randint(2, 20),\n    'estimator__min_samples_leaf': randint(1, 10),\n}\n\nrandom_search_adaboost = RandomizedSearchCV(\n    estimator=adaboost_model,\n    param_distributions=param_distributions,\n    n_iter=20,\n    cv=5,\n    scoring='recall',\n    n_jobs=-1,\n    random_state=123\n)\n\n\nrandom_search_adaboost.fit(X_train_concat, y_train_dropped)\n```\n:::\n\n\n::: {#00ad73d9 .cell execution_count=77}\n``` {.python .cell-code}\nprint(\"Best Parameters:\", random_search_adaboost.best_params_)\nprint(\"Best Cross-Validated recall:\", random_search_adaboost.best_score_)\n```\n:::\n\n\n::: {#90a2120a .cell execution_count=78}\n``` {.python .cell-code}\nadaboost_best_model = random_search_adaboost.best_estimator_\ny_pred_adaboost = adaboost_best_model.predict(X_test_clean)\n\nreport_adaboost = classification_report(y_test, y_pred_adaboost)\nprint(report_adaboost)\n```\n:::\n\n\n### Final Best Model\n\n::: {#215ef8fc .cell execution_count=79}\n``` {.python .cell-code}\n# Best Model: Decision Tree\nbest_model = randomcv_dtree.best_estimator_\n```\n:::\n\n\n## Task 3: Model Evaluation\n\n### Score on test data\n\n::: {#c1ffb955 .cell execution_count=80}\n``` {.python .cell-code}\ny_pred_dt = best_model.predict(X_test_clean)\nreport_dt = classification_report(y_test, y_pred_dt)\nprint(report_dt)\n```\n:::\n\n\n### Financial Impact Comparison\n\n::: {#bf21f459 .cell execution_count=81}\n``` {.python .cell-code}\n# False Negative potential loss : 35juta\n# False positive potential loss : 10 juta\n\ny_pred_dummy = dummy_clf.predict(X_test)\n\nmodel_predictions = {\n    'dummy': y_pred_dummy,\n    'knn': y_pred_knn,\n    'dt': y_pred_dt,\n    'logreg': y_pred_logreg,\n    'svc': y_pred_svc,\n    'bagging': y_pred_bagging,\n    'rf': y_pred_rf,\n    'boost': y_pred_adaboost\n}\n\nconfusion_matrices = {}\n\nfor model_name, y_pred in model_predictions.items():\n    confusion_matrices[model_name] = confusion_matrix(y_test, y_pred).ravel()\n\n#Financial Loss\ncost_fn = 35  # dalam juta\ncost_fp = 10\n\nfinancial_losses = {}\n\n# Hitung Total Loss\nfor model_name, cm_values in confusion_matrices.items():\n    fn, fp = cm_values[2], cm_values[1]  # FN dan FP berada di indeks 2 dan 1\n    financial_losses[model_name] = fn * cost_fn + fp * cost_fp\n\n\ncomparison_data = []\n\n# Loop untuk mengisi data financial comparison\nfor model_name, cm_values in confusion_matrices.items():\n    fn, fp = cm_values[2], cm_values[1]  # FN dan FP berada di indeks 2 dan 1\n    total_loss = fn * cost_fn + fp * cost_fp\n    comparison_data.append({\n        'Model': model_name.capitalize(),        # Nama model dengan huruf kapital\n        'False Negative (FN)': fn,              # Jumlah FN\n        'False Positive (FP)': fp,              # Jumlah FP\n        'Total Loss (Rp juta)': total_loss      # Total loss dalam juta\n    })\n\n# Buat DataFrame dari data yang terkumpul\nfinancial_comparison = pd.DataFrame(comparison_data)\n\n# Tampilkan DataFrame\nfinancial_comparison\n```\n:::\n\n\n::: {#a3be75f7 .cell execution_count=82}\n``` {.python .cell-code}\nfinancial_comparison_sorted = financial_comparison.sort_values(\n    by='Total Loss (Rp juta)',ascending = False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    data=financial_comparison_sorted,\n    x='Model',\n    y='Total Loss (Rp juta)',\n    palette='Blues_d'  # Pilih palet warna\n)\n\n# Tambahkan label dan judul\nplt.title('Financial Loss by Model', fontsize=16)\nplt.xlabel('Model', fontsize=12)\nplt.ylabel('Total Loss (Rp juta)', fontsize=12)\n\n\n\n# Tampilkan plot\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\nModel Bagging memiliki potential loss paling rendah\n\n",
    "supporting": [
      "DaffaDanielR_Mentoring_3_Introduction_to_Machine_Learning_files"
    ],
    "filters": [],
    "includes": {}
  }
}